[{"title":"Attention TPä¸GQA","url":"/2025/08/17/distribute/attention/","content":"\n\nä¾‹å­é…ç½®ï¼ˆè´¯ç©¿å…¨æ–‡ï¼‰ï¼š hidden_size=4096, num_attention_heads=32, tensor_parallel_size=4, num_query_groups=8ï¼ˆGQAï¼‰, kv_channels=hidden/heads=128ã€‚ è¾“å…¥å½¢çŠ¶ç”¨ [B, S, H] è®°ï¼ˆæ‰¹ã€åºåˆ—ã€éšè—ï¼‰ã€‚\n\n\n1. åè¯ä¸æ´¾ç”Ÿå˜é‡ï¼ˆå…ˆæŠŠé‡ç®—æ¸…æ¥šï¼‰\n\nå•å¤´ç»´åº¦ï¼ˆä¹Ÿæ˜¯ç¼©æ”¾ç”¨çš„ \\(d_k\\)ï¼‰ï¼škv_channels = 4096 / 32 = 128\nQ æŠ•å½±æ€»ç»´ï¼šquery_projection_size = kv_channels * num_attention_heads = 128*32 = 4096\nK/V æŠ•å½±æ€»ç»´ï¼ˆGQAï¼‰ï¼škv_projection_size = kv_channels * num_query_groups = 128*8 = 1024\næ¯å¡ Q å¤´æ•°ï¼šnum_attention_heads_per_partition = 32 / TP = 8\næ¯å¡ KV ç»„æ•°ï¼šnum_query_groups_per_partition = 8 / TP = 2\næ¯å¡æŠ•å½±ç»´ï¼ˆåˆ—å¹¶è¡Œåæœ¬åœ°è¾“å‡ºç»´ï¼‰ï¼šhidden_size_per_partition = 4096 / TP = 1024\n\n\nGQA çš„å«ä¹‰ï¼šå½“ num_key_value_heads (=num_query_groups) å°äº num_attention_heads æ—¶ï¼Œä¸ºè¾ƒå°‘çš„ KV å¤´/ç»„ äº§å‡º K/Vï¼Œè®©å¤šä¸ª Q å¤´å…±äº«å®ƒä»¬ï¼›=heads é€€åŒ–ä¸º MHAï¼Œ=1 æ˜¯ MQAã€‚è¿™ä¸€ç‚¹åœ¨ HF æ¨¡å‹æ–‡æ¡£ä¸­æ˜¯æ˜ç¡®çš„å®šä¹‰ã€‚(Hugging Face)\n\n\n2. ç«¯åˆ°ç«¯è®¡ç®—ä¸å½¢çŠ¶æµï¼ˆä»¥å•å±‚è‡ªæ³¨æ„åŠ›ä¸ºä¾‹ï¼‰\nMegatron ç»å…¸åšæ³•ï¼šQ/K/V çš„çº¿æ€§å±‚ç”¨åˆ—å¹¶è¡Œï¼ˆColumn-Parallelï¼‰ï¼ŒæŒ‰è¾“å‡ºåˆ—åˆ‡ç»™å„å¡ï¼›è¾“å‡ºæŠ•å½±ç”¨è¡Œå¹¶è¡Œï¼ˆRow-Parallelï¼‰ï¼ŒæŒ‰è¾“å…¥è¡Œåˆ‡ç»™å„å¡ï¼Œå‰å‘åªåœ¨è¾“å‡ºæŠ•å½±åšä¸€æ¬¡ all-reduceã€‚è¿™æ˜¯ Megatron-LM è®ºæ–‡ä¸ Megatron-Core æ–‡æ¡£æ¨èçš„å¼ é‡å¹¶è¡Œåˆ‡æ³•ã€‚(arXiv, NVIDIA Docs)\n2.1 çº¿æ€§æŠ•å½±ï¼ˆåˆ—å¹¶è¡Œï¼‰\n\nQ æŠ•å½±ï¼ˆå…¨å±€æƒé‡ [4096,4096] â†’ æ¯å¡ [4096,1024]ï¼‰ï¼š æœ¬å¡è¾“å‡º Q_local: [B,S,1024] â†’ reshape ä¸º [B, 8, S, 128]ï¼ˆæœ¬å¡ 8 ä¸ª Q å¤´ï¼‰ã€‚\nK æŠ•å½±ï¼ˆå…¨å±€æƒé‡ [4096,1024] â†’ æ¯å¡ [4096,256]ï¼‰ï¼š K_local: [B,S,256] â†’ reshape ä¸º [B, 2, S, 128]ï¼ˆæœ¬å¡ 2 ä¸ª KV ç»„ï¼‰ã€‚\nV æŠ•å½± åŒ Kã€‚\n\n\nä¸ºä»€ä¹ˆå¿…é¡» reshape å‡º head ç»´ï¼Ÿ å¤šå¤´æ³¨æ„åŠ›çš„è¯­ä¹‰æ˜¯â€œå¤´å†…ç‹¬ç«‹â€çš„ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œå†…æ ¸ï¼ˆSDPA/FlashAttentionï¼‰ä¸å¹¿æ’­ï¼ˆmaskã€RoPEã€repeat_kvï¼‰éƒ½è¦æ±‚æ˜¾å¼çš„ head ç»´ [B,H,S,D] æˆ–å±•å¹³ä¸º [BÂ·H,S,D]ã€‚ä¸æ‹†å¤´ä¼šæŠŠä¸åŒ head çš„å­ç©ºé—´æ··åœ¨ä¸€èµ·ï¼Œä¹Ÿæ— æ³•è‡ªç„¶æ‰§è¡Œ GQA çš„ repeat_kvã€‚(PyTorch)\n\n2.2 GQA çš„ K/V å¯¹é½ï¼ˆrepeat/expandï¼‰\næœ¬å¡åªæœ‰ 2 ä¸ª KV ç»„ï¼Œä½†è¦æœåŠ¡ 8 ä¸ª Q å¤´ â‡’ åœ¨å¤´ç»´åšé€»è¾‘é‡å¤/å¹¿æ’­ï¼š [B, 2, S, 128] â†’ [B, 8, S, 128]ï¼ˆæ¯ä¸ª KV ç»„æœåŠ¡ 4 ä¸ª Q å¤´ï¼‰ã€‚ä¸»æµå®ç°ç›´æ¥åœ¨ head ç»´åš repeat_kvã€‚(Hugging Face)\n2.3 Scaled Dot-Product Attentionï¼ˆæ¯å¡åªç®—è‡ªå·±çš„ 8 ä¸ªå¤´ï¼‰\n\nscores = (Q @ K^T) / sqrt(128) â†’ softmax(scores) @ V\nå¾—åˆ°ä¸Šä¸‹æ–‡ ctx_local: [B, 8, S, 128] â†’ æ‹¼æ¥ä¸º [B,S,1024]\nè¿™ä¸€æ­¥å¯ä»¥ç”± PyTorch SDPA æˆ–é—ªå­˜æ³¨æ„åŠ›å†…æ ¸é«˜æ•ˆå®Œæˆã€‚(PyTorch)\n\n2.4 è¾“å‡ºæŠ•å½±ï¼ˆè¡Œå¹¶è¡Œ + 1 æ¬¡ all-reduceï¼‰\n\næ¯å¡æŠŠ [B,S,1024] ä¹˜ä»¥æœ¬å¡çš„è¾“å‡ºæƒé‡åˆ†ç‰‡ï¼Œå¾—åˆ° Y_local: [B,S,4096] çš„éƒ¨åˆ†å’Œï¼›\nè·¨å¡åš all-reduce(sum) å¾—åˆ°æœ€ç»ˆ Y: [B,S,4096]ã€‚ Megatron è®ºæ–‡æŒ‡å‡ºï¼šè‡ªæ³¨æ„åŠ›æœ¬ä½“ä¸éœ€é€šä¿¡ï¼Œåªåœ¨è¾“å‡ºæŠ•å½±å¤„åšä¸€æ¬¡è§„çº¦å³å¯ã€‚(arXiv)\n\n\n3. åˆ—å¹¶è¡Œ / è¡Œå¹¶è¡Œçš„æ•°å­¦ç­‰ä»·ï¼ˆä¸ºä½•â€œåˆ‡äº†å†æ‹¼/æ±‚å’Œâ€ä»ç­‰ä»·å•å¡ï¼‰\næŠŠ [B,S,Â·] å±•å¹³ä¸ºçŸ©é˜µ \\(X\\in\\mathbb{R}^{N\\times(HD)}\\)ï¼ˆ\\(N=B\\cdot S\\)ï¼‰ï¼Œè¾“å‡ºéšè—è®°ä¸º \\(H\\)ã€‚\n3.1 åˆ—å¹¶è¡Œï¼ˆColumn-Parallel Linearï¼‰â‰¡ æ‹¼æ¥\nè®¾ Q çš„å…¨é‡æƒé‡ \\(W_Q\\in\\mathbb{R}^{(HD)\\times(HD)}\\)ï¼Œæ²¿åˆ—åˆ‡æˆ \\(p\\) å—ï¼š\n\\[\nW_Q=\\big[W_Q^{(0)}\\;\\;W_Q^{(1)}\\;\\;\\cdots\\;\\;W_Q^{(p-1)}\\big],\n\\quad W_Q^{(i)}\\in\\mathbb{R}^{(HD)\\times(H/p\\cdot D)}.\n\\]\nåˆ™\n\\[\nQ=XW_Q=\\big[XW_Q^{(0)}\\;\\;XW_Q^{(1)}\\;\\;\\cdots\\;\\;XW_Q^{(p-1)}\\big]\n      =\\operatorname{Concat}(Q^{(0)},\\dots,Q^{(p-1)}).\n\\]\næ¯å¡ç‹¬ç«‹è®¡ç®—è‡ªå·±çš„ \\(Q^{(i)}\\)ï¼Œæ— é¡»é€šä¿¡ã€‚K/V åŒç†ã€‚è¿™å°±æ˜¯ Column-Parallel çš„ç²¾ç¡®å®šä¹‰ã€‚(arXiv, NVIDIA Docs)\n3.2 æ¯å¤´ç‹¬ç«‹ â‡’ æŒ‰å¤´åˆ‡ç»™å„å¡ä»ç„¶æ­£ç¡®\nå•å¤´/å•ç»„æ³¨æ„åŠ›ï¼š\n\\[\nY_h=\\operatorname{softmax}\\!\\Big(\\tfrac{Q_hK_{g(h)}^\\top}{\\sqrt{D}}\\Big)V_{g(h)}.\n\\]\nGQA ä¸‹ \\(g(h)\\) æŠŠå¤šä¸ª Q å¤´æ˜ å°„åˆ°åŒä¸€ KV ç»„ï¼›ç”±äºå¤´é—´äº’ä¸ç›¸å¹²ï¼ŒæŠŠ 32 ä¸ªå¤´å¹³å‡åˆ†æˆ 4 ä»½åˆ° 4 å¼ å¡ï¼Œå„å¡åªä¾èµ–è‡ªå·±çš„ KV ç»„ï¼Œå°±ä¸å•å¡ä¸€è‡´ã€‚repeat_kv æ­£æ˜¯æ²¿ head ç»´æŠŠ KV å¯¹é½åˆ° Q å¤´æ•°ã€‚(Hugging Face)\n3.3 è¡Œå¹¶è¡Œï¼ˆRow-Parallel Linearï¼‰â‰¡ æ±‚å’Œï¼ˆall-reduceï¼‰\næŠŠæ³¨æ„åŠ›è¾“å‡ºçš„æ‹¼æ¥å¼ é‡ \\(C\\in\\mathbb{R}^{N\\times(HD)}\\) æŒ‰åˆ—ï¼ˆç‰¹å¾ï¼‰åˆ‡å—ï¼š\n\\[\nC=\\big[C^{(0)}\\;\\;C^{(1)}\\;\\;\\cdots\\;\\;C^{(p-1)}\\big],\\quad\nC^{(i)}\\in\\mathbb{R}^{N\\times(H/p\\cdot D)}.\n\\]\nè¾“å‡ºæƒé‡ \\(W_O\\in\\mathbb{R}^{(HD)\\times H}\\) æŒ‰è¡Œåˆ‡å—ï¼š\n\\[\nW_O=\\begin{bmatrix}\nW_O^{(0)}\\\\ W_O^{(1)}\\\\ \\vdots\\\\ W_O^{(p-1)}\n\\end{bmatrix},\n\\quad W_O^{(i)}\\in\\mathbb{R}^{(H/p\\cdot D)\\times H}.\n\\]\nå—ä¹˜æ³•æ’ç­‰å¼ï¼š\n\\[\nC\\,W_O=\\sum_{i=0}^{p-1} C^{(i)}W_O^{(i)}.\n\\]\nå› æ­¤å„å¡è®¡ç®— \\(Y^{(i)}=C^{(i)}W_O^{(i)}\\)ï¼Œå† all-reduce(sum)ï¼Œå°±å¾—åˆ°ä¸å•å¡å®Œå…¨ç›¸åŒçš„ \\(Y\\)ã€‚è¿™æ­£æ˜¯ Row-Parallel çš„æœ¬è´¨ã€‚(arXiv)\n\n4. ä¸ºä»€ä¹ˆ GQA ä¼šè®© kv_projection_size å˜å°ã€KV cache å˜çœï¼Ÿ\n\nK/V çº¿æ€§å±‚åªä¸º num_query_groups äº§å‡ºé€šé“ï¼šä» 4096ï¼ˆ=32Ã—128ï¼‰é™ä¸º 1024ï¼ˆ=8Ã—128ï¼‰ï¼ŒK/V æŠ•å½±çš„ å‚æ•°é‡ä¸ FLOPs çº¦ä¸ºåŸæ¥çš„ 1/4ï¼›\næ¨ç†é˜¶æ®µçš„ KV cache ä»¥ã€ŒKV å¤´ Ã— åºåˆ— Ã— å¤´ç»´ã€è®¡é‡ï¼ŒKV å¤´ä» 32 å˜ 8ï¼Œç¼“å­˜ä¸ç›¸å…³å¸¦å®½å‡ç›¸åº”ä¸‹é™ã€‚HF æ–‡æ¡£æ˜ç¡®ä»¥ num_key_value_heads æè¿°è¯¥è¡Œä¸ºã€‚(Hugging Face)\n\n\n5. å½¢çŠ¶é€ŸæŸ¥ï¼ˆä»¥æœ¬ä¾‹ä¸ºå‡†ï¼‰\n\n\n\nå¼ é‡/æ­¥éª¤\nå…¨å±€ï¼ˆä¸åˆ†ç‰‡ï¼‰\næ¯å¡ï¼ˆTP=4ï¼‰\nè¯´æ˜\n\n\n\n\nQ çº¿æ€§è¾“å‡ºç»´\n4096\n1024\nColumn-Parallelï¼Œæ— é€šä¿¡\n\n\nK çº¿æ€§è¾“å‡ºç»´\n1024\n256\nGQAï¼šåªå‡º 8 ä¸ª KV ç»„\n\n\nV çº¿æ€§è¾“å‡ºç»´\n1024\n256\nåŒä¸Š\n\n\nQ å¤´æ•°\n32\n8\næœ¬å¡åªç®—è‡ªå·± 8 ä¸ªå¤´\n\n\nKV ç»„æ•°\n8\n2\næ¯ç»„æœåŠ¡ 4 ä¸ª Q å¤´\n\n\nå¤´ç»´ \\(D\\)\n128\n128\nç”¨äº \\(1/\\sqrt{D}\\)\n\n\næœ¬å¡æ³¨æ„åŠ›è¾“å‡ºï¼ˆæ‹¼å¤´åï¼‰\nâ€“\n[B,S,1024]\nè¿›å…¥è¾“å‡ºæŠ•å½±\n\n\næœ€ç»ˆè¾“å‡ºï¼ˆall-reduce åï¼‰\n[B,S,4096]\n[B,S,4096]\nRow-Parallel + sum\n\n\n\nï¼ˆè‹¥å¯ç”¨ Sequence Parallelï¼Œåªä¼šæ²¿ S å†åˆ‡ä¸€ç»´ï¼Œä¸å½±å“ä¸Šè¿°å¤´/é€šé“ç»´é€»è¾‘ã€‚åˆ—/è¡Œå¹¶è¡Œä¸ä¸€æ¬¡é€šä¿¡çš„ç»“æ„æ˜¯ Megatron-LM çš„â€œç»å…¸æ‹†åˆ†â€ã€‚ï¼‰(awsdocs-neuron.readthedocs-hosted.com, Better Tomorrow with Computer Science)\n\n6. Mermaidï¼šä¸€å¼ â€œç»´åº¦/å¹¶è¡Œæ–¹å¼â€å°å›¾\n%%&#123;init: &#123; &quot;flowchart&quot;: &#123; &quot;htmlLabels&quot;: true, &quot;wrap&quot;: true &#125; &#125;&#125;%%flowchart TB  X[&quot;Input X: [B,S,4096]&quot;] --&gt; QKV[&quot;Column-Parallel Q/K/V&lt;br/&gt;Q:[B,S,1024]  K/V:[B,S,256]&quot;]  QKV --&gt; Reshape[&quot;Reshape by heads&lt;br/&gt;Q:[B,8,S,128]&lt;br/&gt;K/V:[B,2,S,128]&quot;]  Reshape --&gt; RepeatKV[&quot;repeat_kv on head dim&lt;br/&gt;K/V:[B,8,S,128]&quot;]  RepeatKV --&gt; SDPA[&quot;SDPA per head&lt;br/&gt;ctx_local:[B,8,S,128]&lt;br/&gt;concat-&gt;[B,S,1024]&quot;]  SDPA --&gt; OutProj[&quot;Row-Parallel OutProj&lt;br/&gt;Y_local:[B,S,4096]&quot;]  OutProj --&gt; AllReduce[&quot;all-reduce(sum)&quot;]  AllReduce --&gt; Y[&quot;Final Y: [B,S,4096]&quot;]\n\n7. æç®€ä¼ªç ï¼ˆPyTorch é£æ ¼ï¼‰\n# åˆ—å¹¶è¡Œçš„çº¿æ€§ï¼šæ¯å¡æ‹¿åˆ° Q/K/V çš„ä¸€æ®µè¾“å‡ºåˆ—Q_local = linear_col_parallel_Q(X)   # [B,S,1024] -&gt; view [B,8,S,128]K_local = linear_col_parallel_K(X)   # [B,S,256]  -&gt; view [B,2,S,128]V_local = linear_col_parallel_V(X)   # [B,S,256]  -&gt; view [B,2,S,128]Q = Q_local.view(B, S, 8, 128).transpose(1, 2)  # [B,8,S,128]K = K_local.view(B, S, 2, 128).transpose(1, 2)  # [B,2,S,128]V = V_local.view(B, S, 2, 128).transpose(1, 2)  # [B,2,S,128]# GQA: è®© 2 ä¸ª KV ç»„åŒ¹é… 8 ä¸ª Q å¤´ï¼ˆé€»è¾‘ repeat/expandï¼‰K = repeat_kv(K, n_rep=4)   # [B,8,S,128]V = repeat_kv(V, n_rep=4)   # [B,8,S,128]# SDPAï¼ˆæ¯å¡åªç®—è‡ªå·±çš„ 8 ä¸ªå¤´ï¼‰ctx = torch.nn.functional.scaled_dot_product_attention(Q, K, V)  # [B,8,S,128]ctx = ctx.transpose(1, 2).reshape(B, S, 1024)                    # [B,S,1024]# è¡Œå¹¶è¡Œè¾“å‡º + ä¸€æ¬¡ all-reduce(sum)Y_local = linear_row_parallel_out(ctx)   # partial: [B,S,4096]Y = all_reduce_sum(Y_local)              # final:   [B,S,4096]\n\nSDPA çš„æ¥å£ä¸è¯­ä¹‰è§ PyTorch æ–‡æ¡£ï¼›repeat_kv çš„è¯­ä¹‰ä¸ GQA çš„é…ç½®åœ¨ HF æ–‡æ¡£/å®ç°ä¸­æœ‰æ˜ç¡®å®šä¹‰ã€‚(PyTorch, Hugging Face)\n\n\n8. æ­£ç¡®æ€§ Checklistï¼ˆå®è·µä¸­æœ€å¸¸è§çš„å‘ï¼‰\n\næ•´é™¤å…³ç³»ï¼š num_attention_heads % TP == 0ï¼Œnum_query_groups % TP == 0ï¼Œä¸” num_attention_heads % num_query_groups == 0ï¼ˆGQAï¼‰ã€‚(Hugging Face)\næ˜¾å¼ head ç»´ï¼šå½¢çŠ¶åº”ä¸º [B,H,S,D] æˆ–å±•å¹³ä¸º [BÂ·H,S,D]ï¼Œä»¥å¥‘åˆ SDPA/FlashAttention ä¸ repeat_kvã€‚(PyTorch)\né€šä¿¡ä½ç½®ï¼šè‡ªæ³¨æ„åŠ›æœ¬ä½“æ— è·¨å¡é€šä¿¡ï¼›ä»…è¾“å‡ºæŠ•å½±éœ€è¦ä¸€æ¬¡ all-reduceã€‚(arXiv)\n\n\nå‚è€ƒä¸å»¶ä¼¸é˜…è¯»\n\nMegatron-LM è®ºæ–‡ï¼šæå‡ºå±‚å†…ï¼ˆå¼ é‡ï¼‰å¹¶è¡Œï¼Œæ³¨æ„åŠ›ç”¨åˆ—å¹¶è¡Œï¼Œè¾“å‡ºç”¨è¡Œå¹¶è¡Œï¼Œå‰å‘ä»…ä¸€å¤„é€šä¿¡ã€‚(arXiv, ar5iv)\nMegatron-Core æ–‡æ¡£ï¼šTensor Parallel API/ç”¨æˆ·æŒ‡å—ï¼ˆNVIDIA å®˜æ–¹ï¼‰ã€‚(NVIDIA Docs)\nPyTorch SDPA æ–‡æ¡£/æ•™ç¨‹ï¼šå®˜æ–¹çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æ¥å£ä¸é«˜æ€§èƒ½å®ç°ã€‚(PyTorch, PyTorch Docs)\nHF æ–‡æ¡£ï¼ˆLlama/Qwen ç³»åˆ—ï¼‰ï¼šnum_key_value_heads çš„å®šä¹‰ã€GQA/MQA/MHA çš„å…³ç³»ï¼›å®ç°é‡Œ repeat_kv çš„ç”¨æ³•ã€‚(Hugging Face)\nåˆ—å¹¶è¡Œ/è¡Œå¹¶è¡Œå¯è§†åŒ–è®²è§£ï¼šå¯¹ ColumnParallelLinear / RowParallelLinear çš„ç›´è§‚å›¾è§£ã€‚(awsdocs-neuron.readthedocs-hosted.com, Better Tomorrow with Computer Science)\n\n\n","categories":["distribute"],"tags":["attention"]},{"title":"pytorch DeviceMeshï¼šå®ç°åŸç†ä¸å®æˆ˜","url":"/2025/06/14/distribute/device_mesh/","content":"\nä¸€ã€ä¸ºä½•ä½¿ç”¨ DeviceMeshï¼Ÿ\nåœ¨æ··åˆå¹¶è¡Œï¼ˆDP/TP/PP/HSDP/â€¦ï¼‰ä¸­ï¼Œéœ€è¦ç®¡ç†å¤šä¸ªå­é€šä¿¡ç»„ï¼ˆProcessGroupï¼‰ï¼Œå¯¹åº”å¤æ‚çš„è®¾å¤‡æ‹“æ‰‘ç»“æ„ã€‚DeviceMesh æä¾›äº†ï¼š\n\nç†è®ºä¸Šæ— ç¼æ”¯æŒä»»æ„ç»´åº¦çš„å¤šç»´æ‹“æ‰‘ï¼›\nè‡ªåŠ¨æ‹†åˆ†è¿›ç¨‹ç»„(new_group/split_group)ï¼›\nçµæ´»åˆ‡ç‰‡å­ Meshï¼›\nç»å†è®¾è®¡å‘¨å…¨çš„é«˜æ•ˆåˆå§‹åŒ–æ–¹æ¡ˆ (docs.pytorch.org, pytorch.org)ã€‚\n\n\näºŒã€åˆå§‹åŒ–æµç¨‹\ninit_device_mesh(...) çš„ä½œç”¨\nä¸€ä¸ªä¸€è¡Œæå®šçš„æ–¹æ³•ï¼Œå®ƒä¼šï¼š\n\nåˆå§‹åŒ–å…¨å±€ init_process_group(...)ï¼ˆè‹¥æœªåˆå§‹åŒ–ï¼‰ï¼›\næ ¹æ® mesh_shape è‡ªåŠ¨æ„é€  CPU ä¸Šçš„ torch.arange(...).view(...)ï¼›\nåˆ›å»º DeviceMesh(...)ã€‚å†…éƒ¨å®Œæˆå­ç»„æ‹†åˆ†åŸç†ï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰ã€‚\n\n\nDeviceMesh.__init__() + _init_process_groups()\n\nå­˜å‚¨ï¼šdevice_typeã€meshã€mesh_dim_namesï¼›\né€šä¿¡ç»„æ‹†åˆ†ï¼šéå†æ¯ä¸ªç»´åº¦ dimï¼š\n\nä½¿ç”¨ mesh.swapdims(-1, dim).reshape(-1, size(dim)) åˆ—å‡ºè¯¥ç»´æ‰€æœ‰å­ç»„ rankï¼›\nè‹¥ NCCL å·²ç»‘å®š GPUï¼Œå³å¯ç”¨ split_group ä¸€æ¬¡æ‹†å‡ºå…¨éƒ¨å­ç»„ï¼›\nå¦åˆ™ä½¿ç”¨ new_group() åˆ† group æ‹†ï¼›\nå¹¶å°†å½“å‰ rank å±äºçš„é‚£ç»„ä¿¡æ¯æ”¾å…¥ self._dim_group_infos[dim]ï¼›\n\nç»“æœï¼šæ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªåŒ…å«å½“å‰ rank çš„ ProcessGroup ä¿¡æ¯åˆ—è¡¨ã€‚\n\n#ppmesh = torch.tensor([  [0, 1],  # pp=0  [2, 3],  # pp=1  [4, 5],  # pp=2  [6, 7]   # pp=3])mesh.swapdims(-1, 0)tensor([[0,2,4,6],        [1,3,5,7]])pg_ranks_by_dim = tmp.reshape(-1, mesh.size(0))[  [0,2,4,6],  # å¯¹åº” tp è¡Œ 0 å„ pp æ®µ  [1,3,5,7]   # å¯¹åº” tp è¡Œ 1 å„ pp æ®µ]#tptmp = mesh.swapdims(-1, 1)  # ç­‰äº transpose(1,1)ï¼Œæœ¬èº«æ— å˜åŒ–pg_ranks_by_dim = tmp.reshape(-1, mesh.size(1))[  [0,1],  # pp=0  [2,3],  [4,5],  [6,7]]\n\nä¸‰ã€æ ¸å¿ƒæ¥å£ä¸å†…éƒ¨å®ç°è§£æ\n1. å±æ€§ä¸æ–¹æ³•\nmesh.shape  # tuple(self.mesh.shape)mesh.ndim   # int(self.mesh.ndim)mesh.size(dim=None)  # æ€»å…ƒç´ æ•° or self.mesh.size(dim)\nç”¨äºè·å– mesh å…ƒç»“æ„å’Œè§„æ¨¡ï¼Œé€‚ç”¨äºåˆ¤æ–­ç»´åº¦æ•°é‡ã€å¾ªç¯è¿­ä»£ã€å¹¶è¡Œç­–ç•¥é…ç½®ç­‰åœºæ™¯ã€‚\n\n2. Rank ä¸åæ ‡\n\nget_rank()ï¼šç­‰ä»·äº torch.distributed.get_rank()ï¼Œè¿”å›å…¨å±€ rankï¼›\nget_local_rank(mesh_dim)ï¼šå†…éƒ¨è°ƒç”¨ get_rank(self.get_group(mesh_dim)) â†’ å½“å‰ç»´åº¦çš„å°ç»„å†…ç¼–å·ï¼›\nget_coordinate()ï¼šè¿”å› self._coordinate_on_dimï¼Œå…¶åœ¨åˆå§‹åŒ–ä¸­é€šè¿‡ (self.mesh==global_rank).nonzero() è·å¾—ã€‚\n\nç¤ºä¾‹ï¼šmesh_shape=(4,2)ï¼Œrank=5 â†’ local_pp=2ã€local_tp=1ï¼Œcoordinate [2,1]ã€‚\n\n3. é€šä¿¡ç»„è·å–\n\nget_group(mesh_dim)ï¼š\n\nè‹¥ 1D ä¸”ä¸ä¼ å‚ï¼Œç›´æ¥è¿”å›å”¯ä¸€å­è¿›ç¨‹ç»„ï¼›\nå¤šç»´åˆ™æ ¹æ® mesh_dimï¼ˆç´¢å¼•æˆ–åå­—ï¼‰æ£€ç´¢ self._dim_group_infos[dim]ï¼Œç”¨ _find_pg_by_ranks_and_tag() è·å–å¯¹åº” ProcessGroupã€‚\n\nget_all_groups()ï¼šè¿”å›æ‰€æœ‰ç»´åº¦çš„ group åˆ—è¡¨ï¼›\n__getitem__(dims)ï¼šåˆ‡ç‰‡æ¥å£è°ƒç”¨ _mesh_resources._get_slice_mesh_dims(...)ï¼Œåˆ›å»ºæ–°çš„å­ meshï¼Œä¿ç•™åº•å±‚ communicatorï¼Œä½†ç»´åº¦é™ã€‚\n\næ”¯æŒå•ç»´æˆ–å¤šç»´åˆ‡ç‰‡ï¼Œä¸”è¿”å›çš„ submesh é¡ºåºæŒ‰ä¼ å…¥é¡ºåºæ’åˆ— (discuss.ray.io, gemfury.com, pytorch.org)ã€‚\n\n\n\n4. from_group(...) æ–¹æ³•\n\nå¯æ¥å—å• group æˆ– group åˆ—è¡¨ï¼›\nåˆ›å»ºæ–°çš„ DeviceMesh æ—¶ä¸ä¼šè°ƒç”¨ backend åˆå§‹åŒ–ï¼›\nä¼šå¤ç”¨ç°æœ‰ ProcessGroupï¼Œå¹¶å¡«å…… _dim_group_infosï¼Œå› æ­¤ get_group(...) å°†ç›´æ¥è¿”å›ä¼ å…¥çš„å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º groupã€‚\n\n\nå››ã€å®Œæ•´å•æœº 8 å¡ Demoï¼štp=2, pp=4\nä¸‹é¢æ¼”ç¤ºå¦‚ä½•è°ƒç”¨æ‰€æœ‰æ¥å£å¹¶è¾“å‡ºç»“æœã€‚æ³¨æ„ï¼šéœ€åœ¨ torchrun --nproc_per_node=8 ä¸‹è¿è¡Œã€‚\nimport os, torch, torch.distributed as distfrom torch.distributed.device_mesh import init_device_meshdef run_device_mesh_demo():    dist.init_process_group(&quot;nccl&quot;)    # â¬‡ï¸ åˆå§‹åŒ– 2-ç»´ meshï¼špp=4, tp=2    mesh = init_device_mesh(&quot;cuda&quot;, mesh_shape=(4, 2), mesh_dim_names=(&quot;pp&quot;, &quot;tp&quot;))        # âœ… rank å’Œåæ ‡    gr = mesh.get_rank()            # å…¨å±€ rank    coord = mesh.get_coordinate()   # [pp_idx, tp_idx]    local_pp = mesh.get_local_rank(&quot;pp&quot;)    local_tp = mesh.get_local_rank(&quot;tp&quot;)        # â¬‡ï¸ mesh åŸºæœ¬ç»“æ„    total = mesh.size()    pp_size, tp_size = mesh.size(&quot;pp&quot;), mesh.size(&quot;tp&quot;)    ndim = mesh.ndim    shape = mesh.shape        # â¬‡ï¸ è·å–é€šä¿¡ç»„    pp_group = mesh.get_group(&quot;pp&quot;)    tp_group = mesh.get_group(&quot;tp&quot;)    all_groups = mesh.get_all_groups()        # â¬‡ï¸ åˆ‡ç‰‡å‡ºå­ mesh    tp_mesh = mesh[&quot;tp&quot;]    pp_mesh = mesh[&quot;pp&quot;]        # â¬‡ï¸ è¾“å‡ºç»“æœ    print(f&quot;rank=&#123;gr&#125;, coord=&#123;coord&#125;, local_pp=&#123;local_pp&#125;, local_tp=&#123;local_tp&#125;&quot;)    print(f&quot;ndim=&#123;ndim&#125;, shape=&#123;shape&#125;, total=&#123;total&#125;, pp=&#123;pp_size&#125;, tp=&#123;tp_size&#125;&quot;)    print(&quot;pp_group ranks:&quot;, dist.get_process_group_ranks(pp_group))    print(&quot;tp_group ranks:&quot;, dist.get_process_group_ranks(tp_group))    print(&quot;all_groups sizes:&quot;, [len(dist.get_process_group_ranks(g)) for g in all_groups])    print(&quot;tp_mesh ndim, shape:&quot;, tp_mesh.ndim, tp_mesh.shape)    print(&quot;pp_mesh ndim, shape:&quot;, pp_mesh.ndim, pp_mesh.shape)if __name__ == &quot;__main__&quot;:    run_device_mesh_demo()\nğŸ’¬ é¢„æœŸè¾“å‡ºï¼ˆä¾‹å¦‚ rank = 5ï¼‰ï¼š\nrank=5, coord=[2,1], local_pp=2, local_tp=1 ndim=2, shape=(4,2), total=8, pp=4, tp=2 pp_group ranks: [4,5,6,7] tp_group ranks: [5,7] all_groups sizes: [4,2] tp_mesh ndim, shape: 1 (2,) pp_mesh ndim, shape: 1 (4,)\nè¯´æ˜ï¼š - rank=5 ä½äº pipeline æ®µ 2ï¼Œtp å†…ç¼–å· 1ï¼› - pp_group åŒ…å«ä¸å…¶åŒ segment çš„ 4 å¼ å¡ï¼› - tp_group åŒ…å«åŒ segment tp ç»´åº¦çš„ä¸¤å¼ å¡ï¼› - åˆ‡ç‰‡å tp_meshã€pp_mesh æˆä¸º 1 ç»´ç»“æ„ï¼Œç”¨äºåç»­ parallelizationã€‚\n\nğŸ‘ æ€»ç»“\n\nDeviceMesh æ„å»ºè‡ªèº«é€šè¿‡ init_device_mesh() å®Œæˆåˆå§‹åŒ–ä¸å­ç»„æ‹†åˆ†ï¼›\næ¥å£å†…éƒ¨å®ç°é€»è¾‘ä¸ Group ç®¡ç†æœºåˆ¶æ¸…æ™°ã€é«˜æ•ˆï¼›\n__getitem__ä¸ºå¤šç»´å¹¶è¡Œä¸‹å­ Mesh åˆ‡ç‰‡å…³é”®å·¥å…·ï¼Œå¯¹é›†æˆ parallel APIs è‡³å…³é‡è¦ï¼›\né€šè¿‡è¯¥æœºåˆ¶ï¼Œå¯ä»¥ç®€å•åœ°ç»„ç»‡å¤æ‚çš„ hybrid-parallel pipelinesï¼ŒåŒæ—¶å……åˆ†å¤ç”¨ communicator èµ„æºå¹¶ç®€åŒ–å¼€å‘æµç¨‹ã€‚\n\n","categories":["distribute"],"tags":["devicemesh"]},{"title":"pytorch send recv ç”¨æ³•è¯¦è§£","url":"/2025/06/14/distribute/send_recv/","content":"\n1. åŸºæœ¬æ¦‚å¿µä¸è¿›ç¨‹ç»„\n2. åŸºæœ¬å¼ é‡é€šä¿¡\n3. å¯¹è±¡åˆ—è¡¨é€šä¿¡\n4. æ˜“é”™ç‚¹ä¸å¸¸è§é—®é¢˜\n5. æ‰¹é‡ç‚¹å¯¹ç‚¹é€šä¿¡æ¥å£\n6. æ€»ç»“è¡¥å……\n7. å‚è€ƒèµ„æ–™\n\n\n1. åŸºæœ¬æ¦‚å¿µä¸è¿›ç¨‹ç»„\n\ngroupï¼ˆé€šä¿¡ç»„ï¼‰ï¼šåˆ†å¸ƒå¼é€šä¿¡æ—¶çš„ã€Œå­é›†ã€ï¼Œå…è®¸åªåœ¨ä¸€éƒ¨åˆ† rank ä¹‹é—´é€šä¿¡ã€‚\nglobal rankï¼šå…¨å±€è¿›ç¨‹ç¼–å·ï¼ˆè¿›ç¨‹å¯åŠ¨æ—¶åˆ†é…çš„ç¼–å·ï¼‰ã€‚\ngroup rankï¼šç»„å†…è¿›ç¨‹ç¼–å·ï¼Œç»„å†…ç¬¬å‡ ä¸ªè¿›ç¨‹ï¼ˆä¸ global rank æ— å¿…ç„¶å¯¹åº”å…³ç³»ï¼‰ã€‚\nsrc/dstï¼šé€šä¿¡ç›®æ ‡ï¼ˆæº/ç›®çš„ï¼‰rankï¼Œæ³¨æ„ï¼šå¦‚æœæŒ‡å®š groupï¼Œè¿™é‡Œæ˜¯ç»„å†…ç¼–å·ï¼Œä¸æ˜¯å…¨å±€ç¼–å·ã€‚\n\nè¿›ç¨‹ç»„ä¸¾ä¾‹\nå‡å¦‚ group = [2, 4, 6, 8, 10]ï¼š\n\n\n\ngroup_rank\nglobal_rank\n\n\n\n\n0\n2\n\n\n1\n4\n\n\n2\n6\n\n\n3\n8\n\n\n4\n10\n\n\n\n\n2. åŸºæœ¬å¼ é‡é€šä¿¡\n2.1 send / recv / isend / irecv\nå‚æ•°è¯´æ˜\n\nsend(tensor, dst, group=None, tag=0) å‘é€ tensor åˆ°ç»„å†… rank=dst çš„è¿›ç¨‹ã€‚\nrecv(tensor, src, group=None, tag=0) ä»ç»„å†… rank=src çš„è¿›ç¨‹æ¥æ”¶ tensorã€‚\nisend/irecv å¼‚æ­¥ç‰ˆæœ¬ï¼Œè¿”å› Work å¥æŸ„ï¼Œéœ€è¦ work.wait()ã€‚\n\ntag\n\ntag æ˜¯æ¶ˆæ¯ç¼–å·/æ ‡ç­¾ï¼Œç”¨äºåŒºåˆ†å¤šæ¡å¹¶å‘æ¶ˆæ¯ï¼Œåªæœ‰ tag ä¸€è‡´æ‰èƒ½æ­£ç¡®é…å¯¹ã€‚\n\ngroup_dst/group_src\n\nä¸€èˆ¬ä¸ç”¨æ‰‹åŠ¨ä¼ ï¼Œæ¡†æ¶ä¼šæ ¹æ® dst/src å’Œ group è‡ªåŠ¨æ¨ç®—ã€‚\n\n\n2.2 é€šä¿¡æµç¨‹ç¤ºæ„å›¾\nä»¥ group = [2, 4, 6, 8, 10]ï¼Œè®© rank=2 å‘ï¼Œrank=10 æ”¶ä¸ºä¾‹ï¼š\ngraph TD    subgraph group [group: [2, 4, 6, 8, 10]]        A[&quot;global_rank=2&lt;br&gt;group_rank=0&quot;]        B[&quot;global_rank=10&lt;br&gt;group_rank=4&quot;]    end    A -- send(tensor, dst=4, group=group) --&gt; B    B -- recv(tensor, src=0, group=group) --&gt; A\n\nå‘é€ç«¯ï¼ˆglobal_rank=2ï¼Œgroup_rank=0ï¼‰ï¼šsend(tensor, dst=4, group=group)\næ¥æ”¶ç«¯ï¼ˆglobal_rank=10ï¼Œgroup_rank=4ï¼‰ï¼šrecv(tensor, src=0, group=group)\n\n\n2.3 ä»£ç å®ä¾‹\n# å‘é€ç«¯ï¼ˆglobal_rank=2ï¼‰group = dist.new_group([2, 4, 6, 8, 10])tensor = torch.tensor([123])dist.send(tensor, dst=4, group=group)   # dst=4 æ˜¯ group å†… rank=4 â†’ global_rank=10# æ¥æ”¶ç«¯ï¼ˆglobal_rank=10ï¼‰group = dist.new_group([2, 4, 6, 8, 10])tensor = torch.zeros(1, dtype=torch.int)dist.recv(tensor, src=0, group=group)   # src=0 æ˜¯ group å†… rank=0 â†’ global_rank=2print(tensor)\n\nâš ï¸ åªè¦ç”¨äº† groupï¼Œsrc/dst éƒ½æ˜¯ç»„å†… rankï¼Œä¸æ˜¯ global rankï¼\n\n\n2.4 å¼‚æ­¥é€šä¿¡ï¼ˆisend/irecvï¼‰\nwork = dist.isend(tensor, dst=4, group=group)work.wait()  # ç­‰å¾…å‘é€å®Œæˆ\nå¼‚æ­¥ recv åŒç†ã€‚\n\n3. å¯¹è±¡åˆ—è¡¨é€šä¿¡\n3.1 send_object_list / recv_object_list ç”¨æ³•\n\nç”¨äºå‘é€/æ¥æ”¶åŒ…å«ä»»æ„ Python å¯¹è±¡çš„ listï¼Œåº•å±‚é€šè¿‡åºåˆ—åŒ–å®ç°ã€‚\nå‘é€è¿‡ç¨‹æ‹†ä¸ºä¸¤æ­¥ï¼šå…ˆå‘æ¯ä¸ªå¯¹è±¡åºåˆ—åŒ–åçš„ sizeï¼Œå†å‘æ‰€æœ‰å†…å®¹æ‹¼æ¥åçš„ tensorã€‚\n\n\n3.2 å¯¹è±¡é€šä¿¡æµç¨‹å›¾\nsequenceDiagram    participant Sender    participant Receiver    Sender-&gt;&gt;Receiver: send(object_sizes_tensor)    Sender-&gt;&gt;Receiver: send(object_tensor)    Receiver-&gt;&gt;Receiver: 1. è¯»å– object_sizes_tensor    Receiver-&gt;&gt;Receiver: 2. æŒ‰ size æ‹† object_tensor    Receiver-&gt;&gt;Receiver: 3. ååºåˆ—åŒ–ä¸ºå¯¹è±¡\n\n3.3 å…¸å‹ä»£ç ç¤ºä¾‹\nå‘é€ç«¯\nobject_list = [&quot;hello&quot;, 123, [1, 2, 3]]dist.send_object_list(object_list, dst=4, group=group)\næ¥æ”¶ç«¯\nrecv_list = [None, None, None]dist.recv_object_list(recv_list, src=0, group=group)print(recv_list)  # [&#x27;hello&#x27;, 123, [1, 2, 3]]\n\n3.4 æ¥å£å®ç°æ ¸å¿ƒä»£ç \n# æ¥æ”¶ç«¯åˆ†å‰²ååºåˆ—åŒ–offset = 0for i, obj_size in enumerate(object_sizes_tensor):    obj_view = object_tensor[offset : offset + obj_size]    object_list[i] = _tensor_to_object(obj_view, obj_size, group)    offset += obj_size\n\nobject_sizes_tensor è®°å½•æ¯ä¸ªå¯¹è±¡çš„åºåˆ—åŒ–é•¿åº¦\nobject_tensor æ˜¯æ‰€æœ‰å†…å®¹æ‹¼èµ·æ¥çš„ä¸€ç»´ tensor\næŒ‰é¡ºåºåˆ‡ç‰‡å’Œååºåˆ—åŒ–ï¼Œå¡«å› object_list\n\n\n3.5 å…³äº rank_objects\n\nrank_objects æ˜¯ recv çš„è¿”å›å€¼ï¼Œè¡¨ç¤ºæ¶ˆæ¯æ¥è‡ªå“ªä¸ª rankï¼ˆä¸€èˆ¬ç­‰äº srcï¼‰\nåœ¨å¤šå¯¹å¤šé€šä¿¡æˆ– src=ANY_SOURCE æ—¶ç”¨æ¥ç¡®è®¤æ¶ˆæ¯æ¥æºï¼Œå’Œå®é™…å¯¹è±¡å†…å®¹è¿˜åŸæ— å…³\n\n\n4. æ˜“é”™ç‚¹ä¸å¸¸è§é—®é¢˜\n\nåªè¦ç”¨äº† groupï¼Œsrc/dst éƒ½æ˜¯ç»„å†… rankï¼Œä¸æ˜¯ global rankã€‚\ntag ç”¨äºåŒºåˆ†å¤šæ¡æ¶ˆæ¯ï¼Œå¿…é¡» send å’Œ recv ä¸€è‡´ã€‚\nsend_object_list/recv_object_list å¿…é¡» object_list é•¿åº¦ã€é¡ºåºä¸€è‡´ã€‚\ngroup_src/group_dst æ­£å¸¸ä¸šåŠ¡ä¸éœ€è¦è‡ªå·±ä¼ ã€‚\n\n4.1. groupã€src/dstã€group_src/group_dst å‚æ•°å…³ç³»\n\ngroup å†³å®šé€šä¿¡å­é›†ï¼Œsrc/dst å†³å®šæ”¶å‘ç›®æ ‡ç¼–å·ã€‚\nå¦‚æœæŒ‡å®š groupï¼Œåˆ™ src/dst ä¸ºç»„å†… rankï¼Œä¸æ˜¯ global rankã€‚\ngroup_src/group_dst ä¸€èˆ¬ä¸ç”¨æ‰‹åŠ¨ä¼ ï¼Œæ¡†æ¶è‡ªåŠ¨æ¨ç®—ã€‚\næ˜ å°„å…³ç³»ï¼š\n\nå…¨å±€è½¬ç»„å†…ï¼šgroup_ranks.index(global_rank)\nç»„å†…è½¬å…¨å±€ï¼šgroup_ranks[group_rank]\n\n\n\n5. æ‰¹é‡ç‚¹å¯¹ç‚¹é€šä¿¡æ¥å£\n5.1 æ¥å£ç®€ä»‹\ntorch.distributed.batch_isend_irecv æ”¯æŒåŒæ—¶å‘èµ·å¤šç»„å¼‚æ­¥ç‚¹å¯¹ç‚¹é€šä¿¡æ“ä½œï¼ˆisend/irecvï¼‰ï¼Œæ˜¾è‘—æé«˜å¤§æ‰¹é‡æ•°æ®åˆ†å‘/æ”¶é›†çš„æ•ˆç‡ã€‚ åº•å±‚æ”¯æŒ NCCLã€Glooã€UCC ç­‰åˆ†å¸ƒå¼åç«¯ï¼Œå¸¸ç”¨äºåˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ çš„ pipeline/é€šä¿¡ pattern ä¼˜åŒ–ã€‚\nå‡½æ•°ç­¾å\ntorch.distributed.batch_isend_irecv(p2p_op_list: list[P2POp]) -&gt; list[Work]\n\np2p_op_listï¼šä¸€ç»„ torch.distributed.P2POp å®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹æè¿°ä¸€æ¬¡ isend/irecvã€‚\nè¿”å›ï¼šæ‰€æœ‰æ“ä½œçš„ request å¥æŸ„ï¼ˆWork å¯¹è±¡ï¼‰åˆ—è¡¨ï¼Œå¯é€šè¿‡ .wait() åŒæ­¥ã€‚\n\n\n5.2 å…¸å‹ä½¿ç”¨åœºæ™¯\n\nå¤§æ‰¹é‡ç‚¹å¯¹ç‚¹é€šä¿¡ï¼Œä¾‹å¦‚ pipeline å¹¶è¡Œã€ç¯å½¢ allreduce æ‰‹å†™ä¼˜åŒ–ç­‰åœºæ™¯ã€‚\næ”¯æŒ isend/irecv æ··åˆï¼Œèƒ½æ‰¹é‡æå‡ååé‡ã€‚\n\n\n5.3 è°ƒç”¨æµç¨‹ä¸å‚æ•°è¯´æ˜\nP2POp ç”¨æ³•\næ¯ä¸ª P2POp å®šä¹‰ä¸€æ¬¡é€šä¿¡æ“ä½œï¼Œå¦‚ä¸‹ï¼š\nP2POp(op, tensor, peer, group=None, tag=0)\n\nopï¼šæ“ä½œç±»å‹ï¼ˆdist.isend æˆ– dist.irecvï¼‰\ntensorï¼šè¦å‘é€/æ¥æ”¶çš„ tensor\npeerï¼šç›®æ ‡ peer çš„ç¼–å·ï¼ˆç»„å†… rankï¼‰\ngroupï¼ˆå¯é€‰ï¼‰ï¼šé€šä¿¡ç»„ï¼ˆé»˜è®¤ä¸º worldï¼‰\ntagï¼ˆå¯é€‰ï¼‰ï¼šæ¶ˆæ¯ç¼–å·/æ ‡ç­¾\n\n\n5.4 ä»£ç å®ä¾‹\nå‡è®¾ world_size=2ï¼Œrank 0 å’Œ rank 1 åšä¸€ä¸ªç¯å½¢é€šä¿¡ï¼š\nimport torchimport torch.distributed as distrank = dist.get_rank()world_size = dist.get_world_size()send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rankrecv_tensor = torch.zeros(2, dtype=torch.float32)send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1) % world_size)recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size) % world_size)reqs = dist.batch_isend_irecv([send_op, recv_op])for req in reqs:    req.wait()print(f&quot;Rank &#123;rank&#125; æ”¶åˆ°: &#123;recv_tensor&#125;&quot;)\nè¿è¡Œç»“æœï¼š\nRank 0 æ”¶åˆ°: tensor([2., 3.])Rank 1 æ”¶åˆ°: tensor([0., 1.])\n\n5.5 é€šä¿¡æµç¨‹å›¾\nsequenceDiagram    participant Rank0    participant Rank1    Rank0-&gt;&gt;Rank1: isend(send_tensor, dst=1)    Rank1-&gt;&gt;Rank0: isend(send_tensor, dst=0)    Rank0-&gt;&gt;Rank0: irecv(recv_tensor, src=1)    Rank1-&gt;&gt;Rank1: irecv(recv_tensor, src=0)    Note over Rank0,Rank1: batch_isend_irecv([send_op, recv_op])&lt;br&gt;å¹¶å‘å‘èµ·é€šä¿¡å¹¶ç­‰å¾…å®Œæˆ\n\n5.6 é‡è¦æ³¨æ„äº‹é¡¹\n\næ³¨æ„\n\nå¦‚æœä½¿ç”¨ NCCL åç«¯ï¼Œå¿…é¡»æå‰ç”¨ torch.cuda.set_device è®¾ç½®å¥½å½“å‰ GPUï¼\nå¦‚æœè¿™æ˜¯æŸä¸ª group çš„ç¬¬ä¸€æ¬¡é€šä¿¡ï¼Œgroup é‡Œçš„æ‰€æœ‰ rank å¿…é¡»éƒ½è°ƒç”¨ batch_isend_irecvï¼Œå¦åˆ™è¡Œä¸ºæœªå®šä¹‰ã€‚\nä»¥ååªè¦ä¸æ˜¯ç¬¬ä¸€æ¬¡ collectiveï¼Œå…è®¸åªç”¨éƒ¨åˆ† rank å‚ä¸ã€‚\n\n\n\n5.7 æºç å®ç°è¦ç‚¹\n\nè‡ªåŠ¨åˆ¤æ–­é€šä¿¡åç«¯æ˜¯å¦æ”¯æŒæ“ä½œåˆå¹¶ï¼ˆcoalescingï¼‰ï¼Œå¦‚ NCCL ä¼šåœ¨åŒä¸€ä¸ªä¸Šä¸‹æ–‡ä¸‹æ‰¹é‡å¯åŠ¨ï¼Œæå‡æ€§èƒ½ã€‚\nè¿”å›æ‰€æœ‰ requestï¼ˆWorkï¼‰å¯¹è±¡ï¼Œç”¨æˆ·å¯ wait()ã€‚\n\n\n5.8 API æ–‡æ¡£é“¾æ¥\n\nPyTorch å®˜æ–¹ batch_isend_irecv æ–‡æ¡£\nP2POp å®˜æ–¹è¯´æ˜\n\n\n6. æ€»ç»“è¡¥å……\n\nå¼ é‡ç‚¹å¯¹ç‚¹é€šä¿¡ï¼šsend/recv/isend/irecv/batch_isend_irecv\nå¯¹è±¡é€šä¿¡ï¼šsend_object_list/recv_object_list\næ‰¹é‡ç‚¹å¯¹ç‚¹é€šä¿¡èƒ½æå¤§æå‡ pipeline é€šä¿¡æ•ˆç‡\nç»Ÿä¸€è¿”å› Work å¥æŸ„ï¼Œæ”¯æŒåŒæ­¥æˆ–å¼‚æ­¥\ngroup/src/dst ä½¿ç”¨æ–¹å¼åŒä¸Šæ–‡æè¿°\n\n\n7. å‚è€ƒèµ„æ–™\n\nPyTorch Distributed å®˜æ–¹æ–‡æ¡£\nPyTorch distributed_c10d.py æºç \nMermaid Live Editor\n\n\n","categories":["distribute"],"tags":["send recv"]},{"title":"PyTorch Shard å®ç°ä¸åˆ†æ","url":"/2025/06/20/distribute/shard/","content":"\n1. _split_tensoråˆ†æ\n1.1 ä»£ç å®ç°æµç¨‹å›¾ï¼ˆMermaidï¼‰\nflowchart TD  A[&quot;è¾“å…¥ï¼štensor, num_chunks, with_padding, contiguous&quot;] --&gt; B&#123;&quot;dim â‰¤ tensor.ndim?&quot;&#125;  B -- å¦ --&gt; E[&quot;AssertionError æŠ›å‡º&quot;]  B -- æ˜¯ --&gt; C[&quot;è°ƒç”¨ torch.chunk æ²¿ dim åˆ†å—&quot;]  C --&gt; D[&quot;tensor_list, è®¡ç®— num_empty_tensors = num_chunks - len(tensor_list)&quot;]  D --&gt; F&#123;&quot;æ— éœ€ padding æˆ– å‡åŒ€å¯åˆ†?&quot;&#125;  F -- æ˜¯ --&gt; G[&quot;(å¯é€‰) å¯¹æ¯å—è°ƒç”¨ .contiguous()&quot;]  G --&gt; H[&quot;è°ƒç”¨ fill_empty_tensor_to_shards è¡¥ç©º shard&quot;]  H --&gt; I[&quot;è¿”å› shards åˆ—è¡¨ å’Œ ç©º pad_sizes []&quot;]  F -- å¦ --&gt; J[&quot;è®¡ç®— full_chunk_size = ceil(dim_size / num_chunks)&quot;]  J --&gt; K[&quot;æ”¶é›†åŸå§‹ chunk_sizes&quot;]  K --&gt; L[&quot;pad_sizes = full_chunk_size - chunk_size&quot;]  L --&gt; M[&quot;è°ƒç”¨ fill_empty_tensor_to_shards è¡¥ç©º shard&quot;]  M --&gt; N[&quot;å¯¹æ¯ä¸ª shardï¼šè‹¥ pad_size &gt; 0ï¼Œåˆ™ pad_tensor(shard, dim, pad_size)&quot;]  N --&gt; O[&quot;(å¯é€‰) shard.contiguous()&quot;]  O --&gt; P[&quot;æ”¶é›† shard_list å’Œ pad_sizes&quot;]  P --&gt; Q[&quot;è¿”å› shard_list å’Œ pad_sizes&quot;]\n\n1.2 å…³é”®ç‚¹è¯¦è§£\nğŸ§  ä¸ºä»€ä¹ˆè¦ Paddingï¼Ÿ\nç”¨äºä¿è¯åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ï¼ˆæ¯”å¦‚ scatterã€all_gather ç­‰ collective æ“ä½œï¼‰æ¯ä¸ª rank çš„ shard å¤§å°ä¸€è‡´ï¼Œé¿å…å› ä¸ºå°ºå¯¸ä¸å¯¹é½å¯¼è‡´é€šä¿¡å¤±è´¥ã€‚åªæœ‰ tensor.size(dim) % num_chunks â‰  0 ä¸” with_padding=True æ—¶ï¼Œæ‰ä¼šè¿›è¡Œ paddingã€‚\nğŸ§© fill_empty_tensor_to_shards\ntorch.chunk åœ¨å°ºå¯¸è¾ƒå°æˆ– num_chunks æ›´å¤§æ—¶ä¸ä¼šè¾“å‡ºç©º tensorã€‚è¯¥å‡½æ•°ç”¨äºè¡¥å…¨ï¼šåœ¨ tensor_list å°‘äº num_chunks æ—¶ï¼Œè¡¥å……å½¢çŠ¶åˆæ³•ä½† dim ä¸Šä¸º 0 çš„ç©º tensorï¼Œä½¿ shard æ•°ç›®ä¸€è‡´ï¼Œä¾¿äºåç»­ç»Ÿä¸€å¤„ç†ã€‚\nğŸ§¼ pad_tensor\nè‹¥å½“å‰ shard å°äº full_chunk_sizeï¼Œåˆ™åœ¨æŒ‡å®šç»´åº¦æœ«å°¾è¡¥é›¶ï¼Œç¡®ä¿æ‰€æœ‰ shard çš„å½¢çŠ¶ä¸€è‡´ã€‚\nğŸ§± contiguous\nä¸ºæå‡å†…å­˜è¿è´¯æ€§å’Œé€šä¿¡æ•ˆç‡ï¼Œå¯è°ƒç”¨ .contiguous() é‡æ’å†…å­˜å¸ƒå±€ã€‚\n\n1.3 å®é™…è°ƒç”¨ç¤ºä¾‹ï¼ˆéœ€ Paddingï¼‰\nä»¥ä¸‹ä¸ºæ— æ³•å‡åŒ€åˆ†ç‰‡ï¼Œå›  num_chunks=4 è€Œè§¦å‘ pad çš„åœºæ™¯ï¼š\nimport torchfrom torch.distributed.tensor.placement_types import Shard# æ„é€ å¼ é‡tensor = torch.arange(1, 13).reshape(2, 6)  # shape [2, 6]# åœ¨ dim=1 ä¸Šæ‹†ä¸º 4 ä»½ï¼Œä¸æ•´é™¤å°†è§¦å‘ paddingsharder = Shard(dim=1)shards, pad_sizes = sharder._split_tensor(tensor, num_chunks=4, with_padding=True)print(&quot;Pad sizes:&quot;, pad_sizes)for i, (sh, pad) in enumerate(zip(shards, pad_sizes)):    print(f&quot;Shard &#123;i&#125; shape: &#123;tuple(sh.shape)&#125;, pad: &#123;pad&#125;&quot;)    print(sh)\nâœ… é¢„æœŸç»“æœ\n\ntensor.size(1)=6, num_chunks=4 â‡’ full_chunk_size = ceil(6/4) = 2\ntorch.chunk ä¼šå‡º 4 å—ï¼Œä½†æœ€åä¸€ä¸¤å—å¯èƒ½ä¸º empty\npad_sizes å¯èƒ½ä¸º [0, 0, 0, 2]\næœ€ç»ˆæ¯å—å¤§å°éƒ½æ˜¯ [2] (dim=1)ï¼Œpadding è¡¥é½\n\nPad sizes: [0, 0, 0, 2]Shard 0 shape: (2, 2), pad: 0tensor([[1, 2],        [7, 8]])Shard 1 shape: (2, 2), pad: 0tensor([[ 3,  4],        [ 9, 10]])Shard 2 shape: (2, 2), pad: 0tensor([[ 5,  6],        [11, 12]])Shard 3 shape: (2, 2), pad: 2tensor([[0, 0],        [0, 0]])\n\n1.4 æ€»ç»“\n\n_split_tensor çš„ä½œç”¨æ˜¯å°†ä¸€ä¸ª Tensor æ²¿æŒ‡å®šç»´åº¦åˆ‡åˆ†ä¸ºå›ºå®šä»½æ•°ï¼Œå¹¶åœ¨ ä¸èƒ½æ•´é™¤æ—¶è‡ªåŠ¨è¡¥é½ã€‚\nå®ƒä¿éšœäº†å„ shard åœ¨é€šä¿¡é˜¶æ®µå°ºå¯¸ä¸€è‡´ï¼Œé€‚ç”¨äºåˆ†å¸ƒå¼å¼ é‡å¹¶è¡Œåœºæ™¯ã€‚\nå®é™…ä»£ç é€šè¿‡ torch.chunkã€fill_empty_tensor_to_shardsã€pad_tensor ç­‰æ‰‹æ®µï¼Œè½»æ¾å®ç°è¿™ä¸€ç›®æ ‡ã€‚\n\n\n"},{"title":"PyTorch åˆ†å¸ƒå¼ TCPStore Rendezvous æœºåˆ¶","url":"/2025/06/14/distribute/tcpstore_rendezvous/","content":"\nğŸ§  èƒŒæ™¯æ¦‚è¿°\n\nç›®æ ‡ï¼šåœ¨ init_process_group ä¸­å®ç°è·¨è¿›ç¨‹æ³¨å†Œã€æ’åºåŠ barrier åŒæ­¥ï¼Œä¸º NCCL/Gloo é€šä¿¡ç»„æ„å»ºåˆ›å»ºä¸€è‡´ä¸Šä¸‹æ–‡ã€‚\næ—¶åºï¼šæ‰€æœ‰ set/get/wait æ“ä½œå‡å‘ç”Ÿåœ¨ NCCL é€šä¿¡åˆå§‹åŒ–ä¹‹å‰ï¼ˆå³ rendezvous é˜¶æ®µï¼‰ã€‚\næœºåˆ¶ï¼šsocket å®¢æˆ·ç«¯â€”æœåŠ¡å™¨æ¨¡å‹ + backend æ§åˆ¶åŒæ­¥é€»è¾‘ã€‚\n\n\n1. æ¶ˆæ¯åè®®æ ¼å¼\nå®¢æˆ·ç«¯å‘ master å‘é€çš„åŒ…æ ¼å¼ä¸ºï¼š\n\\[4â€¯B æ€»é•¿åº¦]\\[1â€¯B æ“ä½œç ]\\[4â€¯B key\\_len]\\[4â€¯B value\\_len]\\[key]\\[value]\n\næ€»é•¿åº¦ï¼šç½‘ç»œå­—èŠ‚åºï¼Œä¸å«è‡ªèº«ï¼›\næ“ä½œç ï¼š1=SET, 2=GET, 3=WAITï¼›\nkey_len, value_lenï¼šåç»­å­—æ®µé•¿åº¦ï¼›\nkey, valueï¼šå®é™…æ•°æ®ï¼›\nMaster è§£æåï¼Œå›å¤ï¼šOK / value å†…å®¹ / READY ç­‰ã€‚\n\n\n2. Rendezvous é˜¶æ®µæµç¨‹ï¼ˆ2 æœºï¼Œ4 å¡ eachï¼Œèšç„¦ rank1 &amp; rank5ï¼‰\nflowchart TB  subgraph A[&quot;Machine A (rank0-3)&quot;]    master[&quot;TCPStoreBackend (master)&quot;]    r1[Worker rank1]    master --- r1  end  subgraph B[&quot;Machine B (rank4-7)&quot;]    r5[Worker rank5]    master --- r5  end  r1 --&gt;|SET key rank1_addr| master  r5 --&gt;|SET key rank5_addr| master  r1 --&gt;|WAIT  rendezvous_done| master  r5 --&gt;|WAIT  rendezvous_done| master  %% Server: waits until all ranks set, then:  master --&gt;|write READY| r1  master --&gt;|write READY| r5  %% å®Œæˆ WAIT è¿”å›ï¼Œè¿›å…¥ NCCL åˆå§‹åŒ–  r1 --&gt;|recv READY â†’ NCCL init| NCCL_1[NCCL Init rank1]  r5 --&gt;|recv READY â†’ NCCL init| NCCL_5[NCCL Init rank5]\nğŸ§© æ­¥éª¤è§£æ\n\nMaster åœ¨ç«¯å£ï¼ˆå¦‚ 29500ï¼‰ä¾¦å¬ï¼Œæ¥æ”¶è¿æ¥ï¼›\nrank1 / rank5 åˆ†åˆ«å‘é€ SETï¼ˆæ³¨å†Œåœ°å€ï¼‰ï¼›\néšåå‘é€ WAIT(\"rendezvous_done\")ï¼ŒSocket å¤„äºé˜»å¡çŠ¶æ€ï¼›\nMaster æ”¶é›†æ‰€æœ‰ 8 ä¸ª rank çš„ SET åï¼Œéå† wait é˜»å¡çš„è¿æ¥ï¼Œé€ä¸€å†™å…¥ READYï¼›\nWorker æ”¶åˆ° READYï¼Œé€€å‡ºé˜»å¡ï¼Œè¿›å…¥ NCCL åˆå§‹åŒ–é˜¶æ®µï¼›\néšååœ¨è¿™ä¸€é˜¶æ®µå†…ï¼šäº¤æ¢ ncclUniqueId (via store), è°ƒç”¨ ncclCommInitRank æ„å»ºé€šä¿¡ç»„ (github.com, pytorch.org)ã€‚\n\n\n3. Backend ç»†èŠ‚å¯¹æ¯”\n\n\n\n\n\n\n\n\nBackend\nI/O æ¨¡å‹\nç‰¹ç‚¹ä¸é€‚åº”æ€§\n\n\n\n\nç»å…¸ TCPStoreBackend\naccept() + per-conn é˜»å¡/POLL\nç®€å•ï¼Œè¿æ¥è¾ƒå¤šæ—¶æ‰©å±•æ€§å·®\n\n\nlibuv å¼‚æ­¥ Backend\nå•çº¿ç¨‹ event-loop, readable/writeable\né»˜è®¤å¯ç”¨ï¼ˆv2.4+ï¼‰ï¼Œé«˜å¹¶å‘æ›´ä¼˜ (docs.pytorch.org)\n\n\n\n\nlibuv backend ä½¿ç”¨ uv_read_start è‡ªåŠ¨åˆ†å—è¯»å–ï¼Œæ ¹æ® header æ§åˆ¶æ‹¼åŒ…ï¼›\næ³¨å†Œ WAIT æ—¶ï¼Œå°† conn ä¿å­˜åœ¨ map ä¸­ï¼Œä¸ç«‹å³å›å†™ï¼›å½“æ¡ä»¶æ»¡è¶³ï¼Œè§¦å‘ uv_write() â†’ uv_write_cb å®ç°å”¤é†’ã€‚\n\n\n4. partial-key WAIT æœºåˆ¶\n\nå®¢æˆ·ç«¯å¯ä»¥æ‰§è¡Œ store.wait([\"kA\", \"kB\"])ï¼›\nMaster å°†æ­¤ç­‰å¾…ç™»è®°è‡³ MultiWaitRegistryï¼›\nå½“ æ‰€æœ‰ç›¸å…³ key å‡è¢« SET åï¼Œæ‰ç»Ÿä¸€å‘è¯¥è¿æ¥å†™ READYï¼Œè§¦å‘å”¤é†’ã€‚\n\n\n5. â€œå¹¿æ’­ READYâ€ çš„å®ç°æœºåˆ¶\n\nä¸æ˜¯é€šè¿‡ NCCL/Gloo broadcast ç®—å­ï¼›\nMaster éå†æŒ‚èµ·çš„ WAIT socketsï¼Œé€ä¸ªå†™ READYï¼›\nä¸º rendezvous è¿‡ç¨‹è‡ªèº«æä¾›åŒæ­¥æœºåˆ¶ï¼Œé€šä¿¡ç»„å°šæœªåˆ›å»ºã€‚\n\n\n6. æ—¶é—´çº¿æ¦‚è§ˆ\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ SET/WAIT via TCP Store   â”‚  # rendezvous é˜¶æ®µâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â†“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ recv READY â†’ wait returnsâ”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â†“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ NCCL Init                â”‚  # è°ƒç”¨ ncclUniqueId, CommInitRankâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â†“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Collective Ops (DDP)     â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… æ€»ç»“è¦ç‚¹\n\næ ‡æ³¨ rank1 / rank5 çš„æµç¨‹å›¾ï¼Œæ›´ç›´è§‚ï¼›\nSET + WAIT æ“ä½œå…¨éƒ¨å‘ç”Ÿäº rendezvous é˜¶æ®µï¼Œè§å›¾ï¼›\nMaster â€œå¹¿æ’­ READYâ€ æ˜¯ socket å†™æ“ä½œï¼Œä¸æ˜¯é€šä¿¡åº“å¹¿æ’­ï¼›\nNCCL åˆå§‹åŒ–åœ¨ rendezvous å®Œæˆåè¿›è¡Œï¼›\nlibuv backend æä¾›æ›´é«˜æ•ˆ I/O å¤„ç†åŠ message æ‹¼æ¥å¤„ç†èƒ½åŠ› (docs.pytorch.org, pytorch.org, github.com)ã€‚\n\n\n","categories":["distribute"],"tags":["tcpstore"]},{"title":"Ubuntu å¸¸è§ Shell å‘½ä»¤","url":"/2025/08/17/other/shell/","content":"\n\n1. ç£ç›˜å ç”¨ä¸æ’åºï¼ˆdu/sortï¼‰\nå¸¸ç”¨å†™æ³•\n# æŒ‰â€œå½“å‰ç›®å½•çš„ç›´æ¥å­é¡¹â€æ±‡æ€»ï¼ˆäººç±»å¯è¯»ï¼‰ï¼Œå¹¶æŒ‰å¤§å°å€’åºdu -h --max-depth=1 . | sort -hr# ä»…ç»Ÿè®¡æ¯ä¸ªæ¡ç›®æ€»å¤§å°ï¼ˆä¸æ˜¾ç¤ºå­å±‚çº§ï¼‰ï¼Œå¹¶å¯¹æ¡ç›®æ’åºdu -sh -- * | sort -h\n2. æ–‡æœ¬æœç´¢ï¼ˆgrepï¼‰\nåŸºç¡€\ngrep &quot;keyword&quot; file.txt          # åœ¨å•ä¸ªæ–‡ä»¶ä¸­æŸ¥æ‰¾grep -n &quot;keyword&quot; file.txt       # æ˜¾ç¤ºè¡Œå·grep -i &quot;keyword&quot; file.txt       # å¿½ç•¥å¤§å°å†™\nç›®å½•é€’å½’ä¸ä¸Šä¸‹æ–‡\ngrep -rin --color=auto &quot;keyword&quot; .      # é€’å½’ã€å¿½ç•¥å¤§å°å†™ã€è¡Œå·ã€é«˜äº®grep -nC 3 &quot;keyword&quot; file.txt           # ä¸Šä¸‹å„ 3 è¡Œgrep -nA 2 &quot;keyword&quot; file.txt           # å 2 è¡Œgrep -nB 2 &quot;keyword&quot; file.txt           # å‰ 2 è¡Œ\nç²¾ç¡®åŒ¹é…ä¸æ­£åˆ™\ngrep -rw &quot;\\&lt;token\\&gt;&quot; .                  # æŒ‰â€œæ•´è¯â€åŒ¹é…grep -E &quot;err(or)?|fail(ed)?&quot; app.log    # æ‰©å±•æ­£åˆ™grep -rF &quot;literal*text&quot; .               # çº¯å­—ç¬¦ä¸²ï¼ˆä¸å½“æ­£åˆ™ï¼‰ï¼Œæ›´å¿«\næ’é™¤æ–‡ä»¶/ç›®å½•\ngrep -rin &quot;keyword&quot; . \\  --exclude-dir=&#123;.git,node_modules,dist&#125; \\  --exclude=&quot;*.min.js&quot;\n\n3. æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾ï¼ˆfind/locateï¼‰\nfindï¼šçµæ´»ä½†å®æ—¶æ‰«æï¼ˆæ…¢ï¼‰\n# æŒ‰æ–‡ä»¶åï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰find /path -type f -iname &quot;*name*&quot;# é™åˆ¶æœç´¢æ·±åº¦find . -maxdepth 2 -type d -name &quot;build&quot;# æŸ¥æ‰¾å¤§æ–‡ä»¶ï¼ˆ&gt; 100MBï¼‰å¹¶æŒ‰å¤§å°é™åºåˆ—å‡ºå‰ 20 ä¸ªfind /var -type f -size +100M -printf &#x27;%s\\t%p\\n&#x27; | sort -nr | head -20# æŸ¥æ‰¾æœ€è¿‘ 1 å¤©å†…ä¿®æ”¹çš„æ–‡ä»¶find . -type f -mtime -1# å¯¹ç»“æœæ‰§è¡Œå‘½ä»¤ï¼ˆå®‰å…¨å¤„ç†ç©ºæ ¼ï¼‰find . -type f -name &quot;*.log&quot; -print0 | xargs -0 gzip\n\nè·³è¿‡ç³»ç»Ÿç›®å½•ä¸”å‹åˆ¶æŠ¥é”™\n\nfind / \\( -path /proc -o -path /sys -o -path /run \\) -prune -o \\  -type f -name &quot;*.conf&quot; -print 2&gt;/dev/null\nlocate/plocateï¼šåŸºäºç´¢å¼•ï¼ˆå¿«ï¼‰\nsudo apt-get install -y plocatesudo updatedb                 # é€šå¸¸è‡ªåŠ¨å®šæ—¶æ›´æ–°locate filename_or_pattern\n\n4. å¸¸è§ç½‘ç»œå·¥å…·å®‰è£…åŒ…\n# pingsudo apt-get install -y iputils-ping# ifconfigï¼ˆè€å·¥å…·ï¼Œä»å¸¸è§ï¼‰sudo apt-get install -y net-tools# ç°ä»£æ›¿ä»£ï¼šipï¼ˆé€šå¸¸å·²è‡ªå¸¦äº iproute2ï¼‰ip addrip linkip route# killallsudo apt-get install -y psmisc\n\n5. è¿›ç¨‹æŸ¥æ€ï¼ˆkill/pkill/killallï¼‰\nps -ef | grep python3 | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9\næ›´å®‰å…¨çš„åšæ³•\n# ä¼˜é›…ç»ˆæ­¢ï¼ˆSIGTERMï¼‰ï¼›æ—  PID æ—¶ä¸æ‰§è¡Œ (-r)pgrep -f python3 | xargs -r kill# ç›´æ¥æŒ‰åç§°åŒ¹é…ï¼ˆä¼˜é›…ç»ˆæ­¢ï¼‰ï¼Œå¿…è¦æ—¶å† -9pkill -f python3pkill -9 -f python3# é¿å…åŒ¹é…åˆ° grep è‡ªèº«ps -ef | grep &#x27;[p]ython3&#x27; | awk &#x27;&#123;print $2&#125;&#x27; | xargs -r kill\n\nå»ºè®®å…ˆå°è¯• SIGTERMï¼ˆé»˜è®¤ï¼‰ï¼Œæ— å“åº”å†ç”¨ SIGKILLï¼ˆ-9ï¼‰ã€‚\n\n\n6. é«˜é¢‘å‘½ä»¤æ¸…å•ä¸ç¤ºä¾‹\nç³»ç»Ÿ/èµ„æº\ntop                     # å®æ—¶æ¦‚è§ˆhtop                    # æ›´å‹å¥½ï¼ˆéœ€ï¼šsudo apt-get install -y htopï¼‰free -h                 # å†…å­˜df -h                   # ç£ç›˜åˆ†åŒºå®¹é‡du -sh * | sort -h      # ç›®å½•å ç”¨uname -a                # å†…æ ¸ä¿¡æ¯lsb_release -a          # å‘è¡Œç‰ˆä¿¡æ¯\nè¿›ç¨‹/ç½‘ç»œ\nps aux | lesspstree -p               # è¿›ç¨‹æ ‘ï¼ˆéœ€ï¼šsudo apt-get install -y psmiscï¼‰lsof -i :8080           # ç«¯å£å ç”¨ï¼ˆéœ€ï¼šsudo apt-get install -y lsofï¼‰ss -lntp                # ç›‘å¬ç«¯å£ + è¿›ç¨‹\næ–‡æœ¬/æ—¥å¿—\nless file.logtail -f file.logwc -l file.txtsort file | uniq -c | sort -nrcut -d&#x27;,&#x27; -f1,3 file.csvsed -n &#x27;1,20p&#x27; file.txtawk -F: &#x27;&#123;print $1,$3&#125;&#x27; /etc/passwd\næ–‡ä»¶/å½’æ¡£/ä¼ è¾“\ntar -czf logs.tgz logs/        # å‹ç¼©tar -xzf logs.tgz              # è§£å‹zip -r src.zip src/            # zipï¼ˆéœ€ï¼šsudo apt-get install -y zip unzipï¼‰rsync -av --progress src/ dst/scp file user@host:/path/\næƒé™/é“¾æ¥\nchmod +x run.shchown user:group fileln -s /real/path link_name\næœåŠ¡ä¸æ—¥å¿—ï¼ˆsystemdï¼‰\nsystemctl status nginxsudo systemctl start nginxjournalctl -u nginx --since &quot;1 hour ago&quot;\nå…¶ä»–\nwhich python3command -v nodedate &quot;+%F %T&quot;nohup python3 app.py &gt;out.log 2&gt;&amp;1 &amp;tmux new -s work              # ç»ˆç«¯å¤ç”¨ï¼ˆéœ€ï¼šsudo apt-get install -y tmuxï¼‰\n\n7. å°è´´å£«ä¸å¸¸è§å‘\n\néšè—æ–‡ä»¶ï¼š* ä¸åŒ¹é…éšè—é¡¹ï¼Œå¯ç”¨ .* * ç»„åˆæˆ–å¼€å¯ dotglobã€‚\né˜²æ­¢å‚æ•°è¢«å½“ä½œé€‰é¡¹ï¼šå½“æ–‡ä»¶åä»¥ - å¼€å¤´æ—¶åŠ  --ï¼Œå¦‚ rm -- -weirdfileã€‚\nxargs å®‰å…¨ï¼šäºŒè¿›åˆ¶æ–‡ä»¶/ç©ºæ ¼ç”¨ -0 é…åˆ -print0ï¼›æ— ç»“æœæ—¶ä¸æ‰§è¡Œç”¨ -rã€‚\nä¼˜é›…åœæœåŠ¡ä¼˜å…ˆï¼škill -TERM â†’ ä¸è¡Œå† kill -KILLã€‚\næƒé™ï¼šç³»ç»Ÿç›®å½•æ“ä½œæ…ç”¨ sudoï¼Œå†™å‰å…ˆ ls/du/stat ç¡®è®¤ã€‚\ngrep æ­£åˆ™ vs å­—ç¬¦ä¸²ï¼šçº¯æ–‡æœ¬åŒ¹é…æ›´ç¨³æ›´å¿«ç”¨ -Fã€‚\nfind æ€§èƒ½ï¼šå¤§ç›®å½•ç”¨ -maxdepth é™åˆ¶å±‚çº§æˆ–æ”¹ç”¨ locate/plocateã€‚\n\n\n","categories":["other"],"tags":["shell"]},{"title":"Ubuntu æ­å»ºæŠ€æœ¯åšå®¢æŒ‡å—","url":"/2025/06/14/other/web_init/","content":"\n1. å®‰è£… Hexo ç¯å¢ƒ\n2. é€‰æ‹©ä¸é…ç½® Hexo ä¸»é¢˜\n3. æ’°å†™ä¸ç®¡ç†åšå®¢å†…å®¹\n4. SEO ä¼˜åŒ–\n5. åšå®¢éƒ¨ç½²\n6. ç»´æŠ¤ä¸ä¼˜åŒ–\n\næœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº†å¦‚ä½•åœ¨ Ubuntu æœåŠ¡å™¨ä¸Šæ­å»ºå¹¶éƒ¨ç½²ä¸€ä¸ª Hexo æŠ€æœ¯åšå®¢ï¼ŒåŒ…æ‹¬ä»ç¯å¢ƒå®‰è£…åˆ°åæœŸç»´æŠ¤çš„å®Œæ•´æ­¥éª¤ã€‚\n1. å®‰è£… Hexo ç¯å¢ƒ\næ­å»º Hexo åšå®¢é¦–å…ˆéœ€è¦å®‰è£… Node.jsï¼ˆHexo åŸºäº Node.jsï¼‰ã€npmã€Git ä»¥åŠ Hexo CLI å·¥å…·ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤é…ç½®ç¯å¢ƒï¼š\nå®‰è£… Node.js å’Œ npmï¼š\nåœ¨ Ubuntu ä¸Šï¼Œé€šè¿‡åŒ…ç®¡ç†å™¨æˆ– Node å®˜æ–¹ä»“åº“å®‰è£… Node.jsã€‚å»ºè®®å®‰è£… LTS ç‰ˆæœ¬ï¼ˆå¦‚ Node 14+ï¼‰ã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ·»åŠ  NodeSource ä»“åº“å¹¶å®‰è£… Node.jsï¼š\ncurl -sL https://deb.nodesource.com/setup_18.x | sudo -E bash -sudo apt-get install -y nodejs\nå®‰è£…å®Œæˆåï¼Œæ£€æŸ¥ç‰ˆæœ¬ä»¥ç¡®ä¿ Node æ­£å¸¸å¯ç”¨ï¼š\nnode -v  # åº”è¿”å›ç±»ä¼¼ v18.20.6 çš„ç‰ˆæœ¬å·npm -v   # éªŒè¯ npm æ˜¯å¦æ­£å¸¸å®‰è£…\nå®‰è£… Gitï¼š\nGit æ˜¯ Hexo éƒ¨ç½²å’Œå¤‡ä»½çš„å¸¸ç”¨å·¥å…·ã€‚Ubuntu é€šå¸¸é¢„è£… Gitï¼Œè‹¥æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œï¼š\nsudo apt-get install -y git\nå®‰è£…åï¼Œé…ç½® Git çš„å…¨å±€ç”¨æˆ·åå’Œé‚®ç®±ï¼š\ngit config --global user.name &quot;Your Name&quot;git config --global user.email &quot;youremail@example.com&quot;\nå®‰è£… Hexo CLIï¼š\né€šè¿‡ npm å…¨å±€å®‰è£… Hexo CLIï¼š\nsudo npm install -g hexo-cli\nå®‰è£…æˆåŠŸåï¼Œé€šè¿‡ hexo -v æ£€æŸ¥ç‰ˆæœ¬ï¼Œç¡®ä¿ Hexo CLI å¯ç”¨ã€‚\nåˆå§‹åŒ– Hexo åšå®¢ï¼š\né€‰æ‹©åšå®¢æ–‡ä»¶å¤¹ï¼ˆä¾‹å¦‚ /var/www/hexo æˆ–å½“å‰ç”¨æˆ·ä¸»ç›®å½•ä¸‹çš„ my-blog æ–‡ä»¶å¤¹ï¼‰ï¼Œå¹¶åœ¨è¯¥ç›®å½•ä¸‹åˆå§‹åŒ– Hexo åšå®¢ï¼š\nsudo mkdir -p /var/www/hexo &amp;&amp; sudo chown $USER:$USER /var/www/hexocd /var/www/hexohexo initnpm install\nåˆå§‹åŒ–å®Œæˆåï¼ŒHexo ä¼šç”Ÿæˆé»˜è®¤çš„åšå®¢ç»“æ„ï¼ŒåŒ…æ‹¬ _config.yml é…ç½®æ–‡ä»¶ã€scaffolds/ æ¨¡æ¿ç›®å½•ã€source/ å†…å®¹ç›®å½•å’Œ themes/ ä¸»é¢˜ç›®å½•ç­‰ã€‚å¯ä»¥é€šè¿‡è¿è¡Œ hexo server é¢„è§ˆæœ¬åœ°åšå®¢ã€‚\nå¼€å¯é˜²ç«å¢™ï¼š\nä¸ºäº†ç¡®ä¿æœåŠ¡å™¨å®‰å…¨ï¼Œå»ºè®®å¼€å¯é˜²ç«å¢™ã€‚Ubuntu è‡ªå¸¦ UFW é˜²ç«å¢™ï¼Œå¯ä»¥å¼€å¯ SSHã€HTTP(S) ä»¥åŠ Hexo é»˜è®¤é¢„è§ˆç«¯å£ 4000ï¼š\nsudo apt-get install ufw  sudo ufw allow &quot;OpenSSH&quot;  sudo ufw allow 4000  sudo ufw allow http  sudo ufw allow https  sudo ufw enable\n2. é€‰æ‹©ä¸é…ç½® Hexo ä¸»é¢˜\nHexo é»˜è®¤ä¸»é¢˜ä¸º Landscapeï¼Œä½†ä¸ºäº†æ‰“é€ ä¸€ä¸ªç®€æ´ç¾è§‚çš„æŠ€æœ¯åšå®¢ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ NexT ä¸»é¢˜ï¼Œå®ƒåŠŸèƒ½å¼ºå¤§ä¸”å¤–è§‚ä¼˜é›…ã€‚ä»¥ä¸‹æ˜¯ä¸»é¢˜çš„å®‰è£…å’Œé…ç½®æ­¥éª¤ï¼š\nè·å– NexT ä¸»é¢˜ï¼š\nåœ¨ Hexo åšå®¢æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥å…‹éš† NexT ä¸»é¢˜ï¼š\ncd /var/www/hexogit clone https://github.com/theme-next/hexo-theme-next themes/next\nä¿®æ”¹ä¸»é¢˜é…ç½®ï¼š\nå…‹éš†å®Œæˆåï¼Œæ‰“å¼€ _config.yml é…ç½®æ–‡ä»¶ï¼Œå°† theme é…ç½®ä»é»˜è®¤çš„ landscape æ”¹ä¸º nextï¼š\n# _config.ymltheme: next\nå®‰è£…ä¸»é¢˜ä¾èµ–ï¼š\næ ¹æ®éœ€è¦å®‰è£… NexT ä¸»é¢˜çš„ä¾èµ–ï¼Œå¹¶å¯ç”¨ä½ æ‰€éœ€çš„åŠŸèƒ½ã€‚\nç”Ÿæˆå¸¸ç”¨é¡µé¢ï¼š\nä¸ºäº†å®Œå–„ç½‘ç«™ç»“æ„ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆæ ‡ç­¾ã€åˆ†ç±»ã€å½’æ¡£ç­‰é¡µé¢ï¼š\nhexo new page &quot;tags&quot;hexo new page &quot;categories&quot;hexo new page &quot;archives&quot;hexo new page &quot;about&quot;\nç¼–è¾‘æ¯ä¸ªé¡µé¢çš„ index.mdï¼Œåœ¨ Front-matter ä¸­æŒ‡å®šé¡µé¢ç±»å‹ï¼š\ntitle: æ ‡ç­¾date: 2025-03-06 15:00:00type: &quot;tags&quot;\nå¯¼èˆªæ èœå•å®šåˆ¶ï¼š\nåœ¨ themes/next/_config.yml ä¸­æ‰¾åˆ° menu è®¾ç½®ï¼Œå¹¶æ·»åŠ æ–°åˆ›å»ºçš„é¡µé¢ï¼š\nmenu:  home: / || home  categories: /categories/ || th  tags: /tags/ || tags  archives: /archives/ || archive  about: /about/ || user\nä¿å­˜ä¿®æ”¹åï¼Œé‡æ–°ç”Ÿæˆç«™ç‚¹ï¼Œæ–°çš„å¯¼èˆªæ èœå•å³ä¼šæ˜¾ç¤ºã€‚\n3. æ’°å†™ä¸ç®¡ç†åšå®¢å†…å®¹\nHexo ä½¿ç”¨ Markdown æ ¼å¼æ¥æ’°å†™æ–‡ç« ï¼Œéå¸¸é€‚åˆæŠ€æœ¯åšå®¢ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ç®¡ç†å’Œç¼–å†™æ–‡ç« çš„æ­¥éª¤ï¼š\næ–°å»ºåšæ–‡ï¼š\nä½¿ç”¨ Hexo CLI åˆ›å»ºæ–°çš„æ–‡ç« ï¼š\nhexo new &quot;æ–‡ç« æ ‡é¢˜&quot;\nè¿™å°†åœ¨ source/_posts/ ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª Markdown æ–‡ä»¶ï¼Œæ–‡ä»¶çš„å¼€å¤´æ˜¯ Front-matterï¼Œç”¨äºé…ç½®æ–‡ç« çš„å…ƒæ•°æ®ï¼ˆå¦‚æ ‡é¢˜ã€æ—¥æœŸã€åˆ†ç±»å’Œæ ‡ç­¾ç­‰ï¼‰ï¼š\ntitle: æ·±åº¦å­¦ä¹ å…¥é—¨æŒ‡å—  date: 2025-03-06 15:00:00  categories:    - äººå·¥æ™ºèƒ½    - æ·±åº¦å­¦ä¹   tags:    - ç¥ç»ç½‘ç»œ    - å…¥é—¨æ•™ç¨‹ \nä½¿ç”¨ Markdown æ’°å†™å†…å®¹ï¼š\nåœ¨ Front-matter ä¸‹æ–¹ï¼Œç”¨ Markdown è¯­æ³•æ’°å†™æ­£æ–‡ã€‚Hexo é»˜è®¤æ”¯æŒ GFMï¼ˆGitHub Flavored Markdownï¼‰ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ä¹¦å†™æ ¼å¼ï¼Œä¾‹å¦‚ï¼š\n# ä¸€çº§æ ‡é¢˜## äºŒçº§æ ‡é¢˜**ç²—ä½“**ã€*æ–œä½“*å¼ºè°ƒ\næ’å…¥å›¾ç‰‡å’Œèµ„æºï¼š\nå¯ç”¨ post_asset_folder: true åï¼Œæ¯ç¯‡æ–‡ç« ä¼šæœ‰ç‹¬ç«‹çš„èµ„æºç›®å½•ã€‚å¯ä»¥å°†å›¾ç‰‡æ–‡ä»¶æ”¾å…¥è¯¥æ–‡ä»¶å¤¹ï¼Œå¹¶åœ¨æ–‡ç« ä¸­å¼•ç”¨ï¼š\n![](my-post/images/example.png)\nè‰ç¨¿ç®¡ç†ä¸å‘å¸ƒï¼š\nå¯ç”¨è‰ç¨¿åŠŸèƒ½åï¼Œæ–°åˆ›å»ºçš„æ–‡ç« ä¼šå…ˆæ”¾åœ¨ _drafts/ ä¸‹ã€‚å®Œæˆåï¼Œä½¿ç”¨ hexo publish \"æ–‡ç« æ ‡é¢˜\" å°†å…¶å‘å¸ƒã€‚\næ–‡ç« ç»“æ„å’Œåˆ†é¡µï¼š\nHexo æ”¯æŒæ–‡ç« åˆ†ç±»å’Œæ ‡ç­¾è‡ªåŠ¨æ•´ç†ã€‚ä½ ä¹Ÿå¯ä»¥é€šè¿‡ &lt;!-- more --&gt; æ¥æ‰‹åŠ¨æˆªæ–­æ‘˜è¦ï¼Œæé«˜é¦–é¡µåŠ è½½é€Ÿåº¦ã€‚\n4. SEO ä¼˜åŒ–\nä¸ºäº†è®©æ›´å¤šäººçœ‹åˆ°ä½ çš„æŠ€æœ¯åšå®¢ï¼Œè¿›è¡Œ SEOï¼ˆæœç´¢å¼•æ“ä¼˜åŒ–ï¼‰éå¸¸é‡è¦ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¼˜åŒ–æªæ–½ï¼š\nç«™ç‚¹æ ‡é¢˜ä¸å…ƒä¿¡æ¯ï¼š\nåœ¨ _config.yml ä¸­å¡«å†™æœ‰åŠ©äº SEO çš„ç«™ç‚¹åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬ titleï¼ˆæ ‡é¢˜ï¼‰ã€descriptionï¼ˆæè¿°ï¼‰å’Œ keywordsï¼ˆå…³é”®è¯ï¼‰ã€‚\né“¾æ¥ä¼˜åŒ–ï¼š\nä¿®æ”¹æ°¸ä¹…é“¾æ¥æ ¼å¼ï¼Œç®€åŒ– URL ç»“æ„ï¼š\npermalink: :category/:title/\nç«™ç‚¹åœ°å›¾ï¼š\nç”Ÿæˆç«™ç‚¹åœ°å›¾å¸®åŠ©æœç´¢å¼•æ“æŠ“å–æ‰€æœ‰é¡µé¢ï¼š\nnpm install hexo-generator-sitemap hexo-generator-baidu-sitemap --save\nå¹¶åœ¨ _config.yml ä¸­æ·»åŠ é…ç½®ï¼š\nsitemap:  path: sitemap.xmlbaidusitemap:  path: baidusitemap.xml\næœºå™¨äººåè®®ï¼š\nåœ¨ source/ ç›®å½•ä¸‹åˆ›å»º robots.txt æ–‡ä»¶ï¼Œå¹¶å†™å…¥è§„åˆ™ï¼š\nUser-agent: *Allow: /Disallow: /admin/Sitemap: https://ä½ çš„åŸŸå/sitemap.xml\nå¥½çš„ï¼Œä»¥ä¸‹æ˜¯æˆ‘é‡æ–°ç”Ÿæˆå¹¶ä¿æŒå®Œæ•´çš„ Hexo Deploy è‡ªåŠ¨éƒ¨ç½²éƒ¨åˆ†ï¼Œç¡®ä¿æ²¡æœ‰çœç•¥ä»»ä½•ç»†èŠ‚ï¼š\n\n5. åšå®¢éƒ¨ç½²\nå®Œæˆå†…å®¹åˆ›ä½œå’Œä¼˜åŒ–åï¼Œå°±éœ€è¦å°†åšå®¢éƒ¨ç½²ä¸Šçº¿ã€‚Hexo ç”Ÿæˆçš„æ˜¯çº¯é™æ€ç½‘é¡µï¼Œå¯ä»¥éƒ¨ç½²åœ¨ä»»æ„é™æ€æœåŠ¡å™¨æˆ–æ‰˜ç®¡å¹³å°ä¸Šã€‚è¿™é‡Œä»‹ç»åœ¨ Ubuntu æœåŠ¡å™¨ä¸Šä½¿ç”¨ Nginx éƒ¨ç½²çš„æ–¹æ¡ˆï¼Œå¹¶è®¨è®º Nginx é…ç½®å’Œ Git è‡ªåŠ¨éƒ¨ç½²æ–¹æ³•ã€‚\næœ¬åœ°ç”Ÿæˆé™æ€æ–‡ä»¶ï¼š\nHexo æä¾›å‘½ä»¤å°† Markdown å†…å®¹ç”Ÿæˆé™æ€ç½‘é¡µã€‚ä¸€èˆ¬åœ¨æœ¬åœ°æˆ–æœåŠ¡å™¨ä¸Šè¿è¡Œï¼š\nhexo clean        # æ¸…ç†ä¸Šæ¬¡ç”Ÿæˆçš„æ–‡ä»¶hexo generate (hexo g)   # ç”Ÿæˆæœ€æ–°é™æ€ç½‘é¡µ\nç”Ÿæˆçš„æ–‡ä»¶ä½äºåšå®¢ç›®å½•ä¸‹çš„ public/ æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«åšå®¢çš„æ‰€æœ‰ HTMLã€CSSã€JSã€å›¾ç‰‡ç­‰é™æ€èµ„æºã€‚è¿™ä¸ª public æ–‡ä»¶å¤¹å³æ˜¯æœ€ç»ˆéƒ¨ç½²çš„ç½‘ç«™å†…å®¹ã€‚\nNginx éƒ¨ç½²é™æ€ç«™ç‚¹ï¼š\nåœ¨æœåŠ¡å™¨ä¸Šå®‰è£… Nginx å¹¶é…ç½®ç«™ç‚¹ï¼Œä»¥æä¾› Web æœåŠ¡ï¼š\nå®‰è£… Nginxï¼š\nsudo apt-get install -y nginx\nå®‰è£…åå¯åŠ¨ Nginx æœåŠ¡ï¼š\nsudo systemctl start nginx  # å¯è®¾ç½®å¼€æœºè‡ªå¯\né…ç½®ç«™ç‚¹ï¼šåœ¨ /etc/nginx/sites-available/ ç›®å½•ä¸‹åˆ›å»ºé…ç½®æ–‡ä»¶ï¼Œå¦‚ hexo.confï¼Œå†…å®¹å¦‚ä¸‹ï¼š\nserver &#123;    listen 80;    server_name example.com;  # å°†æ­¤æ›¿æ¢ä¸ºä½ çš„åŸŸåæˆ–æœåŠ¡å™¨IP    root /var/www/hexo/public;    index index.html index.htm;    location / &#123;        try_files $uri $uri/ =404;    &#125;&#125;\nä¸Šè¿°é…ç½®æŒ‡å®šæœåŠ¡å™¨ç›‘å¬ 80 ç«¯å£ï¼Œserver_name ä¸ºä½ çš„åŸŸåï¼ˆéœ€è¦å°†åŸŸåè§£ææŒ‡å‘è¯¥æœåŠ¡å™¨ï¼‰ã€‚root æŒ‡å‘ Hexo ç”Ÿæˆçš„ public ç›®å½•ï¼Œindex å£°æ˜é»˜è®¤é¦–é¡µæ–‡ä»¶ã€‚\nå¯ç”¨ç«™ç‚¹é…ç½®ï¼šå°†é…ç½®æ–‡ä»¶é“¾æ¥åˆ° sites-enabledï¼š\nln -s /etc/nginx/sites-available/hexo.conf /etc/nginx/sites-enabled/nginx -t  # æµ‹è¯•é…ç½®è¯­æ³•æ­£ç¡®æ€§systemctl reload nginx  # é‡æ–°åŠ è½½ Nginx é…ç½®\næ‰§è¡Œä»¥ä¸Šå‘½ä»¤åï¼Œåšå®¢ç«™ç‚¹å³å¯é€šè¿‡åŸŸåè®¿é—®ã€‚å¦‚æœæš‚æ—¶æ²¡æœ‰åŸŸåï¼Œä½¿ç”¨æœåŠ¡å™¨ IP ä¹Ÿèƒ½è®¿é—®ï¼ˆæ­¤æ—¶å¯å°† server_name æ”¹ä¸º _ é€šé…ç¬¦ï¼‰ã€‚\né…ç½® HTTPSï¼ˆå¯é€‰ï¼‰ï¼š\nå»ºè®®ä¸ºåšå®¢é…ç½® SSL è¯ä¹¦ã€‚å¯ä»¥ä½¿ç”¨ Certbot è·å– Letâ€™s Encrypt å…è´¹è¯ä¹¦ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š\napt-get install -y certbot python3-certbot-nginx  certbot --nginx -d example.com -d www.example.com\næŒ‰æç¤ºå®ŒæˆåŸŸåæ‰€æœ‰æƒéªŒè¯åï¼ŒCertbot ä¼šè‡ªåŠ¨ç”Ÿæˆè¯ä¹¦å¹¶é…ç½® Nginx å°†ç«™ç‚¹å‡çº§ä¸º HTTPSã€‚\nHexo Deploy è‡ªåŠ¨éƒ¨ç½²ï¼š\næ¯æ¬¡æ›´æ–°å†…å®¹åéƒ½è¦é‡æ–°ç”Ÿæˆå¹¶ä¸Šä¼ æ–‡ä»¶ï¼Œä½¿ç”¨ Hexo çš„éƒ¨ç½²åŠŸèƒ½å¯ä»¥ç®€åŒ–æµç¨‹ã€‚Hexo æ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ï¼Œå…¶ä¸­ Git éƒ¨ç½²æ˜¯å¸¸ç”¨æ–¹æ¡ˆä¹‹ä¸€ã€‚åŸºæœ¬æ€è·¯æ˜¯åˆ©ç”¨ Git æŠŠç”Ÿæˆçš„é™æ€æ–‡ä»¶æ¨é€åˆ°æœåŠ¡å™¨æˆ–æ‰˜ç®¡æœåŠ¡ã€‚æ¦‚æ‹¬äº†è¿™ç§æ€è·¯ï¼šåœ¨æœåŠ¡å™¨ä¸Šå®‰è£… Nginx æä¾›ç½‘é¡µæœåŠ¡ï¼Œç”¨ Git å®ç°ä»£ç ä¸Šä¼ è‡ªåŠ¨åŒ–ï¼Œè¿™æ ·æœ¬åœ°æ‰§è¡Œä¸€æ¬¡ hexo dï¼ˆdeployï¼‰å°±èƒ½è®©ç½‘ç«™æ›´æ–°ã€‚\næ¨é€åˆ°è¿œç¨‹æ‰˜ç®¡ï¼š\nå°†åšå®¢é™æ€æ–‡ä»¶éƒ¨ç½²åˆ°åƒ GitHub Pagesã€Coding Pages è¿™ç±»å¹³å°ã€‚è¿™éœ€è¦åœ¨ _config.yml ä¸­é…ç½®ï¼š\ndeploy:  type: git  repo: https://github.com/yourname/yourrepo.git  branch: main  # æˆ– gh-pages åˆ†æ”¯ç­‰\nç„¶åè¿è¡Œ hexo generate &amp;&amp; hexo deployï¼ŒHexo ä¼šæŠŠ public æ–‡ä»¶å¤¹å†…å®¹æ¨é€åˆ°æŒ‡å®šä»“åº“çš„åˆ†æ”¯ã€‚å¯¹äº GitHub Pagesï¼Œå¦‚æœ repo æ˜¯ yourname.github.io åˆ™ç›´æ¥ç”¨ä¸»åˆ†æ”¯ï¼›è‹¥æ˜¯é¡¹ç›®ä»“åº“ï¼Œå¯ä»¥ç”¨ gh-pages åˆ†æ”¯æ‰˜ç®¡ã€‚\néƒ¨ç½²åï¼ŒGitHub Pages æœåŠ¡å°†æ‰˜ç®¡ä½ çš„é™æ€åšå®¢ï¼Œä½ å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰åŸŸåç»‘å®šå®ƒã€‚ä½†æ³¨æ„ï¼šå¦‚æœä½ å¸Œæœ›åšå®¢è¿è¡Œåœ¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Šï¼ˆè€Œéç¬¬ä¸‰æ–¹å¹³å°ï¼‰ï¼Œåˆ™è¿™ç§æ–¹æ¡ˆä¸æ¶‰åŠä½ çš„æœåŠ¡å™¨ Nginxã€‚å¦å¤–ï¼Œå›½å†…è®¿é—® GitHub Pages å¯èƒ½ä¸ç¨³å®šï¼Œéœ€ç»“åˆå®é™…æƒ…å†µè€ƒè™‘ã€‚\næ¨é€åˆ°è‡ªå·±æœåŠ¡å™¨ï¼š\næ­å»ºå±äºè‡ªå·±çš„ Git è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹ï¼Œå®ç°å°†æœ¬åœ°æ›´æ–°ä¸€é”®éƒ¨ç½²åˆ°æœåŠ¡å™¨ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š\n\nåœ¨æœåŠ¡å™¨ä¸Šåˆ›å»ºä¸€ä¸ªè£¸ä»“åº“ï¼ˆbare repositoryï¼‰ï¼Œç”¨äºæ¥æ”¶æ¨é€ã€‚ä¾‹å¦‚åˆ›å»º /home/git/hexo.git è£¸ä»“åº“ã€‚\nç¼–å†™ Git é’©å­ï¼ˆpost-receiveï¼‰ï¼šè£¸ä»“åº“çš„ hooks/post-receive è„šæœ¬ä¼šåœ¨æ”¶åˆ°æ–°æ¨é€æ—¶æ‰§è¡Œã€‚è„šæœ¬å†…å®¹å¯ä»¥æ˜¯å°†æ›´æ–°çš„å†…å®¹æ£€å‡ºåˆ° Nginx ç›®å½•ã€‚ä¾‹å¦‚ï¼š\nGIT_WORK_TREE=/var/www/hexo git checkout -f  # å°†ä»“åº“å†…å®¹å¼ºåˆ¶æ£€å‡ºåˆ° /var/www/hexocd /var/www/hexo &amp;&amp; hexo generate            # ï¼ˆè‹¥æ¨é€çš„æ˜¯æºç è€Œéç”Ÿæˆæ–‡ä»¶ï¼Œåˆ™éœ€è¦åœ¨æœåŠ¡å™¨æ‰§è¡Œç”Ÿæˆï¼‰\nç»™è„šæœ¬å¯æ‰§è¡Œæƒé™ï¼š\nchmod +x post-receive\nè¿™æ ·ï¼Œæ¯å½“æ¨é€åˆ°è¯¥ä»“åº“æ—¶ï¼Œå®ƒå°±ä¼šæŠŠæ›´æ–°éƒ¨ç½²åˆ°åšå®¢ç›®å½•å¹¶ç”Ÿæˆæœ€æ–°é¡µé¢ã€‚\næœ¬åœ° Hexo é…ç½®éƒ¨ç½²ï¼šå°† _config.yml ä¸­çš„ deploy.repo è®¾ç½®ä¸ºä¸Šè¿°è£¸ä»“åº“çš„åœ°å€ï¼ˆé€šè¿‡ SSHï¼‰ã€‚ä¾‹å¦‚ï¼š\ndeploy:  type: git  repo: ssh://[emailÂ protected]/home/git/hexo.git  branch: master\nç„¶åæ‰§è¡Œ hexo clean &amp;&amp; hexo deployã€‚Hexo ä¼šé€šè¿‡ Git æ¨é€åˆ°æœåŠ¡å™¨ä»“åº“ï¼Œè§¦å‘ post-receive é’©å­ï¼Œå®ç°è‡ªåŠ¨éƒ¨ç½²ã€‚å®Œæˆåï¼ŒNginx ä¼šç«‹åˆ»æä¾›æ–°å†…å®¹æœåŠ¡ï¼Œæ— éœ€æ‰‹åŠ¨ç™»å½•æœåŠ¡å™¨æ“ä½œã€‚\n\né€šè¿‡è¿™ç§æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨æœ¬åœ°å†™å¥½æ–‡ç« åä¸€æ¡å‘½ä»¤å®Œæˆéƒ¨ç½²ï¼Œéå¸¸é«˜æ•ˆã€‚è®¸å¤šå¼€æºåšå®¢éƒ¨ç½²è„šæœ¬å’Œå·¥å…·ä¹Ÿæ˜¯åŸºäºç±»ä¼¼åŸç†å®ç°çš„ã€‚åˆæ¬¡è®¾ç½®å¯èƒ½ç¨æ˜¾ç¹çï¼Œä½†ä¸€æ—¦é…ç½®æˆåŠŸï¼Œæ—¥å¸¸æ›´æ–°å°†éå¸¸ä¾¿æ·ã€‚\næç¤ºï¼š ä½¿ç”¨ Git è‡ªåŠ¨éƒ¨ç½²éœ€ç¡®ä¿æœåŠ¡å™¨å¼€æ”¾ Git æ‰€ç”¨çš„ SSH ç«¯å£ï¼ˆé»˜è®¤ä¸º 22ï¼‰ï¼Œå¹¶é…ç½®å¥½å…¬é’¥å…å¯†ç™»å½•ï¼Œä»¥ä¾¿ Hexo åœ¨æœ¬åœ°èƒ½é¡ºåˆ©æ¨é€åˆ°æœåŠ¡å™¨ã€‚å¦‚æœä½ çš„æœåŠ¡å™¨ SSH ç«¯å£ä¸æ˜¯ 22ï¼Œå¯åœ¨éƒ¨ç½²é…ç½®ä¸­åŠ å…¥ç«¯å£å·æˆ–åœ¨ .ssh/config ä¸­é…ç½®åˆ«åã€‚å¯¹äºä¸ç†Ÿæ‚‰ Git é’©å­çš„æ–°æ‰‹ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘ä½¿ç”¨ç®€å•çš„ rsync è„šæœ¬åŒæ­¥æ–‡ä»¶æˆ–å€ŸåŠ© CI å¹³å°å®ç°éƒ¨ç½²ï¼Œä½†åŸç†ç±»ä¼¼ã€‚\n\n6. ç»´æŠ¤ä¸ä¼˜åŒ–\nåšå®¢æ­å»ºå®Œæˆå¹¶ä¸æ„å‘³ç€ä¸€åŠ³æ°¸é€¸ï¼Œå®šæœŸçš„ç»´æŠ¤å’Œä¼˜åŒ–èƒ½ä¿è¯åšå®¢ç¨³å®šã€å®‰å…¨ï¼Œå¹¶æŒç»­æå‡ç”¨æˆ·ä½“éªŒã€‚\n\næ’ä»¶æ‰©å±•ï¼š Hexo æ‹¥æœ‰ä¸°å¯Œçš„æ’ä»¶ç”Ÿæ€ï¼Œå¯æ ¹æ®éœ€è¦å®‰è£…æ’ä»¶ä»¥å¢å¼ºåŠŸèƒ½ã€‚\nå¤‡ä»½ä¸ç‰ˆæœ¬æ§åˆ¶ï¼š ä½¿ç”¨ Git ç®¡ç†åšå®¢æºç ï¼Œå®šæœŸå¤‡ä»½ã€‚\næ›´æ–°ä¸å‡çº§ï¼š å…³æ³¨ Hexo çš„ç‰ˆæœ¬æ›´æ–°ã€æ’ä»¶æ›´æ–°ç­‰ã€‚\n\n","categories":["other"]},{"title":"Lumos : Efficient Performance Modeling and Estimation for Large-scale LLM Training","url":"/2025/08/17/paper/lumos/","content":"\n\nä¸€å¥è¯æ€»ç»“ï¼šLumos æ˜¯ä¸€ä¸ªåŸºäºè¿è¡Œæ—¶ trace çš„å»ºæ¨¡/æ¨¡æ‹Ÿå·¥å…·ï¼Œä» PyTorch Kineto ç­‰é‡‡é›†åˆ°çš„äº‹ä»¶ä¸­è‡ªåŠ¨æ¢å¤ç²¾ç»†çš„æ‰§è¡Œå›¾ï¼ˆå«ç®—-é€šé‡å ä¸è·¨æµä¾èµ–ï¼‰ï¼Œå¹¶æ”¯æŒåœ¨ä¸é‡æ–°è·‘æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå¯¹ DP/PP/æ¨¡å‹ç»“æ„ åš â€œwhat-ifâ€ ä¿®æ”¹ä¸å¿«é€Ÿä¼°ç®—ï¼›åœ¨ 512Ã—H100 é›†ç¾¤ä¸Šå›æ”¾å¹³å‡è¯¯å·®çº¦ 3.3%ã€‚(arXiv)\n\n\n1. æ ¸å¿ƒè´¡çŒ®ä¸å®šä½\n\nç²¾ç»†æ‰§è¡Œå›¾ï¼šä»…ç”¨æ¡†æ¶å†…ç½®çš„ profilerï¼ˆå¦‚ PyTorch Kinetoï¼‰å³å¯ä» CPU/GPU äº‹ä»¶æ¢å¤å››ç±»å…³é”®ä¾èµ–ï¼ˆCPUâ†’GPUã€GPUâ†’CPUã€åŒæµé¡ºåºã€è·¨æµäº‹ä»¶ï¼‰ï¼Œç²¾å‡†è¡¨è¾¾ç®—-é€šé‡å ä¸åŒæ­¥å…³ç³»ã€‚(arXiv)\nå›¾ç¼–è¾‘ &amp; å¿«é€Ÿå¤–æ¨ï¼šåœ¨ä¸æ”¹åŠ¨æ¨¡å‹/ç³»ç»Ÿçš„å‰æä¸‹ï¼Œä»åŸå§‹ trace-graph å‡ºå‘ï¼Œå¯¹ æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ã€æµæ°´å¹¶è¡Œï¼ˆPPï¼‰ ä¸æ¨¡å‹å±‚æ•°/éšè—ç»´åº¦åšå›¾çº§æ”¹å†™ï¼Œå†ç”¨æ¨¡æ‹Ÿå™¨é‡æ”¾ä¸€æ•´ä¸ªè¿­ä»£ä¼°ç®—æ€§èƒ½ã€‚(arXiv)\né«˜ç²¾åº¦å›æ”¾ï¼šåœ¨ç”Ÿäº§é›†ç¾¤ æœ€å¤š 512Ã—H100ã€å¤šç§ GPT-3 å˜ä½“ã€ä¸åŒå¹¶è¡Œç­–ç•¥ä¸‹ï¼Œè¿­ä»£æ—¶é—´å›æ”¾å¹³å‡è¯¯å·® 3.3%ï¼Œå¹¶èƒ½å†ç°å®æµ‹çš„æ‰§è¡Œç»†åˆ†å æ¯”ã€‚(arXiv)\n\n\n2. Lumos å¦‚ä½•ä» trace æ„å»ºæ‰§è¡Œå›¾\n\näº‹ä»¶æ¥æºï¼šç›´æ¥ä½¿ç”¨ PyTorch/TensorFlow çš„å†…ç½® profilerï¼ˆå¦‚ Kinetoï¼‰ï¼Œæ— éœ€å¯¹æ¨¡å‹æˆ–æ¡†æ¶åšä¾µå…¥å¼æ”¹é€ ã€‚(arXiv)\nä¾èµ–å»ºæ¨¡ï¼ˆå››ç±»ï¼‰ï¼š\n\nCPUâ†’GPUï¼ˆlaunchï¼‰ï¼šç”¨ correlation ID ç»‘å®š CPU ç«¯çš„ cudaLaunchKernel/cudaMemsetAsync ä¸å¯¹åº”çš„ GPU kernelã€‚\nGPUâ†’CPUï¼ˆåŒæ­¥ï¼‰ï¼šcudaDeviceSync/cudaStreamSync ç­‰éœ€è¦ç­‰åˆ°ç›¸å…³ GPU kernel å®Œæˆã€‚\nåŒæµé¡ºåºï¼šåŒä¸€ CUDA stream å†…æ ¸ä¸¥æ ¼é¡ºåºã€‚\nè·¨æµäº‹ä»¶ï¼šcudaEventRecord ä¸ cudaStreamWaitEvent å½¢æˆâ€œè®°å½•â†’ç­‰å¾…â€çš„è·¨æµä¾èµ–ï¼Œè¡¨è¾¾ä¸åŒæµé—´çš„æœ‰åºæ€§ã€‚(arXiv)\n\n\n\n3. å›¾ç¼–è¾‘ï¼šæ”¯æŒå“ªäº› â€œwhat-ifâ€ æ”¹åŠ¨\n\næ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ï¼šåªéœ€è°ƒæ•´é€šä¿¡ä»»åŠ¡ï¼ˆå¦‚æ¢¯åº¦è§„çº¦ç±»ï¼‰çš„æ‰§è¡Œæ—¶é—´ï¼›æœ¬åœ°è®¡ç®—ä¸å˜ã€‚(arXiv)\næµæ°´å¹¶è¡Œï¼ˆPPï¼‰ï¼š\n\nå…ˆæŒ‰æ‰€é€‰è°ƒåº¦ï¼ˆå¦‚ 1F1Bï¼‰æ›´æ–°å„å¾®æ‰¹çš„å‰åå‘é¡ºåºï¼›\nå°†åŸå›¾ä¸­ä»»åŠ¡æŒ‰å±‚èšç±»åé‡åˆ†é…åˆ°æ–° stageï¼›\nåœ¨ stage è¾¹ç•Œæ’å…¥/é‡è¿æ¿€æ´»ä¸æ¢¯åº¦çš„ send/recvï¼›\nä¿ç•™åŸ trace ä¸­çš„ä¾èµ–æ¨¡å¼ä»¥ä¿è¯å¯é‡æ”¾æ­£ç¡®æ€§ã€‚(arXiv)\n\næ¨¡å‹ç»“æ„ï¼š\n\néšè—ç»´åº¦å˜æ›´ï¼šé‡å†™ç›¸å…³ç®—å­/å†…æ ¸çš„è¾“å…¥å¼ é‡ç»´åº¦å¹¶é‡ä¼°æ—¶é•¿ï¼›\nå±‚æ•°å˜æ›´ï¼šå¤åˆ¶/åˆ å‡å±‚å—å¹¶é‡è¿ä¾èµ–ä¸é€šä¿¡ã€‚(arXiv)\n\næš‚ä¸æ”¯æŒï¼šä¿®æ”¹ Tensor Parallelismï¼ˆTPï¼‰ï¼ˆé€šå¸¸å—é™äºå•æœºä¸”é€šä¿¡é‡ï¼Œç•™ä½œæœªæ¥å·¥ä½œï¼‰ã€‚(arXiv)\n\n\n4. æ¨¡æ‹Ÿå™¨ï¼šäº‹ä»¶é©±åŠ¨æµç¨‹ï¼ˆè®ºæ–‡ç®—æ³• 1 çš„è¦ç‚¹ï¼‰\n\nç»´æŠ¤ä¸¤ä¸ªé›†åˆï¼š\n\nå›ºå®šä¾èµ–ï¼ˆåˆå§‹åŒ–é˜¶æ®µä¸€æ¬¡æ€§ç¡®å®šï¼Œå¦‚åŒçº¿ç¨‹/åŒæµé¡ºåºã€CPUâ†’GPU çš„ launch è¾¹ï¼‰ï¼›\nè¿è¡ŒæœŸä¾èµ–ï¼ˆä¾‹å¦‚ cudaStreamSync éœ€è¦ç­‰å¾…**è¯¥æµä¸Šâ€œæœ€åä¸€ä¸ª kernelâ€**å®Œæˆï¼Œè¿™ä¸ªâ€œæœ€åâ€è¦åœ¨è°ƒåº¦æ—¶æ‰èƒ½ç¡®å®šï¼‰ã€‚\n\nä¸»å¾ªç¯ï¼šä»å°±ç»ªé›†åˆå–ä»»åŠ¡ â†’ åˆ†é…åˆ°å…¶â€œå¤„ç†å™¨â€ï¼ˆCPU çº¿ç¨‹/CUDA streamï¼‰ä¸Šè¿è¡Œ â†’ æ›´æ–°å¤„ç†å™¨å¯ç”¨æ—¶é—´ä¸åç»§ä»»åŠ¡çš„æœ€æ—©å¯å¯åŠ¨æ—¶é—´ï¼›è‹¥ä»æœ‰è¿è¡ŒæœŸä¾èµ–æœªæ»¡è¶³åˆ™å»¶åã€‚(arXiv)\n\n\n5. è¯„æµ‹è®¾ç½®ä¸å…³é”®æ•°å­—\n\nè§„æ¨¡ä¸ç¯å¢ƒï¼šæœ€å¤š 512Ã—H100ï¼ˆ32 å°ä¸»æœºï¼‰ï¼ŒRoCE æ•°æ®ä¸­å¿ƒç½‘ç»œï¼ˆæ¯ä¸»æœº 8Ã—400Gbpsï¼‰ï¼ŒCUDA 12.4ï¼ŒPyTorch 2.5ï¼ŒTransformer Engine 0.12.0ï¼ŒLightning 1.9.4ã€‚(arXiv)\nå¯¹æ¯”åŸºçº¿ï¼šä¸ dPROï¼ˆtrace-driven å›æ”¾ç³»ç»Ÿï¼‰ç›¸æ¯”ï¼ŒLumos åœ¨å¤æ‚å¹¶è¡Œé…ç½®ä¸‹èƒ½æ›´å¥½æ•æ‰è·¨æµä¾èµ–ä¸ç®—-é€šé‡å ï¼Œæ˜¾è‘—é™ä½å›æ”¾è¯¯å·®ã€‚(arXiv)\nç»“æœï¼šå›æ”¾å¹³å‡è¯¯å·® ~3.3%ï¼›å¹¶å±•ç¤ºåœ¨ DP/PP/ç»“æ„å¤–æ¨æ—¶çš„ä¼°ç®—å‡†ç¡®æ€§ä¸æ‰§è¡Œç»†åˆ†ï¼ˆæš´éœ²è®¡ç®—/æš´éœ²é€šä¿¡/é‡å /å…¶ä»–ï¼‰ã€‚(arXiv)\n\n\n6. å·¥ç¨‹å®ç°ä¸ä½¿ç”¨é—¨æ§›\n\nå®ç°è§„æ¨¡ï¼šçº¦ 5,200 è¡Œ Pythonã€‚(arXiv)\næ¥å…¥æˆæœ¬ï¼šåœ¨è®­ç»ƒä»£ç é‡Œæ’å…¥ ~10 è¡Œ profiler hook é‡‡é›† Kineto traceï¼Œéšåèµ°è‡ªåŠ¨åŒ–æµç¨‹ï¼šå»ºå›¾ â†’ å›¾ç¼–è¾‘ â†’ æ¨¡æ‹Ÿä¼°ç®—ã€‚(arXiv)\n\n\n7. é€‚ç”¨/ä¸é€‚ç”¨åœºæ™¯\n\né€‚ç”¨ï¼š\n\néœ€è¦åœ¨çœŸå®æœºç¾¤å¤–å¿«é€Ÿæ¯”è¾ƒå¹¶è¡Œ/ç»“æ„é…ç½®ï¼ˆDP/PP/å±‚æ•°/éšè—ç»´ï¼‰å¹¶ä¼°ç®—æ”¶ç›Šï¼›\néœ€è¦é«˜ä¿çœŸå›æ”¾æ¥å®šä½ç®—-é€šé‡å ä¸è·¨æµåŒæ­¥å¤„çš„æ€§èƒ½ç“¶é¢ˆã€‚\n\nå½“å‰ä¸é€‚ç”¨/æ³¨æ„ï¼š\n\nä¿®æ”¹ TP çš„å¤–æ¨ï¼ˆè®ºæ–‡æš‚æœªæ”¯æŒï¼‰ï¼›\nè¿½æ±‚ FLOPsã€å†…å­˜ã€å¸¦å®½ã€èƒ½è€—ç­‰ç³»ç»Ÿçº§æŒ‡æ ‡ï¼ˆè®ºæ–‡ç§°ä¸ºåç»­è®¡åˆ’ï¼‰ï¼›\nä¼°ç®—å‡è®¾æ–°é…ç½®å¯æ­£å¸¸è¿è¡Œï¼ˆä¸è€ƒè™‘ OOM ç­‰å¤±æ•ˆæƒ…å½¢ï¼‰ã€‚(arXiv)\n\n\n\n8. ä¸æ—¢æœ‰å·¥ä½œçš„å…³ç³»ï¼ˆç¤ºä¾‹ï¼šdPROï¼‰\n\ndPRO åŒæ ·æ˜¯ trace-driven çš„æ€§èƒ½è¯Šæ–­/å›æ”¾ç³»ç»Ÿï¼Œä½†åœ¨å¤æ‚ LLM å¹¶è¡Œä¸‹ï¼Œè·¨æµä¾èµ–ä¸é‡å çš„ç²¾ç»†å»ºæ¨¡æ›´å›°éš¾ï¼Œå®¹æ˜“å¯¼è‡´è¿‡åº¦ä¹è§‚çš„å¹¶è¡Œé¢„æµ‹ï¼›Lumos åœ¨è¿™äº›æ–¹é¢åšäº†ç³»ç»Ÿå¢å¼ºå¹¶æ˜¾è‘—é™ä½è¯¯å·®ã€‚(arXiv)\n\n\n9. è®ºæ–‡ä¸ä¼šè®®ä¿¡æ¯ï¼ˆå¯å¼•ç”¨ï¼‰\n\nè®ºæ–‡ï¼ˆarXivï¼‰ï¼šâ€œLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Trainingâ€ï¼ˆ2025-04-12 é¦–æ¬¡æäº¤ï¼‰ã€‚(arXiv)\nPDFï¼ˆä½œè€…ä¸»é¡µé•œåƒ/MLSys è®ºæ–‡ï¼‰ï¼šå¯ä¸‹è½½å…¨æ–‡ã€‚(mingyu-liang.github.io)\nMLSys 2025 æ¥æ”¶ä¸æ—¥ç¨‹é¡µé¢ï¼ˆå«æŠ¥å‘Š/å½•æ’­å…¥å£ï¼‰ã€‚(mlsys.org)\n\n\n10. ä»£ç ä¸å¼€æºçŠ¶æ€ï¼ˆæˆªè‡³ 2025-08-15ï¼‰\n\næœªè§å®˜æ–¹ä»£ç åº“é“¾æ¥ï¼ˆarXiv/MLSys é¡µé¢ä¸ä½œè€… PDF ä¸­å‡æœªæä¾›ï¼‰ï¼Œç¤¾åŒºé‡Œå­˜åœ¨åŒåä½†æ— å…³çš„ â€œLumosâ€ é¡¹ç›®ï¼ˆå¦‚ Agent/è§†é¢‘ç”Ÿæˆ/è§†è§‰ç­‰ï¼‰ï¼Œæ³¨æ„åŒºåˆ†ã€‚(arXiv, mlsys.org, GitHub)\n\n\n11. å¿«é€Ÿä¸Šæ‰‹ï¼ˆç¤ºæ„ï¼‰\n\né‡‡é›† Kineto trace â†’ æ„å»ºæ‰§è¡Œå›¾ â†’ åœ¨å›¾ä¸Šç¼–è¾‘ï¼ˆDP/PP/ç»“æ„ï¼‰â†’ æ¨¡æ‹Ÿå›æ”¾/ä¼°ç®—ã€‚ è®ºæ–‡æ­£æ–‡ç»™å‡ºäº†å…¸å‹çš„ PyTorch profiler ç”¨æ³•ç¤ºæ„ä¸å…¨æµç¨‹ç¤ºæ„å›¾ã€‚(arXiv)\n\n\n12. ä½ å¯èƒ½å…³å¿ƒçš„ç»†èŠ‚ï¼ˆç²¾ç‚¼ç‰ˆï¼‰\n\nä¸ºä»€ä¹ˆæ›´å‡†ï¼Ÿ\n\nç”¨ correlation ID ä¸²èµ· CPU launch ä¸ GPU kernelï¼›\næ˜¾å¼æ¢å¤ è·¨æµäº‹ä»¶ï¼ˆRecord/Waitï¼‰ä¸åŒæ­¥ï¼ˆStream/Device Syncï¼‰ï¼›\nåœ¨æ¨¡æ‹Ÿå™¨ä¸­å°†ä¾èµ–åˆ†ä¸ºå›ºå®šä¸è¿è¡ŒæœŸï¼Œç¡®ä¿åƒ â€œç­‰å¾…è¯¥æµæœ€åä¸€ä¸ª kernelâ€ è¿™ç±»è¯­ä¹‰è¢«æ­£ç¡®è¡¨è¾¾ã€‚(arXiv)\n\næ”¹ DP/PP æ€ä¹ˆç®—ï¼Ÿ\n\nDPï¼šåªé‡èµ‹é€šä¿¡ä»»åŠ¡æ—¶é•¿ï¼›\nPPï¼šæ›´æ–°è°ƒåº¦ï¼ˆå¦‚ 1F1Bï¼‰â†’ ä»»åŠ¡æŒ‰å±‚åˆ†ç»„å¹¶é‡åˆ† stage â†’ åœ¨è¾¹ç•Œæ’å…¥ send/recv â†’ ä¿æŒä¾èµ–é—­åˆã€‚(arXiv)\n\n\n\nå‚è€ƒæ–‡çŒ® / é“¾æ¥\n\nLumos è®ºæ–‡ï¼ˆarXiv é¡µé¢ä¸ PDFï¼‰ï¼š(arXiv)\nLumosï¼ˆMLSys 2025 ä¼šè®®é¡µé¢/æ—¥ç¨‹/å½•æ’­ï¼‰ï¼š(mlsys.org)\ndPROï¼ˆtrace-driven å›æ”¾åŸºçº¿è®ºæ–‡ï¼‰ï¼š(arXiv)\n\n\n\næ³¨ï¼šæœ¬æ–‡æ¡£åªæ‘˜å–å¯¹å·¥ç¨‹è½åœ°æœ€å…³é”®çš„äº‹å®ä¸æ–¹æ³•ï¼Œæ›´å¤šå›¾ä¾‹ï¼ˆå¦‚ PPÃ—TP å¾®æ‰¹è°ƒåº¦ç¤ºä¾‹ï¼‰ä¸å®Œæ•´ç®—æ³•ç»†èŠ‚è¯·å‚é˜…åŸè®ºæ–‡æ­£æ–‡ä¸é™„å›¾ã€‚(arXiv)\n\n\n","categories":["paper"],"tags":["paper"]}]