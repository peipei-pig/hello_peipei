[{"title":"attentionä¸­å¼ é‡å¹¶è¡Œä¸GQA","url":"/2025/08/17/distribute/attention/","content":"\n\n\nä¾‹å­é…ç½®ï¼ˆè´¯ç©¿å…¨æ–‡ï¼‰ï¼š hidden_size=4096, num_attention_heads=32, tensor_parallel_size=4, num_query_groups=8ï¼ˆGQAï¼‰, kv_channels=hidden/heads=128ã€‚ è¾“å…¥å½¢çŠ¶ç”¨ [B, S, H] è®°ï¼ˆæ‰¹ã€åºåˆ—ã€éšè—ï¼‰ã€‚\n\n\n1. åè¯ä¸æ´¾ç”Ÿå˜é‡ï¼ˆå…ˆæŠŠé‡ç®—æ¸…æ¥šï¼‰\n\nå•å¤´ç»´åº¦ï¼ˆä¹Ÿæ˜¯ç¼©æ”¾ç”¨çš„ \\(d_k\\)ï¼‰ï¼škv_channels = 4096 / 32 = 128\nQ æŠ•å½±æ€»ç»´ï¼šquery_projection_size = kv_channels * num_attention_heads = 128*32 = 4096\nK/V æŠ•å½±æ€»ç»´ï¼ˆGQAï¼‰ï¼škv_projection_size = kv_channels * num_query_groups = 128*8 = 1024\næ¯å¡ Q å¤´æ•°ï¼šnum_attention_heads_per_partition = 32 / TP = 8\næ¯å¡ KV ç»„æ•°ï¼šnum_query_groups_per_partition = 8 / TP = 2\næ¯å¡æŠ•å½±ç»´ï¼ˆåˆ—å¹¶è¡Œåæœ¬åœ°è¾“å‡ºç»´ï¼‰ï¼šhidden_size_per_partition = 4096 / TP = 1024\n\n\nGQA çš„å«ä¹‰ï¼šå½“ num_key_value_heads (=num_query_groups) å°äº num_attention_heads æ—¶ï¼Œä¸ºè¾ƒå°‘çš„ KV å¤´/ç»„ äº§å‡º K/Vï¼Œè®©å¤šä¸ª Q å¤´å…±äº«å®ƒä»¬ï¼›=heads é€€åŒ–ä¸º MHAï¼Œ=1 æ˜¯ MQAã€‚è¿™ä¸€ç‚¹åœ¨ HF æ¨¡å‹æ–‡æ¡£ä¸­æ˜¯æ˜ç¡®çš„å®šä¹‰ã€‚(Hugging Face)\n\n\n2. ç«¯åˆ°ç«¯è®¡ç®—ä¸å½¢çŠ¶æµï¼ˆä»¥å•å±‚è‡ªæ³¨æ„åŠ›ä¸ºä¾‹ï¼‰\nMegatron ç»å…¸åšæ³•ï¼šQ/K/V çš„çº¿æ€§å±‚ç”¨åˆ—å¹¶è¡Œï¼ˆColumn-Parallelï¼‰ï¼ŒæŒ‰è¾“å‡ºåˆ—åˆ‡ç»™å„å¡ï¼›è¾“å‡ºæŠ•å½±ç”¨è¡Œå¹¶è¡Œï¼ˆRow-Parallelï¼‰ï¼ŒæŒ‰è¾“å…¥è¡Œåˆ‡ç»™å„å¡ï¼Œå‰å‘åªåœ¨è¾“å‡ºæŠ•å½±åšä¸€æ¬¡ all-reduceã€‚è¿™æ˜¯ Megatron-LM è®ºæ–‡ä¸ Megatron-Core æ–‡æ¡£æ¨èçš„å¼ é‡å¹¶è¡Œåˆ‡æ³•ã€‚(arXiv, NVIDIA Docs)\n2.1 çº¿æ€§æŠ•å½±ï¼ˆåˆ—å¹¶è¡Œï¼‰\n\nQ æŠ•å½±ï¼ˆå…¨å±€æƒé‡ [4096,4096] â†’ æ¯å¡ [4096,1024]ï¼‰ï¼š æœ¬å¡è¾“å‡º Q_local: [B,S,1024] â†’ reshape ä¸º [B, 8, S, 128]ï¼ˆæœ¬å¡ 8 ä¸ª Q å¤´ï¼‰ã€‚\nK æŠ•å½±ï¼ˆå…¨å±€æƒé‡ [4096,1024] â†’ æ¯å¡ [4096,256]ï¼‰ï¼š K_local: [B,S,256] â†’ reshape ä¸º [B, 2, S, 128]ï¼ˆæœ¬å¡ 2 ä¸ª KV ç»„ï¼‰ã€‚\nV æŠ•å½± åŒ Kã€‚\n\n\nä¸ºä»€ä¹ˆå¿…é¡» reshape å‡º head ç»´ï¼Ÿ å¤šå¤´æ³¨æ„åŠ›çš„è¯­ä¹‰æ˜¯â€œå¤´å†…ç‹¬ç«‹â€çš„ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œå†…æ ¸ï¼ˆSDPA/FlashAttentionï¼‰ä¸å¹¿æ’­ï¼ˆmaskã€RoPEã€repeat_kvï¼‰éƒ½è¦æ±‚æ˜¾å¼çš„ head ç»´ [B,H,S,D] æˆ–å±•å¹³ä¸º [BÂ·H,S,D]ã€‚ä¸æ‹†å¤´ä¼šæŠŠä¸åŒ head çš„å­ç©ºé—´æ··åœ¨ä¸€èµ·ï¼Œä¹Ÿæ— æ³•è‡ªç„¶æ‰§è¡Œ GQA çš„ repeat_kvã€‚(PyTorch)\n\n2.2 GQA çš„ K/V å¯¹é½ï¼ˆrepeat/expandï¼‰\næœ¬å¡åªæœ‰ 2 ä¸ª KV ç»„ï¼Œä½†è¦æœåŠ¡ 8 ä¸ª Q å¤´ â‡’ åœ¨å¤´ç»´åšé€»è¾‘é‡å¤/å¹¿æ’­ï¼š [B, 2, S, 128] â†’ [B, 8, S, 128]ï¼ˆæ¯ä¸ª KV ç»„æœåŠ¡ 4 ä¸ª Q å¤´ï¼‰ã€‚ä¸»æµå®ç°ç›´æ¥åœ¨ head ç»´åš repeat_kvã€‚(Hugging Face)\n2.3 Scaled Dot-Product Attentionï¼ˆæ¯å¡åªç®—è‡ªå·±çš„ 8 ä¸ªå¤´ï¼‰\n\nscores = (Q @ K^T) / sqrt(128) â†’ softmax(scores) @ V\nå¾—åˆ°ä¸Šä¸‹æ–‡ ctx_local: [B, 8, S, 128] â†’ æ‹¼æ¥ä¸º [B,S,1024]\nè¿™ä¸€æ­¥å¯ä»¥ç”± PyTorch SDPA æˆ–é—ªå­˜æ³¨æ„åŠ›å†…æ ¸é«˜æ•ˆå®Œæˆã€‚(PyTorch)\n\n2.4 è¾“å‡ºæŠ•å½±ï¼ˆè¡Œå¹¶è¡Œ + 1 æ¬¡ all-reduceï¼‰\n\næ¯å¡æŠŠ [B,S,1024] ä¹˜ä»¥æœ¬å¡çš„è¾“å‡ºæƒé‡åˆ†ç‰‡ï¼Œå¾—åˆ° Y_local: [B,S,4096] çš„éƒ¨åˆ†å’Œï¼›\nè·¨å¡åš all-reduce(sum) å¾—åˆ°æœ€ç»ˆ Y: [B,S,4096]ã€‚ Megatron è®ºæ–‡æŒ‡å‡ºï¼šè‡ªæ³¨æ„åŠ›æœ¬ä½“ä¸éœ€é€šä¿¡ï¼Œåªåœ¨è¾“å‡ºæŠ•å½±å¤„åšä¸€æ¬¡è§„çº¦å³å¯ã€‚(arXiv)\n\n\n3. åˆ—å¹¶è¡Œ / è¡Œå¹¶è¡Œçš„æ•°å­¦ç­‰ä»·ï¼ˆä¸ºä½•â€œåˆ‡äº†å†æ‹¼/æ±‚å’Œâ€ä»ç­‰ä»·å•å¡ï¼‰\næŠŠ [B,S,Â·] å±•å¹³ä¸ºçŸ©é˜µ \\(X\\in\\mathbb{R}^{N\\times(HD)}\\)ï¼ˆ\\(N=B\\cdot S\\)ï¼‰ï¼Œè¾“å‡ºéšè—è®°ä¸º \\(H\\)ã€‚\n3.1 åˆ—å¹¶è¡Œï¼ˆColumn-Parallel Linearï¼‰â‰¡ æ‹¼æ¥\nè®¾ Q çš„å…¨é‡æƒé‡ \\(W_Q\\in\\mathbb{R}^{(HD)\\times(HD)}\\)ï¼Œæ²¿åˆ—åˆ‡æˆ \\(p\\) å—ï¼š\n\\[\nW_Q=\\big[W_Q^{(0)}\\;\\;W_Q^{(1)}\\;\\;\\cdots\\;\\;W_Q^{(p-1)}\\big],\n\\quad W_Q^{(i)}\\in\\mathbb{R}^{(HD)\\times(H/p\\cdot D)}.\n\\]\nåˆ™\n\\[\nQ=XW_Q=\\big[XW_Q^{(0)}\\;\\;XW_Q^{(1)}\\;\\;\\cdots\\;\\;XW_Q^{(p-1)}\\big]\n      =\\operatorname{Concat}(Q^{(0)},\\dots,Q^{(p-1)}).\n\\]\næ¯å¡ç‹¬ç«‹è®¡ç®—è‡ªå·±çš„ \\(Q^{(i)}\\)ï¼Œæ— é¡»é€šä¿¡ã€‚K/V åŒç†ã€‚è¿™å°±æ˜¯ Column-Parallel çš„ç²¾ç¡®å®šä¹‰ã€‚(arXiv, NVIDIA Docs)\n3.2 æ¯å¤´ç‹¬ç«‹ â‡’ æŒ‰å¤´åˆ‡ç»™å„å¡ä»ç„¶æ­£ç¡®\nå•å¤´/å•ç»„æ³¨æ„åŠ›ï¼š\n\\[\nY_h=\\operatorname{softmax}\\!\\Big(\\tfrac{Q_hK_{g(h)}^\\top}{\\sqrt{D}}\\Big)V_{g(h)}.\n\\]\nGQA ä¸‹ \\(g(h)\\) æŠŠå¤šä¸ª Q å¤´æ˜ å°„åˆ°åŒä¸€ KV ç»„ï¼›ç”±äºå¤´é—´äº’ä¸ç›¸å¹²ï¼ŒæŠŠ 32 ä¸ªå¤´å¹³å‡åˆ†æˆ 4 ä»½åˆ° 4 å¼ å¡ï¼Œå„å¡åªä¾èµ–è‡ªå·±çš„ KV ç»„ï¼Œå°±ä¸å•å¡ä¸€è‡´ã€‚repeat_kv æ­£æ˜¯æ²¿ head ç»´æŠŠ KV å¯¹é½åˆ° Q å¤´æ•°ã€‚(Hugging Face)\n3.3 è¡Œå¹¶è¡Œï¼ˆRow-Parallel Linearï¼‰â‰¡ æ±‚å’Œï¼ˆall-reduceï¼‰\næŠŠæ³¨æ„åŠ›è¾“å‡ºçš„æ‹¼æ¥å¼ é‡ \\(C\\in\\mathbb{R}^{N\\times(HD)}\\) æŒ‰åˆ—ï¼ˆç‰¹å¾ï¼‰åˆ‡å—ï¼š\n\\[\nC=\\big[C^{(0)}\\;\\;C^{(1)}\\;\\;\\cdots\\;\\;C^{(p-1)}\\big],\\quad\nC^{(i)}\\in\\mathbb{R}^{N\\times(H/p\\cdot D)}.\n\\]\nè¾“å‡ºæƒé‡ \\(W_O\\in\\mathbb{R}^{(HD)\\times H}\\) æŒ‰è¡Œåˆ‡å—ï¼š\n\\[\nW_O=\\begin{bmatrix}\nW_O^{(0)}\\\\ W_O^{(1)}\\\\ \\vdots\\\\ W_O^{(p-1)}\n\\end{bmatrix},\n\\quad W_O^{(i)}\\in\\mathbb{R}^{(H/p\\cdot D)\\times H}.\n\\]\nå—ä¹˜æ³•æ’ç­‰å¼ï¼š\n\\[\nC\\,W_O=\\sum_{i=0}^{p-1} C^{(i)}W_O^{(i)}.\n\\]\nå› æ­¤å„å¡è®¡ç®— \\(Y^{(i)}=C^{(i)}W_O^{(i)}\\)ï¼Œå† all-reduce(sum)ï¼Œå°±å¾—åˆ°ä¸å•å¡å®Œå…¨ç›¸åŒçš„ \\(Y\\)ã€‚è¿™æ­£æ˜¯ Row-Parallel çš„æœ¬è´¨ã€‚(arXiv)\n\n4. ä¸ºä»€ä¹ˆ GQA ä¼šè®© kv_projection_size å˜å°ã€KV cache å˜çœï¼Ÿ\n\nK/V çº¿æ€§å±‚åªä¸º num_query_groups äº§å‡ºé€šé“ï¼šä» 4096ï¼ˆ=32Ã—128ï¼‰é™ä¸º 1024ï¼ˆ=8Ã—128ï¼‰ï¼ŒK/V æŠ•å½±çš„ å‚æ•°é‡ä¸ FLOPs çº¦ä¸ºåŸæ¥çš„ 1/4ï¼›\næ¨ç†é˜¶æ®µçš„ KV cache ä»¥ã€ŒKV å¤´ Ã— åºåˆ— Ã— å¤´ç»´ã€è®¡é‡ï¼ŒKV å¤´ä» 32 å˜ 8ï¼Œç¼“å­˜ä¸ç›¸å…³å¸¦å®½å‡ç›¸åº”ä¸‹é™ã€‚HF æ–‡æ¡£æ˜ç¡®ä»¥ num_key_value_heads æè¿°è¯¥è¡Œä¸ºã€‚(Hugging Face)\n\n\n5. å½¢çŠ¶é€ŸæŸ¥ï¼ˆä»¥æœ¬ä¾‹ä¸ºå‡†ï¼‰\n\n\n\nå¼ é‡/æ­¥éª¤\nå…¨å±€ï¼ˆä¸åˆ†ç‰‡ï¼‰\næ¯å¡ï¼ˆTP=4ï¼‰\nè¯´æ˜\n\n\n\n\nQ çº¿æ€§è¾“å‡ºç»´\n4096\n1024\nColumn-Parallelï¼Œæ— é€šä¿¡\n\n\nK çº¿æ€§è¾“å‡ºç»´\n1024\n256\nGQAï¼šåªå‡º 8 ä¸ª KV ç»„\n\n\nV çº¿æ€§è¾“å‡ºç»´\n1024\n256\nåŒä¸Š\n\n\nQ å¤´æ•°\n32\n8\næœ¬å¡åªç®—è‡ªå·± 8 ä¸ªå¤´\n\n\nKV ç»„æ•°\n8\n2\næ¯ç»„æœåŠ¡ 4 ä¸ª Q å¤´\n\n\nå¤´ç»´ \\(D\\)\n128\n128\nç”¨äº \\(1/\\sqrt{D}\\)\n\n\næœ¬å¡æ³¨æ„åŠ›è¾“å‡ºï¼ˆæ‹¼å¤´åï¼‰\nâ€“\n[B,S,1024]\nè¿›å…¥è¾“å‡ºæŠ•å½±\n\n\næœ€ç»ˆè¾“å‡ºï¼ˆall-reduce åï¼‰\n[B,S,4096]\n[B,S,4096]\nRow-Parallel + sum\n\n\n\nï¼ˆè‹¥å¯ç”¨ Sequence Parallelï¼Œåªä¼šæ²¿ S å†åˆ‡ä¸€ç»´ï¼Œä¸å½±å“ä¸Šè¿°å¤´/é€šé“ç»´é€»è¾‘ã€‚åˆ—/è¡Œå¹¶è¡Œä¸ä¸€æ¬¡é€šä¿¡çš„ç»“æ„æ˜¯ Megatron-LM çš„â€œç»å…¸æ‹†åˆ†â€ã€‚ï¼‰(awsdocs-neuron.readthedocs-hosted.com, Better Tomorrow with Computer Science)\n\n6. Mermaidï¼šä¸€å¼ â€œç»´åº¦/å¹¶è¡Œæ–¹å¼â€å°å›¾\n%%&#123;init: &#123; &quot;flowchart&quot;: &#123; &quot;htmlLabels&quot;: true, &quot;wrap&quot;: true &#125; &#125;&#125;%%flowchart TB  X[&quot;Input X: [B,S,4096]&quot;] --&gt; QKV[&quot;Column-Parallel Q/K/V&lt;br/&gt;Q:[B,S,1024]  K/V:[B,S,256]&quot;]  QKV --&gt; Reshape[&quot;Reshape by heads&lt;br/&gt;Q:[B,8,S,128]&lt;br/&gt;K/V:[B,2,S,128]&quot;]  Reshape --&gt; RepeatKV[&quot;repeat_kv on head dim&lt;br/&gt;K/V:[B,8,S,128]&quot;]  RepeatKV --&gt; SDPA[&quot;SDPA per head&lt;br/&gt;ctx_local:[B,8,S,128]&lt;br/&gt;concat-&gt;[B,S,1024]&quot;]  SDPA --&gt; OutProj[&quot;Row-Parallel OutProj&lt;br/&gt;Y_local:[B,S,4096]&quot;]  OutProj --&gt; AllReduce[&quot;all-reduce(sum)&quot;]  AllReduce --&gt; Y[&quot;Final Y: [B,S,4096]&quot;]\n\n7. æç®€ä¼ªç ï¼ˆPyTorch é£æ ¼ï¼‰\n# åˆ—å¹¶è¡Œçš„çº¿æ€§ï¼šæ¯å¡æ‹¿åˆ° Q/K/V çš„ä¸€æ®µè¾“å‡ºåˆ—Q_local = linear_col_parallel_Q(X)   # [B,S,1024] -&gt; view [B,8,S,128]K_local = linear_col_parallel_K(X)   # [B,S,256]  -&gt; view [B,2,S,128]V_local = linear_col_parallel_V(X)   # [B,S,256]  -&gt; view [B,2,S,128]Q = Q_local.view(B, S, 8, 128).transpose(1, 2)  # [B,8,S,128]K = K_local.view(B, S, 2, 128).transpose(1, 2)  # [B,2,S,128]V = V_local.view(B, S, 2, 128).transpose(1, 2)  # [B,2,S,128]# GQA: è®© 2 ä¸ª KV ç»„åŒ¹é… 8 ä¸ª Q å¤´ï¼ˆé€»è¾‘ repeat/expandï¼‰K = repeat_kv(K, n_rep=4)   # [B,8,S,128]V = repeat_kv(V, n_rep=4)   # [B,8,S,128]# SDPAï¼ˆæ¯å¡åªç®—è‡ªå·±çš„ 8 ä¸ªå¤´ï¼‰ctx = torch.nn.functional.scaled_dot_product_attention(Q, K, V)  # [B,8,S,128]ctx = ctx.transpose(1, 2).reshape(B, S, 1024)                    # [B,S,1024]# è¡Œå¹¶è¡Œè¾“å‡º + ä¸€æ¬¡ all-reduce(sum)Y_local = linear_row_parallel_out(ctx)   # partial: [B,S,4096]Y = all_reduce_sum(Y_local)              # final:   [B,S,4096]\n\nSDPA çš„æ¥å£ä¸è¯­ä¹‰è§ PyTorch æ–‡æ¡£ï¼›repeat_kv çš„è¯­ä¹‰ä¸ GQA çš„é…ç½®åœ¨ HF æ–‡æ¡£/å®ç°ä¸­æœ‰æ˜ç¡®å®šä¹‰ã€‚(PyTorch, Hugging Face)\n\n\n8. æ­£ç¡®æ€§ Checklistï¼ˆå®è·µä¸­æœ€å¸¸è§çš„å‘ï¼‰\n\næ•´é™¤å…³ç³»ï¼š num_attention_heads % TP == 0ï¼Œnum_query_groups % TP == 0ï¼Œä¸” num_attention_heads % num_query_groups == 0ï¼ˆGQAï¼‰ã€‚(Hugging Face)\næ˜¾å¼ head ç»´ï¼šå½¢çŠ¶åº”ä¸º [B,H,S,D] æˆ–å±•å¹³ä¸º [BÂ·H,S,D]ï¼Œä»¥å¥‘åˆ SDPA/FlashAttention ä¸ repeat_kvã€‚(PyTorch)\né€šä¿¡ä½ç½®ï¼šè‡ªæ³¨æ„åŠ›æœ¬ä½“æ— è·¨å¡é€šä¿¡ï¼›ä»…è¾“å‡ºæŠ•å½±éœ€è¦ä¸€æ¬¡ all-reduceã€‚(arXiv)\n\n\nå‚è€ƒä¸å»¶ä¼¸é˜…è¯»\n\nMegatron-LM è®ºæ–‡ï¼šæå‡ºå±‚å†…ï¼ˆå¼ é‡ï¼‰å¹¶è¡Œï¼Œæ³¨æ„åŠ›ç”¨åˆ—å¹¶è¡Œï¼Œè¾“å‡ºç”¨è¡Œå¹¶è¡Œï¼Œå‰å‘ä»…ä¸€å¤„é€šä¿¡ã€‚(arXiv, ar5iv)\nMegatron-Core æ–‡æ¡£ï¼šTensor Parallel API/ç”¨æˆ·æŒ‡å—ï¼ˆNVIDIA å®˜æ–¹ï¼‰ã€‚(NVIDIA Docs)\nPyTorch SDPA æ–‡æ¡£/æ•™ç¨‹ï¼šå®˜æ–¹çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æ¥å£ä¸é«˜æ€§èƒ½å®ç°ã€‚(PyTorch, PyTorch Docs)\nHF æ–‡æ¡£ï¼ˆLlama/Qwen ç³»åˆ—ï¼‰ï¼šnum_key_value_heads çš„å®šä¹‰ã€GQA/MQA/MHA çš„å…³ç³»ï¼›å®ç°é‡Œ repeat_kv çš„ç”¨æ³•ã€‚(Hugging Face)\nåˆ—å¹¶è¡Œ/è¡Œå¹¶è¡Œå¯è§†åŒ–è®²è§£ï¼šå¯¹ ColumnParallelLinear / RowParallelLinear çš„ç›´è§‚å›¾è§£ã€‚(awsdocs-neuron.readthedocs-hosted.com, Better Tomorrow with Computer Science)\n\n\n","categories":["åˆ†å¸ƒå¼åŸºç¡€"],"tags":["attention"]},{"title":"pytorch devicemesh","url":"/2025/06/14/distribute/device_mesh/","content":"\n\nä¸€ã€ä¸ºä½•ä½¿ç”¨ DeviceMeshï¼Ÿ\nåœ¨æ··åˆå¹¶è¡Œï¼ˆDP/TP/PP/HSDP/â€¦ï¼‰ä¸­ï¼Œéœ€è¦ç®¡ç†å¤šä¸ªå­é€šä¿¡ç»„ï¼ˆProcessGroupï¼‰ï¼Œå¯¹åº”å¤æ‚çš„è®¾å¤‡æ‹“æ‰‘ç»“æ„ã€‚DeviceMesh æä¾›äº†ï¼š\n\nç†è®ºä¸Šæ— ç¼æ”¯æŒä»»æ„ç»´åº¦çš„å¤šç»´æ‹“æ‰‘ï¼›\nè‡ªåŠ¨æ‹†åˆ†è¿›ç¨‹ç»„(new_group/split_group)ï¼›\nçµæ´»åˆ‡ç‰‡å­ Meshï¼›\nç»å†è®¾è®¡å‘¨å…¨çš„é«˜æ•ˆåˆå§‹åŒ–æ–¹æ¡ˆ (docs.pytorch.org, pytorch.org)ã€‚\n\n\näºŒã€åˆå§‹åŒ–æµç¨‹\ninit_device_mesh(...) çš„ä½œç”¨\nä¸€ä¸ªä¸€è¡Œæå®šçš„æ–¹æ³•ï¼Œå®ƒä¼šï¼š\n\nåˆå§‹åŒ–å…¨å±€ init_process_group(...)ï¼ˆè‹¥æœªåˆå§‹åŒ–ï¼‰ï¼›\næ ¹æ® mesh_shape è‡ªåŠ¨æ„é€  CPU ä¸Šçš„ torch.arange(...).view(...)ï¼›\nåˆ›å»º DeviceMesh(...)ã€‚å†…éƒ¨å®Œæˆå­ç»„æ‹†åˆ†åŸç†ï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰ã€‚\n\n\nDeviceMesh.__init__() + _init_process_groups()\n\nå­˜å‚¨ï¼šdevice_typeã€meshã€mesh_dim_namesï¼›\né€šä¿¡ç»„æ‹†åˆ†ï¼šéå†æ¯ä¸ªç»´åº¦ dimï¼š\n\nä½¿ç”¨ mesh.swapdims(-1, dim).reshape(-1, size(dim)) åˆ—å‡ºè¯¥ç»´æ‰€æœ‰å­ç»„ rankï¼›\nè‹¥ NCCL å·²ç»‘å®š GPUï¼Œå³å¯ç”¨ split_group ä¸€æ¬¡æ‹†å‡ºå…¨éƒ¨å­ç»„ï¼›\nå¦åˆ™ä½¿ç”¨ new_group() åˆ† group æ‹†ï¼›\nå¹¶å°†å½“å‰ rank å±äºçš„é‚£ç»„ä¿¡æ¯æ”¾å…¥ self._dim_group_infos[dim]ï¼›\n\nç»“æœï¼šæ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªåŒ…å«å½“å‰ rank çš„ ProcessGroup ä¿¡æ¯åˆ—è¡¨ã€‚\n\n#ppmesh = torch.tensor([  [0, 1],  # pp=0  [2, 3],  # pp=1  [4, 5],  # pp=2  [6, 7]   # pp=3])mesh.swapdims(-1, 0)tensor([[0,2,4,6],        [1,3,5,7]])pg_ranks_by_dim = tmp.reshape(-1, mesh.size(0))[  [0,2,4,6],  # å¯¹åº” tp è¡Œ 0 å„ pp æ®µ  [1,3,5,7]   # å¯¹åº” tp è¡Œ 1 å„ pp æ®µ]#tptmp = mesh.swapdims(-1, 1)  # ç­‰äº transpose(1,1)ï¼Œæœ¬èº«æ— å˜åŒ–pg_ranks_by_dim = tmp.reshape(-1, mesh.size(1))[  [0,1],  # pp=0  [2,3],  [4,5],  [6,7]]\n\nä¸‰ã€æ ¸å¿ƒæ¥å£ä¸å†…éƒ¨å®ç°è§£æ\n1. å±æ€§ä¸æ–¹æ³•\nmesh.shape  # tuple(self.mesh.shape)mesh.ndim   # int(self.mesh.ndim)mesh.size(dim=None)  # æ€»å…ƒç´ æ•° or self.mesh.size(dim)\nç”¨äºè·å– mesh å…ƒç»“æ„å’Œè§„æ¨¡ï¼Œé€‚ç”¨äºåˆ¤æ–­ç»´åº¦æ•°é‡ã€å¾ªç¯è¿­ä»£ã€å¹¶è¡Œç­–ç•¥é…ç½®ç­‰åœºæ™¯ã€‚\n\n2. Rank ä¸åæ ‡\n\nget_rank()ï¼šç­‰ä»·äº torch.distributed.get_rank()ï¼Œè¿”å›å…¨å±€ rankï¼›\nget_local_rank(mesh_dim)ï¼šå†…éƒ¨è°ƒç”¨ get_rank(self.get_group(mesh_dim)) â†’ å½“å‰ç»´åº¦çš„å°ç»„å†…ç¼–å·ï¼›\nget_coordinate()ï¼šè¿”å› self._coordinate_on_dimï¼Œå…¶åœ¨åˆå§‹åŒ–ä¸­é€šè¿‡ (self.mesh==global_rank).nonzero() è·å¾—ã€‚\n\nç¤ºä¾‹ï¼šmesh_shape=(4,2)ï¼Œrank=5 â†’ local_pp=2ã€local_tp=1ï¼Œcoordinate [2,1]ã€‚\n\n3. é€šä¿¡ç»„è·å–\n\nget_group(mesh_dim)ï¼š\n\nè‹¥ 1D ä¸”ä¸ä¼ å‚ï¼Œç›´æ¥è¿”å›å”¯ä¸€å­è¿›ç¨‹ç»„ï¼›\nå¤šç»´åˆ™æ ¹æ® mesh_dimï¼ˆç´¢å¼•æˆ–åå­—ï¼‰æ£€ç´¢ self._dim_group_infos[dim]ï¼Œç”¨ _find_pg_by_ranks_and_tag() è·å–å¯¹åº” ProcessGroupã€‚\n\nget_all_groups()ï¼šè¿”å›æ‰€æœ‰ç»´åº¦çš„ group åˆ—è¡¨ï¼›\n__getitem__(dims)ï¼šåˆ‡ç‰‡æ¥å£è°ƒç”¨ _mesh_resources._get_slice_mesh_dims(...)ï¼Œåˆ›å»ºæ–°çš„å­ meshï¼Œä¿ç•™åº•å±‚ communicatorï¼Œä½†ç»´åº¦é™ã€‚\n\næ”¯æŒå•ç»´æˆ–å¤šç»´åˆ‡ç‰‡ï¼Œä¸”è¿”å›çš„ submesh é¡ºåºæŒ‰ä¼ å…¥é¡ºåºæ’åˆ— (discuss.ray.io, gemfury.com, pytorch.org)ã€‚\n\n\n\n4. from_group(...) æ–¹æ³•\n\nå¯æ¥å—å• group æˆ– group åˆ—è¡¨ï¼›\nåˆ›å»ºæ–°çš„ DeviceMesh æ—¶ä¸ä¼šè°ƒç”¨ backend åˆå§‹åŒ–ï¼›\nä¼šå¤ç”¨ç°æœ‰ ProcessGroupï¼Œå¹¶å¡«å…… _dim_group_infosï¼Œå› æ­¤ get_group(...) å°†ç›´æ¥è¿”å›ä¼ å…¥çš„å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º groupã€‚\n\n\nå››ã€å®Œæ•´å•æœº 8 å¡ Demoï¼štp=2, pp=4\nä¸‹é¢æ¼”ç¤ºå¦‚ä½•è°ƒç”¨æ‰€æœ‰æ¥å£å¹¶è¾“å‡ºç»“æœã€‚æ³¨æ„ï¼šéœ€åœ¨ torchrun --nproc_per_node=8 ä¸‹è¿è¡Œã€‚\nimport os, torch, torch.distributed as distfrom torch.distributed.device_mesh import init_device_meshdef run_device_mesh_demo():    dist.init_process_group(&quot;nccl&quot;)    # â¬‡ï¸ åˆå§‹åŒ– 2-ç»´ meshï¼špp=4, tp=2    mesh = init_device_mesh(&quot;cuda&quot;, mesh_shape=(4, 2), mesh_dim_names=(&quot;pp&quot;, &quot;tp&quot;))        # âœ… rank å’Œåæ ‡    gr = mesh.get_rank()            # å…¨å±€ rank    coord = mesh.get_coordinate()   # [pp_idx, tp_idx]    local_pp = mesh.get_local_rank(&quot;pp&quot;)    local_tp = mesh.get_local_rank(&quot;tp&quot;)        # â¬‡ï¸ mesh åŸºæœ¬ç»“æ„    total = mesh.size()    pp_size, tp_size = mesh.size(&quot;pp&quot;), mesh.size(&quot;tp&quot;)    ndim = mesh.ndim    shape = mesh.shape        # â¬‡ï¸ è·å–é€šä¿¡ç»„    pp_group = mesh.get_group(&quot;pp&quot;)    tp_group = mesh.get_group(&quot;tp&quot;)    all_groups = mesh.get_all_groups()        # â¬‡ï¸ åˆ‡ç‰‡å‡ºå­ mesh    tp_mesh = mesh[&quot;tp&quot;]    pp_mesh = mesh[&quot;pp&quot;]        # â¬‡ï¸ è¾“å‡ºç»“æœ    print(f&quot;rank=&#123;gr&#125;, coord=&#123;coord&#125;, local_pp=&#123;local_pp&#125;, local_tp=&#123;local_tp&#125;&quot;)    print(f&quot;ndim=&#123;ndim&#125;, shape=&#123;shape&#125;, total=&#123;total&#125;, pp=&#123;pp_size&#125;, tp=&#123;tp_size&#125;&quot;)    print(&quot;pp_group ranks:&quot;, dist.get_process_group_ranks(pp_group))    print(&quot;tp_group ranks:&quot;, dist.get_process_group_ranks(tp_group))    print(&quot;all_groups sizes:&quot;, [len(dist.get_process_group_ranks(g)) for g in all_groups])    print(&quot;tp_mesh ndim, shape:&quot;, tp_mesh.ndim, tp_mesh.shape)    print(&quot;pp_mesh ndim, shape:&quot;, pp_mesh.ndim, pp_mesh.shape)if __name__ == &quot;__main__&quot;:    run_device_mesh_demo()\nğŸ’¬ é¢„æœŸè¾“å‡ºï¼ˆä¾‹å¦‚ rank = 5ï¼‰ï¼š\nrank=5, coord=[2,1], local_pp=2, local_tp=1 ndim=2, shape=(4,2), total=8, pp=4, tp=2 pp_group ranks: [4,5,6,7] tp_group ranks: [5,7] all_groups sizes: [4,2] tp_mesh ndim, shape: 1 (2,) pp_mesh ndim, shape: 1 (4,)\nè¯´æ˜ï¼š - rank=5 ä½äº pipeline æ®µ 2ï¼Œtp å†…ç¼–å· 1ï¼› - pp_group åŒ…å«ä¸å…¶åŒ segment çš„ 4 å¼ å¡ï¼› - tp_group åŒ…å«åŒ segment tp ç»´åº¦çš„ä¸¤å¼ å¡ï¼› - åˆ‡ç‰‡å tp_meshã€pp_mesh æˆä¸º 1 ç»´ç»“æ„ï¼Œç”¨äºåç»­ parallelizationã€‚\n\nğŸ‘ æ€»ç»“\n\nDeviceMesh æ„å»ºè‡ªèº«é€šè¿‡ init_device_mesh() å®Œæˆåˆå§‹åŒ–ä¸å­ç»„æ‹†åˆ†ï¼›\næ¥å£å†…éƒ¨å®ç°é€»è¾‘ä¸ Group ç®¡ç†æœºåˆ¶æ¸…æ™°ã€é«˜æ•ˆï¼›\n__getitem__ä¸ºå¤šç»´å¹¶è¡Œä¸‹å­ Mesh åˆ‡ç‰‡å…³é”®å·¥å…·ï¼Œå¯¹é›†æˆ parallel APIs è‡³å…³é‡è¦ï¼›\né€šè¿‡è¯¥æœºåˆ¶ï¼Œå¯ä»¥ç®€å•åœ°ç»„ç»‡å¤æ‚çš„ hybrid-parallel pipelinesï¼ŒåŒæ—¶å……åˆ†å¤ç”¨ communicator èµ„æºå¹¶ç®€åŒ–å¼€å‘æµç¨‹ã€‚\n\n","categories":["åˆ†å¸ƒå¼åŸºç¡€"],"tags":["devicemesh"]},{"title":"pytorch send and recv","url":"/2025/06/14/distribute/send_recv/","content":"\n\n1. åŸºæœ¬æ¦‚å¿µä¸è¿›ç¨‹ç»„\n2. åŸºæœ¬å¼ é‡é€šä¿¡\n3. å¯¹è±¡åˆ—è¡¨é€šä¿¡\n4. æ˜“é”™ç‚¹ä¸å¸¸è§é—®é¢˜\n5. æ‰¹é‡ç‚¹å¯¹ç‚¹é€šä¿¡æ¥å£\n6. æ€»ç»“è¡¥å……\n7. å‚è€ƒèµ„æ–™\n\n\n1. åŸºæœ¬æ¦‚å¿µä¸è¿›ç¨‹ç»„\n\ngroupï¼ˆé€šä¿¡ç»„ï¼‰ï¼šåˆ†å¸ƒå¼é€šä¿¡æ—¶çš„ã€Œå­é›†ã€ï¼Œå…è®¸åªåœ¨ä¸€éƒ¨åˆ† rank ä¹‹é—´é€šä¿¡ã€‚\nglobal rankï¼šå…¨å±€è¿›ç¨‹ç¼–å·ï¼ˆè¿›ç¨‹å¯åŠ¨æ—¶åˆ†é…çš„ç¼–å·ï¼‰ã€‚\ngroup rankï¼šç»„å†…è¿›ç¨‹ç¼–å·ï¼Œç»„å†…ç¬¬å‡ ä¸ªè¿›ç¨‹ï¼ˆä¸ global rank æ— å¿…ç„¶å¯¹åº”å…³ç³»ï¼‰ã€‚\nsrc/dstï¼šé€šä¿¡ç›®æ ‡ï¼ˆæº/ç›®çš„ï¼‰rankï¼Œæ³¨æ„ï¼šå¦‚æœæŒ‡å®š groupï¼Œè¿™é‡Œæ˜¯ç»„å†…ç¼–å·ï¼Œä¸æ˜¯å…¨å±€ç¼–å·ã€‚\n\nè¿›ç¨‹ç»„ä¸¾ä¾‹\nå‡å¦‚ group = [2, 4, 6, 8, 10]ï¼š\n\n\n\ngroup_rank\nglobal_rank\n\n\n\n\n0\n2\n\n\n1\n4\n\n\n2\n6\n\n\n3\n8\n\n\n4\n10\n\n\n\n\n2. åŸºæœ¬å¼ é‡é€šä¿¡\n2.1 send / recv / isend / irecv\nå‚æ•°è¯´æ˜\n\nsend(tensor, dst, group=None, tag=0) å‘é€ tensor åˆ°ç»„å†… rank=dst çš„è¿›ç¨‹ã€‚\nrecv(tensor, src, group=None, tag=0) ä»ç»„å†… rank=src çš„è¿›ç¨‹æ¥æ”¶ tensorã€‚\nisend/irecv å¼‚æ­¥ç‰ˆæœ¬ï¼Œè¿”å› Work å¥æŸ„ï¼Œéœ€è¦ work.wait()ã€‚\n\ntag\n\ntag æ˜¯æ¶ˆæ¯ç¼–å·/æ ‡ç­¾ï¼Œç”¨äºåŒºåˆ†å¤šæ¡å¹¶å‘æ¶ˆæ¯ï¼Œåªæœ‰ tag ä¸€è‡´æ‰èƒ½æ­£ç¡®é…å¯¹ã€‚\n\ngroup_dst/group_src\n\nä¸€èˆ¬ä¸ç”¨æ‰‹åŠ¨ä¼ ï¼Œæ¡†æ¶ä¼šæ ¹æ® dst/src å’Œ group è‡ªåŠ¨æ¨ç®—ã€‚\n\n\n2.2 é€šä¿¡æµç¨‹ç¤ºæ„å›¾\nä»¥ group = [2, 4, 6, 8, 10]ï¼Œè®© rank=2 å‘ï¼Œrank=10 æ”¶ä¸ºä¾‹ï¼š\ngraph TD    subgraph group [group: [2, 4, 6, 8, 10]]        A[&quot;global_rank=2&lt;br&gt;group_rank=0&quot;]        B[&quot;global_rank=10&lt;br&gt;group_rank=4&quot;]    end    A -- send(tensor, dst=4, group=group) --&gt; B    B -- recv(tensor, src=0, group=group) --&gt; A\n\nå‘é€ç«¯ï¼ˆglobal_rank=2ï¼Œgroup_rank=0ï¼‰ï¼šsend(tensor, dst=4, group=group)\næ¥æ”¶ç«¯ï¼ˆglobal_rank=10ï¼Œgroup_rank=4ï¼‰ï¼šrecv(tensor, src=0, group=group)\n\n\n2.3 ä»£ç å®ä¾‹\n# å‘é€ç«¯ï¼ˆglobal_rank=2ï¼‰group = dist.new_group([2, 4, 6, 8, 10])tensor = torch.tensor([123])dist.send(tensor, dst=4, group=group)   # dst=4 æ˜¯ group å†… rank=4 â†’ global_rank=10# æ¥æ”¶ç«¯ï¼ˆglobal_rank=10ï¼‰group = dist.new_group([2, 4, 6, 8, 10])tensor = torch.zeros(1, dtype=torch.int)dist.recv(tensor, src=0, group=group)   # src=0 æ˜¯ group å†… rank=0 â†’ global_rank=2print(tensor)\n\nâš ï¸ åªè¦ç”¨äº† groupï¼Œsrc/dst éƒ½æ˜¯ç»„å†… rankï¼Œä¸æ˜¯ global rankï¼\n\n\n2.4 å¼‚æ­¥é€šä¿¡ï¼ˆisend/irecvï¼‰\nwork = dist.isend(tensor, dst=4, group=group)work.wait()  # ç­‰å¾…å‘é€å®Œæˆ\nå¼‚æ­¥ recv åŒç†ã€‚\n\n3. å¯¹è±¡åˆ—è¡¨é€šä¿¡\n3.1 send_object_list / recv_object_list ç”¨æ³•\n\nç”¨äºå‘é€/æ¥æ”¶åŒ…å«ä»»æ„ Python å¯¹è±¡çš„ listï¼Œåº•å±‚é€šè¿‡åºåˆ—åŒ–å®ç°ã€‚\nå‘é€è¿‡ç¨‹æ‹†ä¸ºä¸¤æ­¥ï¼šå…ˆå‘æ¯ä¸ªå¯¹è±¡åºåˆ—åŒ–åçš„ sizeï¼Œå†å‘æ‰€æœ‰å†…å®¹æ‹¼æ¥åçš„ tensorã€‚\n\n\n3.2 å¯¹è±¡é€šä¿¡æµç¨‹å›¾\nsequenceDiagram    participant Sender    participant Receiver    Sender-&gt;&gt;Receiver: send(object_sizes_tensor)    Sender-&gt;&gt;Receiver: send(object_tensor)    Receiver-&gt;&gt;Receiver: 1. è¯»å– object_sizes_tensor    Receiver-&gt;&gt;Receiver: 2. æŒ‰ size æ‹† object_tensor    Receiver-&gt;&gt;Receiver: 3. ååºåˆ—åŒ–ä¸ºå¯¹è±¡\n\n3.3 å…¸å‹ä»£ç ç¤ºä¾‹\nå‘é€ç«¯\nobject_list = [&quot;hello&quot;, 123, [1, 2, 3]]dist.send_object_list(object_list, dst=4, group=group)\næ¥æ”¶ç«¯\nrecv_list = [None, None, None]dist.recv_object_list(recv_list, src=0, group=group)print(recv_list)  # [&#x27;hello&#x27;, 123, [1, 2, 3]]\n\n3.4 æ¥å£å®ç°æ ¸å¿ƒä»£ç \n# æ¥æ”¶ç«¯åˆ†å‰²ååºåˆ—åŒ–offset = 0for i, obj_size in enumerate(object_sizes_tensor):    obj_view = object_tensor[offset : offset + obj_size]    object_list[i] = _tensor_to_object(obj_view, obj_size, group)    offset += obj_size\n\nobject_sizes_tensor è®°å½•æ¯ä¸ªå¯¹è±¡çš„åºåˆ—åŒ–é•¿åº¦\nobject_tensor æ˜¯æ‰€æœ‰å†…å®¹æ‹¼èµ·æ¥çš„ä¸€ç»´ tensor\næŒ‰é¡ºåºåˆ‡ç‰‡å’Œååºåˆ—åŒ–ï¼Œå¡«å› object_list\n\n\n3.5 å…³äº rank_objects\n\nrank_objects æ˜¯ recv çš„è¿”å›å€¼ï¼Œè¡¨ç¤ºæ¶ˆæ¯æ¥è‡ªå“ªä¸ª rankï¼ˆä¸€èˆ¬ç­‰äº srcï¼‰\nåœ¨å¤šå¯¹å¤šé€šä¿¡æˆ– src=ANY_SOURCE æ—¶ç”¨æ¥ç¡®è®¤æ¶ˆæ¯æ¥æºï¼Œå’Œå®é™…å¯¹è±¡å†…å®¹è¿˜åŸæ— å…³\n\n\n4. æ˜“é”™ç‚¹ä¸å¸¸è§é—®é¢˜\n\nåªè¦ç”¨äº† groupï¼Œsrc/dst éƒ½æ˜¯ç»„å†… rankï¼Œä¸æ˜¯ global rankã€‚\ntag ç”¨äºåŒºåˆ†å¤šæ¡æ¶ˆæ¯ï¼Œå¿…é¡» send å’Œ recv ä¸€è‡´ã€‚\nsend_object_list/recv_object_list å¿…é¡» object_list é•¿åº¦ã€é¡ºåºä¸€è‡´ã€‚\ngroup_src/group_dst æ­£å¸¸ä¸šåŠ¡ä¸éœ€è¦è‡ªå·±ä¼ ã€‚\n\n4.1. groupã€src/dstã€group_src/group_dst å‚æ•°å…³ç³»\n\ngroup å†³å®šé€šä¿¡å­é›†ï¼Œsrc/dst å†³å®šæ”¶å‘ç›®æ ‡ç¼–å·ã€‚\nå¦‚æœæŒ‡å®š groupï¼Œåˆ™ src/dst ä¸ºç»„å†… rankï¼Œä¸æ˜¯ global rankã€‚\ngroup_src/group_dst ä¸€èˆ¬ä¸ç”¨æ‰‹åŠ¨ä¼ ï¼Œæ¡†æ¶è‡ªåŠ¨æ¨ç®—ã€‚\næ˜ å°„å…³ç³»ï¼š\n\nå…¨å±€è½¬ç»„å†…ï¼šgroup_ranks.index(global_rank)\nç»„å†…è½¬å…¨å±€ï¼šgroup_ranks[group_rank]\n\n\n\n5. æ‰¹é‡ç‚¹å¯¹ç‚¹é€šä¿¡æ¥å£\n5.1 æ¥å£ç®€ä»‹\ntorch.distributed.batch_isend_irecv æ”¯æŒåŒæ—¶å‘èµ·å¤šç»„å¼‚æ­¥ç‚¹å¯¹ç‚¹é€šä¿¡æ“ä½œï¼ˆisend/irecvï¼‰ï¼Œæ˜¾è‘—æé«˜å¤§æ‰¹é‡æ•°æ®åˆ†å‘/æ”¶é›†çš„æ•ˆç‡ã€‚ åº•å±‚æ”¯æŒ NCCLã€Glooã€UCC ç­‰åˆ†å¸ƒå¼åç«¯ï¼Œå¸¸ç”¨äºåˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ çš„ pipeline/é€šä¿¡ pattern ä¼˜åŒ–ã€‚\nå‡½æ•°ç­¾å\ntorch.distributed.batch_isend_irecv(p2p_op_list: list[P2POp]) -&gt; list[Work]\n\np2p_op_listï¼šä¸€ç»„ torch.distributed.P2POp å®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹æè¿°ä¸€æ¬¡ isend/irecvã€‚\nè¿”å›ï¼šæ‰€æœ‰æ“ä½œçš„ request å¥æŸ„ï¼ˆWork å¯¹è±¡ï¼‰åˆ—è¡¨ï¼Œå¯é€šè¿‡ .wait() åŒæ­¥ã€‚\n\n\n5.2 å…¸å‹ä½¿ç”¨åœºæ™¯\n\nå¤§æ‰¹é‡ç‚¹å¯¹ç‚¹é€šä¿¡ï¼Œä¾‹å¦‚ pipeline å¹¶è¡Œã€ç¯å½¢ allreduce æ‰‹å†™ä¼˜åŒ–ç­‰åœºæ™¯ã€‚\næ”¯æŒ isend/irecv æ··åˆï¼Œèƒ½æ‰¹é‡æå‡ååé‡ã€‚\n\n\n5.3 è°ƒç”¨æµç¨‹ä¸å‚æ•°è¯´æ˜\nP2POp ç”¨æ³•\næ¯ä¸ª P2POp å®šä¹‰ä¸€æ¬¡é€šä¿¡æ“ä½œï¼Œå¦‚ä¸‹ï¼š\nP2POp(op, tensor, peer, group=None, tag=0)\n\nopï¼šæ“ä½œç±»å‹ï¼ˆdist.isend æˆ– dist.irecvï¼‰\ntensorï¼šè¦å‘é€/æ¥æ”¶çš„ tensor\npeerï¼šç›®æ ‡ peer çš„ç¼–å·ï¼ˆç»„å†… rankï¼‰\ngroupï¼ˆå¯é€‰ï¼‰ï¼šé€šä¿¡ç»„ï¼ˆé»˜è®¤ä¸º worldï¼‰\ntagï¼ˆå¯é€‰ï¼‰ï¼šæ¶ˆæ¯ç¼–å·/æ ‡ç­¾\n\n\n5.4 ä»£ç å®ä¾‹\nå‡è®¾ world_size=2ï¼Œrank 0 å’Œ rank 1 åšä¸€ä¸ªç¯å½¢é€šä¿¡ï¼š\nimport torchimport torch.distributed as distrank = dist.get_rank()world_size = dist.get_world_size()send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rankrecv_tensor = torch.zeros(2, dtype=torch.float32)send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1) % world_size)recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size) % world_size)reqs = dist.batch_isend_irecv([send_op, recv_op])for req in reqs:    req.wait()print(f&quot;Rank &#123;rank&#125; æ”¶åˆ°: &#123;recv_tensor&#125;&quot;)\nè¿è¡Œç»“æœï¼š\nRank 0 æ”¶åˆ°: tensor([2., 3.])Rank 1 æ”¶åˆ°: tensor([0., 1.])\n\n5.5 é€šä¿¡æµç¨‹å›¾\nsequenceDiagram    participant Rank0    participant Rank1    Rank0-&gt;&gt;Rank1: isend(send_tensor, dst=1)    Rank1-&gt;&gt;Rank0: isend(send_tensor, dst=0)    Rank0-&gt;&gt;Rank0: irecv(recv_tensor, src=1)    Rank1-&gt;&gt;Rank1: irecv(recv_tensor, src=0)    Note over Rank0,Rank1: batch_isend_irecv([send_op, recv_op])&lt;br&gt;å¹¶å‘å‘èµ·é€šä¿¡å¹¶ç­‰å¾…å®Œæˆ\n\n5.6 é‡è¦æ³¨æ„äº‹é¡¹\n\næ³¨æ„\n\nå¦‚æœä½¿ç”¨ NCCL åç«¯ï¼Œå¿…é¡»æå‰ç”¨ torch.cuda.set_device è®¾ç½®å¥½å½“å‰ GPUï¼\nå¦‚æœè¿™æ˜¯æŸä¸ª group çš„ç¬¬ä¸€æ¬¡é€šä¿¡ï¼Œgroup é‡Œçš„æ‰€æœ‰ rank å¿…é¡»éƒ½è°ƒç”¨ batch_isend_irecvï¼Œå¦åˆ™è¡Œä¸ºæœªå®šä¹‰ã€‚\nä»¥ååªè¦ä¸æ˜¯ç¬¬ä¸€æ¬¡ collectiveï¼Œå…è®¸åªç”¨éƒ¨åˆ† rank å‚ä¸ã€‚\n\n\n\n5.7 æºç å®ç°è¦ç‚¹\n\nè‡ªåŠ¨åˆ¤æ–­é€šä¿¡åç«¯æ˜¯å¦æ”¯æŒæ“ä½œåˆå¹¶ï¼ˆcoalescingï¼‰ï¼Œå¦‚ NCCL ä¼šåœ¨åŒä¸€ä¸ªä¸Šä¸‹æ–‡ä¸‹æ‰¹é‡å¯åŠ¨ï¼Œæå‡æ€§èƒ½ã€‚\nè¿”å›æ‰€æœ‰ requestï¼ˆWorkï¼‰å¯¹è±¡ï¼Œç”¨æˆ·å¯ wait()ã€‚\n\n\n5.8 API æ–‡æ¡£é“¾æ¥\n\nPyTorch å®˜æ–¹ batch_isend_irecv æ–‡æ¡£\nP2POp å®˜æ–¹è¯´æ˜\n\n\n6. æ€»ç»“è¡¥å……\n\nå¼ é‡ç‚¹å¯¹ç‚¹é€šä¿¡ï¼šsend/recv/isend/irecv/batch_isend_irecv\nå¯¹è±¡é€šä¿¡ï¼šsend_object_list/recv_object_list\næ‰¹é‡ç‚¹å¯¹ç‚¹é€šä¿¡èƒ½æå¤§æå‡ pipeline é€šä¿¡æ•ˆç‡\nç»Ÿä¸€è¿”å› Work å¥æŸ„ï¼Œæ”¯æŒåŒæ­¥æˆ–å¼‚æ­¥\ngroup/src/dst ä½¿ç”¨æ–¹å¼åŒä¸Šæ–‡æè¿°\n\n\n7. å‚è€ƒèµ„æ–™\n\nPyTorch Distributed å®˜æ–¹æ–‡æ¡£\nPyTorch distributed_c10d.py æºç \nMermaid Live Editor\n\n\n","categories":["åˆ†å¸ƒå¼åŸºç¡€"],"tags":["send recv"]},{"title":"pytorch Shard","url":"/2025/06/20/distribute/shard/","content":"\n\n1. _split_tensoråˆ†æ\n1.1 ä»£ç å®ç°æµç¨‹å›¾ï¼ˆMermaidï¼‰\nflowchart TD  A[&quot;è¾“å…¥ï¼štensor, num_chunks, with_padding, contiguous&quot;] --&gt; B&#123;&quot;dim â‰¤ tensor.ndim?&quot;&#125;  B -- å¦ --&gt; E[&quot;AssertionError æŠ›å‡º&quot;]  B -- æ˜¯ --&gt; C[&quot;è°ƒç”¨ torch.chunk æ²¿ dim åˆ†å—&quot;]  C --&gt; D[&quot;tensor_list, è®¡ç®— num_empty_tensors = num_chunks - len(tensor_list)&quot;]  D --&gt; F&#123;&quot;æ— éœ€ padding æˆ– å‡åŒ€å¯åˆ†?&quot;&#125;  F -- æ˜¯ --&gt; G[&quot;(å¯é€‰) å¯¹æ¯å—è°ƒç”¨ .contiguous()&quot;]  G --&gt; H[&quot;è°ƒç”¨ fill_empty_tensor_to_shards è¡¥ç©º shard&quot;]  H --&gt; I[&quot;è¿”å› shards åˆ—è¡¨ å’Œ ç©º pad_sizes []&quot;]  F -- å¦ --&gt; J[&quot;è®¡ç®— full_chunk_size = ceil(dim_size / num_chunks)&quot;]  J --&gt; K[&quot;æ”¶é›†åŸå§‹ chunk_sizes&quot;]  K --&gt; L[&quot;pad_sizes = full_chunk_size - chunk_size&quot;]  L --&gt; M[&quot;è°ƒç”¨ fill_empty_tensor_to_shards è¡¥ç©º shard&quot;]  M --&gt; N[&quot;å¯¹æ¯ä¸ª shardï¼šè‹¥ pad_size &gt; 0ï¼Œåˆ™ pad_tensor(shard, dim, pad_size)&quot;]  N --&gt; O[&quot;(å¯é€‰) shard.contiguous()&quot;]  O --&gt; P[&quot;æ”¶é›† shard_list å’Œ pad_sizes&quot;]  P --&gt; Q[&quot;è¿”å› shard_list å’Œ pad_sizes&quot;]\n\n1.2 å…³é”®ç‚¹è¯¦è§£\nğŸ§  ä¸ºä»€ä¹ˆè¦ Paddingï¼Ÿ\nç”¨äºä¿è¯åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ï¼ˆæ¯”å¦‚ scatterã€all_gather ç­‰ collective æ“ä½œï¼‰æ¯ä¸ª rank çš„ shard å¤§å°ä¸€è‡´ï¼Œé¿å…å› ä¸ºå°ºå¯¸ä¸å¯¹é½å¯¼è‡´é€šä¿¡å¤±è´¥ã€‚åªæœ‰ tensor.size(dim) % num_chunks â‰  0 ä¸” with_padding=True æ—¶ï¼Œæ‰ä¼šè¿›è¡Œ paddingã€‚\nğŸ§© fill_empty_tensor_to_shards\ntorch.chunk åœ¨å°ºå¯¸è¾ƒå°æˆ– num_chunks æ›´å¤§æ—¶ä¸ä¼šè¾“å‡ºç©º tensorã€‚è¯¥å‡½æ•°ç”¨äºè¡¥å…¨ï¼šåœ¨ tensor_list å°‘äº num_chunks æ—¶ï¼Œè¡¥å……å½¢çŠ¶åˆæ³•ä½† dim ä¸Šä¸º 0 çš„ç©º tensorï¼Œä½¿ shard æ•°ç›®ä¸€è‡´ï¼Œä¾¿äºåç»­ç»Ÿä¸€å¤„ç†ã€‚\nğŸ§¼ pad_tensor\nè‹¥å½“å‰ shard å°äº full_chunk_sizeï¼Œåˆ™åœ¨æŒ‡å®šç»´åº¦æœ«å°¾è¡¥é›¶ï¼Œç¡®ä¿æ‰€æœ‰ shard çš„å½¢çŠ¶ä¸€è‡´ã€‚\nğŸ§± contiguous\nä¸ºæå‡å†…å­˜è¿è´¯æ€§å’Œé€šä¿¡æ•ˆç‡ï¼Œå¯è°ƒç”¨ .contiguous() é‡æ’å†…å­˜å¸ƒå±€ã€‚\n\n1.3 å®é™…è°ƒç”¨ç¤ºä¾‹ï¼ˆéœ€ Paddingï¼‰\nä»¥ä¸‹ä¸ºæ— æ³•å‡åŒ€åˆ†ç‰‡ï¼Œå›  num_chunks=4 è€Œè§¦å‘ pad çš„åœºæ™¯ï¼š\nimport torchfrom torch.distributed.tensor.placement_types import Shard# æ„é€ å¼ é‡tensor = torch.arange(1, 13).reshape(2, 6)  # shape [2, 6]# åœ¨ dim=1 ä¸Šæ‹†ä¸º 4 ä»½ï¼Œä¸æ•´é™¤å°†è§¦å‘ paddingsharder = Shard(dim=1)shards, pad_sizes = sharder._split_tensor(tensor, num_chunks=4, with_padding=True)print(&quot;Pad sizes:&quot;, pad_sizes)for i, (sh, pad) in enumerate(zip(shards, pad_sizes)):    print(f&quot;Shard &#123;i&#125; shape: &#123;tuple(sh.shape)&#125;, pad: &#123;pad&#125;&quot;)    print(sh)\nâœ… é¢„æœŸç»“æœ\n\ntensor.size(1)=6, num_chunks=4 â‡’ full_chunk_size = ceil(6/4) = 2\ntorch.chunk ä¼šå‡º 4 å—ï¼Œä½†æœ€åä¸€ä¸¤å—å¯èƒ½ä¸º empty\npad_sizes å¯èƒ½ä¸º [0, 0, 0, 2]\næœ€ç»ˆæ¯å—å¤§å°éƒ½æ˜¯ [2] (dim=1)ï¼Œpadding è¡¥é½\n\nPad sizes: [0, 0, 0, 2]Shard 0 shape: (2, 2), pad: 0tensor([[1, 2],        [7, 8]])Shard 1 shape: (2, 2), pad: 0tensor([[ 3,  4],        [ 9, 10]])Shard 2 shape: (2, 2), pad: 0tensor([[ 5,  6],        [11, 12]])Shard 3 shape: (2, 2), pad: 2tensor([[0, 0],        [0, 0]])\n\n1.4 æ€»ç»“\n\n_split_tensor çš„ä½œç”¨æ˜¯å°†ä¸€ä¸ª Tensor æ²¿æŒ‡å®šç»´åº¦åˆ‡åˆ†ä¸ºå›ºå®šä»½æ•°ï¼Œå¹¶åœ¨ ä¸èƒ½æ•´é™¤æ—¶è‡ªåŠ¨è¡¥é½ã€‚\nå®ƒä¿éšœäº†å„ shard åœ¨é€šä¿¡é˜¶æ®µå°ºå¯¸ä¸€è‡´ï¼Œé€‚ç”¨äºåˆ†å¸ƒå¼å¼ é‡å¹¶è¡Œåœºæ™¯ã€‚\nå®é™…ä»£ç é€šè¿‡ torch.chunkã€fill_empty_tensor_to_shardsã€pad_tensor ç­‰æ‰‹æ®µï¼Œè½»æ¾å®ç°è¿™ä¸€ç›®æ ‡ã€‚\n\n\n","categories":["åˆ†å¸ƒå¼åŸºç¡€"],"tags":["shard"]},{"title":"pytorchä¸­çš„streamå’Œevent","url":"/2025/09/07/distribute/stream_event/","content":"\n\n\nä¸€å¥è¯æ€»è§ˆï¼šæµï¼ˆstreamï¼‰æ˜¯ GPU ä¸Šçš„â€œæœ‰åºæŒ‡ä»¤é˜Ÿåˆ—â€ï¼Œäº‹ä»¶ï¼ˆeventï¼‰æ˜¯æ’åœ¨æµæ—¶é—´çº¿ä¸Šçš„â€œæ …æ /æ—¶é—´æˆ³â€ã€‚æŠŠ event.record() æ”¾åœ¨ç”Ÿäº§æµä¸Šï¼Œå†åœ¨æ¶ˆè´¹æµé‡Œ wait_event()ï¼Œå°±èƒ½åšåˆ°è®¾å¤‡ä¾§çš„æ— é˜»å¡ä¾èµ–ç¼–æ’ã€‚(docs.pytorch.org)\n\n\n1. åŸºæœ¬æ¦‚å¿µ\n\nStreamï¼ˆæµï¼‰ï¼šåŒä¸€æ¡æµå†…æŒ‰æäº¤é¡ºåºï¼ˆFIFOï¼‰æ‰§è¡Œï¼›ä¸åŒæµå½¼æ­¤ç‹¬ç«‹ï¼Œå¯å¹¶è¡Œè¿è¡Œã€‚PyTorch çš„ torch.cuda.Stream å°±æ˜¯ CUDA æµçš„å°è£…ï¼Œå¹¶æä¾› record_event / wait_event / wait_stream / synchronize ç­‰æ–¹æ³•ã€‚(docs.pytorch.org)\nEventï¼ˆäº‹ä»¶ï¼‰ï¼šåŒæ­¥æ ‡è®°ã€‚å¯ç”¨äºæµ‹æ—¶ä¸è·¨æµåŒæ­¥ï¼šåœ¨ç”Ÿäº§æµ record()ï¼Œåœ¨æ¶ˆè´¹æµ wait()/wait_event()ã€‚äº‹ä»¶ä¹Ÿå¯ elapsed_time() è¯»å–GPU ç«¯çš„æ¯«ç§’è®¡æ—¶ã€‚(docs.pytorch.org)\né»˜è®¤æµè¯­ä¹‰ï¼š\n\nLegacy default stream ä¼šä¸å…¶å®ƒï¼ˆé˜»å¡å‹ï¼‰æµäº’ç›¸åŒæ­¥ï¼›\nPer-thread default streamï¼ˆPTDSï¼‰ ä¸ä¸å…¶ä»–æµåŒæ­¥ï¼Œè¡Œä¸ºæ›´åƒæ˜¾å¼åˆ›å»ºçš„æµã€‚ ä¸¤è€…å¯åœ¨ç¼–è¯‘/å®å±‚é¢é€‰æ‹©ï¼Œè¡Œä¸ºä¸åŒä¼šå½±å“æ˜¯å¦â€œè‡ªåŠ¨åŒæ­¥â€ã€‚(NVIDIA Docs)\n\n\n\n2. ä¸‰ç§â€œç­‰å¾…â€çš„ä½œç”¨åŸŸï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n\nè®¾å¤‡çº§ï¼štorch.cuda.synchronize(device) â€”â€” ç­‰è¯¥è®¾å¤‡ä¸Šæ‰€æœ‰æµåˆ°å½“å‰ä¸ºæ­¢çš„å·¥ä½œå®Œæˆã€‚æœ€é‡ï¼Œä¸€èˆ¬å°‘ç”¨ã€‚ï¼ˆè¯­ä¹‰ç­‰åŒ cudaDeviceSynchronizeï¼‰(developer.download.nvidia.com)\nå•æµçº§ï¼šstream.synchronize() â€”â€” åªç­‰è¿™ä¸€æ¡æµå·²æäº¤çš„å·¥ä½œï¼Œç­‰åŒ cudaStreamSynchronizeã€‚(docs.pytorch.org)\näº‹ä»¶çº§ï¼ševent.synchronize() â€”â€” åªç­‰è¯¥äº‹ä»¶æ‰€æ•è·çš„å·¥ä½œï¼Œç­‰åŒ cudaEventSynchronizeã€‚ç²’åº¦æœ€ç»†ï¼Œæ¨èä¼˜å…ˆç”¨äº‹ä»¶æ¥è¡¨è¾¾ä¾èµ–ã€‚(docs.pytorch.org)\n\n\nå£è¯€ï¼šdevice &gt; stream &gt; eventï¼ˆç­‰å¾…èŒƒå›´ä»å¤§åˆ°å°ï¼‰ã€‚é€‰æœ€å°å¿…è¦èŒƒå›´ï¼Œä¿ç•™å¹¶è¡Œåº¦ã€‚(developer.download.nvidia.com)\n\n\n3. è·¨æµåŒæ­¥çš„ä¸‰ç§æ–¹å¼\n\näº‹ä»¶æ …æ ï¼ˆæ¨èï¼‰\n\nç”Ÿäº§æµï¼ševent.record()\næ¶ˆè´¹æµï¼šconsumer.wait_event(event)ï¼ˆæˆ– event.wait(consumer)ï¼‰ è¯¥è°ƒç”¨ç«‹å³è¿”å›ï¼Œåªæ˜¯æŠŠâ€œç­‰å¾… eâ€è¿™æ¡ä¾èµ–å†™è¿›äº†æ¶ˆè´¹æµçš„é˜Ÿåˆ—ï¼›åç»­æäº¤çš„å·¥ä½œéƒ½ä¼šåœ¨ e å®Œæˆåæ‰§è¡Œã€‚(docs.pytorch.org)\n\næµ-æµç­‰å¾…\n\nthis.wait_stream(that)ï¼šè®© this æµåç»­å·¥ä½œï¼Œç­‰å¾… that æµå½“å‰å·²æäº¤çš„å·¥ä½œå®Œæˆã€‚(docs.pytorch.org)\n\né»˜è®¤æµè¯­ä¹‰ï¼ˆå†å²å…¼å®¹ï¼‰\n\nè‹¥ä½¿ç”¨ legacy default streamï¼Œå®ƒä¼šä¸å…¶å®ƒé˜»å¡æµäº’ç›¸åŒæ­¥ï¼›PTDS åˆ™ä¸ä¼šã€‚æ–°ä»£ç ä¸å»ºè®®ä¾èµ–è¿™ç§â€œéšå¼åŒæ­¥â€ã€‚(NVIDIA Docs)\n\n\n\n4. å¼ é‡ç”Ÿå‘½å‘¨æœŸçš„å®‰å…¨ï¼ˆsafeï¼‰ç”¨æ³•\nè·¨æµå…±äº«åŒä¸€å—æ˜¾å­˜æ—¶ï¼Œé™¤äº†â€œå†™æ¸…æ¥šä¾èµ–â€ï¼ˆäº‹ä»¶/æµç­‰å¾…ï¼‰ï¼Œè¿˜åº”åœ¨ä½¿ç”¨è¯¥å¼ é‡çš„æµä¸Šè°ƒç”¨ï¼š\ntensor.record_stream(consumer_stream)\nè¿™ä¼šå‘Šè¯‰ CUDA ç¼“å­˜åˆ†é…å™¨ï¼šè¯¥å¼ é‡ä¹Ÿåœ¨ consumer_stream ä¸Šè¢«ç”¨è¿‡ï¼Œä»è€Œé¿å…åœ¨ç”Ÿäº§æµé‡Šæ”¾åè¢«è¿‡æ—©å¤ç”¨ï¼Œé€ æˆæ½œåœ¨è¯»å†™ç«æ€ã€‚å¦åˆ™éœ€è¦åœ¨é‡Šæ”¾å‰æŠŠä½¿ç”¨åŒæ­¥å›åˆ›å»ºæµã€‚(docs.pytorch.org)\n\n5. CPUâ†”GPU æ‹·è´ä¸ non_blocking / pinned memory\n\nåªæœ‰å½“é¡µé”å®šå†…å­˜ï¼ˆpinnedï¼‰å‚ä¸æ—¶ï¼Œå¾ˆå¤šæ‹·è´æ‰èƒ½çœŸæ­£å¼‚æ­¥åŒ–å¹¶ä¸è®¡ç®—é‡å ï¼›PyTorch æ•™ç¨‹å¯¹ pin_memory() ä¸ non_blocking=True çš„è¡Œä¸ºåšäº†ç³»ç»Ÿè¯´æ˜ã€‚(docs.pytorch.org)\nè¯»å– D2H ç»“æœå‰ï¼Œåº”ç­‰å¾…æ‹·è´å®Œæˆï¼ˆäº‹ä»¶æˆ–åŒæ­¥ï¼‰ï¼Œä¸è¦ç›´æ¥åœ¨ CPU ç«¯æ¶ˆè´¹å¼‚æ­¥ç»“æœã€‚(docs.pytorch.org)\n\næ¨èæ¨¡å¼ï¼ˆD2H æ‹·è´ä¸â€œå¡ä½â€æ•´æœºï¼Œåªåœ¨ç”¨åˆ°ç»“æœæ—¶å°èŒƒå›´ç­‰å¾…ï¼‰ï¼š\nimport torchx  = torch.randn(1_000_000, device=&quot;cuda&quot;)dst = torch.empty_like(x, device=&quot;cpu&quot;, pin_memory=True)  # pinned CPU buffercopy_stream = torch.cuda.Stream()copy_done   = torch.cuda.Event()with torch.cuda.stream(copy_stream):    dst.copy_(x, non_blocking=True)  # å¼‚æ­¥ D2H    copy_done.record()               # ä»…æ‹·è´å®Œæˆå¤„æ‰“ç‚¹# â€¦â€¦CPU å¯ä»¥å…ˆåšåˆ«çš„æ´»â€¦â€¦copy_done.synchronize()              # åªæœ‰åœ¨çœŸæ­£è¦ç”¨ dst æ—¶æ‰ç­‰è¿™ä¸€æ¬¡print(dst[:5])\n\nè¦ç‚¹ï¼špinned + ä¸“ç”¨æ‹·è´æµ + äº‹ä»¶ï¼›é¿å…ç”¨è®¾å¤‡çº§ torch.cuda.synchronize() ç²—æš´â€œåˆ¹è½¦â€ã€‚(docs.pytorch.org, developer.download.nvidia.com)\n\n\n6. å¯è¿è¡Œæœ€å°ç¤ºä¾‹\n6.1 è®¡ç®—æµ â†’ é€šä¿¡/åå¤„ç†æµï¼ˆäº‹ä»¶æ …æ ï¼‰\nimport torchdevice = &quot;cuda&quot;compute = torch.cuda.Stream()comm    = torch.cuda.Stream()done    = torch.cuda.Event()x = torch.randn(1_000_000, device=device)with torch.cuda.stream(compute):    y = x.relu()    done.record()           # è®°å½•â€œy å·²å°±ç»ªâ€comm.wait_event(done)       # è®© comm æµç­‰åˆ° y å°±ç»ªwith torch.cuda.stream(comm):    z = y * 2               # åœ¨ GPU ç«¯è‡ªåŠ¨ç­‰å¾…ï¼Œä¸é˜»å¡ CPUtorch.cuda.synchronize()    # ç¤ºä¾‹æ”¶å°¾ï¼šçœŸå®å·¥ç¨‹é‡Œå¯ç»§ç»­æäº¤åç»­å·¥ä½œ\næœºåˆ¶è¯´æ˜ï¼šwait_event æŠŠâ€œç­‰å¾… eâ€æ’å…¥åˆ°æ¶ˆè´¹æµé˜Ÿåˆ—ï¼Œåªæœ‰äº‹ä»¶è§¦å‘åï¼Œæ¶ˆè´¹æµåç»­ kernel æ‰ä¼šæ‰§è¡Œï¼›è¿™éƒ½æ˜¯è®¾å¤‡ä¾§å®Œæˆï¼ŒCPU ä¸è¢«é˜»å¡ã€‚(docs.pytorch.org)\n6.2 ä¸‰æµç¤ºä¾‹ï¼ˆS2 ä¸ S3 éƒ½ç­‰ S1ï¼‰\ns1, s2, s3 = torch.cuda.Stream(), torch.cuda.Stream(), torch.cuda.Stream()e = torch.cuda.Event()with torch.cuda.stream(s1):    a = torch.randn(1024, 1024, device=&quot;cuda&quot;) @ torch.randn(1024, 1024, device=&quot;cuda&quot;)    e.record()s2.wait_event(e)s3.wait_event(e)with torch.cuda.stream(s2):    b = a.relu_()with torch.cuda.stream(s3):    c = a.sum()\n\nåŒä¸€ä¸ªäº‹ä»¶å¯ä»¥è¢«å¤šæ¡æµç­‰å¾…ï¼Œé€‚åˆâ€œä¸€å¯¹å¤šâ€çš„ä¾èµ–ã€‚(docs.pytorch.org)\n\n6.3 GPU ç«¯ç²¾å‡†è®¡æ—¶ï¼ˆEvent elapsed_timeï¼‰\nimport torchs = torch.cuda.Stream()start = torch.cuda.Event(enable_timing=True)end   = torch.cuda.Event(enable_timing=True)x = torch.randn(4096, 4096, device=&quot;cuda&quot;)w = torch.randn(4096, 4096, device=&quot;cuda&quot;)# é¢„çƒ­for _ in range(2): (x @ w).sum().relu_()with torch.cuda.stream(s):    start.record()    y = (x @ w).relu_()    end.record()end.synchronize()print(f&quot;elapsed = &#123;start.elapsed_time(end):.3f&#125; ms&quot;)\n\nelapsed_time è¿”å› start.record ä¸ end.record ä¹‹é—´çš„ GPU æ¯«ç§’æ•°ï¼›end.synchronize() ç¡®ä¿æµ‹é‡é—­åŒºé—´å·²å®Œæˆã€‚(docs.pytorch.org)\n\n\n7. å¸¸è§å‘ä¸é€Ÿè®°\n\näº‹ä»¶ä½ç½®è¦å¯¹ï¼šrecord() åªè¦†ç›–å®ƒä¹‹å‰å·²å…¥é˜Ÿçš„å·¥ä½œï¼›ä¹‹åæ–°æäº¤çš„å·¥ä½œä¸åŒ…å«åœ¨æœ¬äº‹ä»¶å†…ã€‚ä½¿ç”¨æ—¶å°† record() æ”¾åœ¨ç”Ÿäº§ç»“æŸç‚¹ã€‚(docs.pytorch.org)\nwait_event/wait_stream å‡ä¸ºâ€œå†™ä¾èµ–ã€ç«‹å³è¿”å›â€ï¼šå®ƒä»¬ä¸ä¼šé˜»å¡ CPUï¼Œåªå½±å“åç»­æäº¤åˆ°è¯¥æµçš„å·¥ä½œã€‚(docs.pytorch.org)\né»˜è®¤æµé™·é˜±ï¼šLegacy ä¸ PTDS è¯­ä¹‰ä¸åŒã€‚æ··ç”¨æ—¶ï¼Œlegacy ä¼šä¸é˜»å¡æµäº’ç›¸ç­‰å¾…ï¼›PTDS ä¸ä¼šã€‚æ–°å·¥ç¨‹å»ºè®®æ˜¾å¼å»ºæµ + æ˜¾å¼åŒæ­¥ï¼Œé¿å…è¸©éšå¼åŒæ­¥ã€‚(NVIDIA Docs)\næµä¼˜å…ˆçº§ï¼šä½æ•°å­—=é«˜ä¼˜å…ˆçº§ï¼›åªæ˜¯â€œå€¾å‘â€ï¼Œä¸æŠ¢å å·²åœ¨è¿è¡Œçš„ kernelã€‚(NVIDIA Docs)\n\n\n8. æœ¯è¯­ä¸€é¡µçº¸\n\nStreamï¼šè®¾å¤‡ä¸Šç‹¬ç«‹çš„æœ‰åºæ‰§è¡Œé˜Ÿåˆ—ã€‚record_eventã€wait_eventã€wait_streamã€synchronizeã€‚(docs.pytorch.org)\nEventï¼šè®¾å¤‡ä¾§æ …æ /æ—¶é—´æˆ³ï¼›recordã€waitã€synchronizeã€elapsed_timeã€‚(docs.pytorch.org)\nå®‰å…¨è·¨æµï¼šå†™ä¾èµ– + tensor.record_stream(consumer)ï¼ˆæˆ–æ‰‹åŠ¨ç¡®ä¿é‡Šæ”¾å‰åŒæ­¥å›åˆ›å»ºæµï¼‰ã€‚(docs.pytorch.org)\né«˜æ•ˆ D2Hï¼špinned + ä¸“ç”¨æ‹·è´æµ + äº‹ä»¶ï¼›æŒ‰éœ€ç­‰å¾…ï¼Œé¿å…å…¨è®¾å¤‡åŒæ­¥ã€‚(docs.pytorch.org)\n\n\nå‚è€ƒèµ„æ–™ï¼ˆå¼ºçƒˆå»ºè®®ç»†è¯»åŸæ–‡ï¼‰\n\nPyTorchï¼štorch.cuda.Stream APIï¼ˆå« wait_event / wait_stream / synchronizeï¼‰ä¸æ–‡æ¡£æ³¨é‡Šã€‚(docs.pytorch.org)\nPyTorchï¼štorch.cuda.Event APIï¼ˆrecord / wait / synchronize / elapsed_timeï¼‰ã€‚(docs.pytorch.org)\nPyTorchï¼štensor.record_streamï¼ˆè·¨æµå†…å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼‰ã€‚(docs.pytorch.org)\nPyTorch æ•™ç¨‹ï¼špin_memory() ä¸ non_blocking ä½¿ç”¨ä¸æ³¨æ„äº‹é¡¹ã€‚(docs.pytorch.org)\nNVIDIA CUDA æ–‡æ¡£ï¼šé»˜è®¤æµï¼ˆLegacy vs PTDSï¼‰è¯­ä¹‰ä¸æµä¼˜å…ˆçº§è¯´æ˜ã€‚(NVIDIA Docs)\nNVIDIA åŸ¹è®­è®²ä¹‰ï¼šcudaDeviceSynchronize / cudaStreamSynchronize / cudaEvent* çš„åŒæ­¥å¯¹æ¯”ä¸ç¤ºä¾‹ã€‚(developer.download.nvidia.com)\n\n\n","categories":["åˆ†å¸ƒå¼åŸºç¡€"],"tags":["stream"]},{"title":"pytorchä¸­TCPStore Rendezvousæœºåˆ¶","url":"/2025/06/14/distribute/tcpstore_rendezvous/","content":"\n\nğŸ§  èƒŒæ™¯æ¦‚è¿°\n\nç›®æ ‡ï¼šåœ¨ init_process_group ä¸­å®ç°è·¨è¿›ç¨‹æ³¨å†Œã€æ’åºåŠ barrier åŒæ­¥ï¼Œä¸º NCCL/Gloo é€šä¿¡ç»„æ„å»ºåˆ›å»ºä¸€è‡´ä¸Šä¸‹æ–‡ã€‚\næ—¶åºï¼šæ‰€æœ‰ set/get/wait æ“ä½œå‡å‘ç”Ÿåœ¨ NCCL é€šä¿¡åˆå§‹åŒ–ä¹‹å‰ï¼ˆå³ rendezvous é˜¶æ®µï¼‰ã€‚\næœºåˆ¶ï¼šsocket å®¢æˆ·ç«¯â€”æœåŠ¡å™¨æ¨¡å‹ + backend æ§åˆ¶åŒæ­¥é€»è¾‘ã€‚\n\n\n1. æ¶ˆæ¯åè®®æ ¼å¼\nå®¢æˆ·ç«¯å‘ master å‘é€çš„åŒ…æ ¼å¼ä¸ºï¼š\n\\[4â€¯B æ€»é•¿åº¦]\\[1â€¯B æ“ä½œç ]\\[4â€¯B key\\_len]\\[4â€¯B value\\_len]\\[key]\\[value]\n\næ€»é•¿åº¦ï¼šç½‘ç»œå­—èŠ‚åºï¼Œä¸å«è‡ªèº«ï¼›\næ“ä½œç ï¼š1=SET, 2=GET, 3=WAITï¼›\nkey_len, value_lenï¼šåç»­å­—æ®µé•¿åº¦ï¼›\nkey, valueï¼šå®é™…æ•°æ®ï¼›\nMaster è§£æåï¼Œå›å¤ï¼šOK / value å†…å®¹ / READY ç­‰ã€‚\n\n\n2. Rendezvous é˜¶æ®µæµç¨‹ï¼ˆ2 æœºï¼Œ4 å¡ eachï¼Œèšç„¦ rank1 &amp; rank5ï¼‰\nflowchart TB  subgraph A[&quot;Machine A (rank0-3)&quot;]    master[&quot;TCPStoreBackend (master)&quot;]    r1[Worker rank1]    master --- r1  end  subgraph B[&quot;Machine B (rank4-7)&quot;]    r5[Worker rank5]    master --- r5  end  r1 --&gt;|SET key rank1_addr| master  r5 --&gt;|SET key rank5_addr| master  r1 --&gt;|WAIT  rendezvous_done| master  r5 --&gt;|WAIT  rendezvous_done| master  %% Server: waits until all ranks set, then:  master --&gt;|write READY| r1  master --&gt;|write READY| r5  %% å®Œæˆ WAIT è¿”å›ï¼Œè¿›å…¥ NCCL åˆå§‹åŒ–  r1 --&gt;|recv READY â†’ NCCL init| NCCL_1[NCCL Init rank1]  r5 --&gt;|recv READY â†’ NCCL init| NCCL_5[NCCL Init rank5]\nğŸ§© æ­¥éª¤è§£æ\n\nMaster åœ¨ç«¯å£ï¼ˆå¦‚ 29500ï¼‰ä¾¦å¬ï¼Œæ¥æ”¶è¿æ¥ï¼›\nrank1 / rank5 åˆ†åˆ«å‘é€ SETï¼ˆæ³¨å†Œåœ°å€ï¼‰ï¼›\néšåå‘é€ WAIT(\"rendezvous_done\")ï¼ŒSocket å¤„äºé˜»å¡çŠ¶æ€ï¼›\nMaster æ”¶é›†æ‰€æœ‰ 8 ä¸ª rank çš„ SET åï¼Œéå† wait é˜»å¡çš„è¿æ¥ï¼Œé€ä¸€å†™å…¥ READYï¼›\nWorker æ”¶åˆ° READYï¼Œé€€å‡ºé˜»å¡ï¼Œè¿›å…¥ NCCL åˆå§‹åŒ–é˜¶æ®µï¼›\néšååœ¨è¿™ä¸€é˜¶æ®µå†…ï¼šäº¤æ¢ ncclUniqueId (via store), è°ƒç”¨ ncclCommInitRank æ„å»ºé€šä¿¡ç»„ (github.com, pytorch.org)ã€‚\n\n\n3. Backend ç»†èŠ‚å¯¹æ¯”\n\n\n\n\n\n\n\n\nBackend\nI/O æ¨¡å‹\nç‰¹ç‚¹ä¸é€‚åº”æ€§\n\n\n\n\nç»å…¸ TCPStoreBackend\naccept() + per-conn é˜»å¡/POLL\nç®€å•ï¼Œè¿æ¥è¾ƒå¤šæ—¶æ‰©å±•æ€§å·®\n\n\nlibuv å¼‚æ­¥ Backend\nå•çº¿ç¨‹ event-loop, readable/writeable\né»˜è®¤å¯ç”¨ï¼ˆv2.4+ï¼‰ï¼Œé«˜å¹¶å‘æ›´ä¼˜ (docs.pytorch.org)\n\n\n\n\nlibuv backend ä½¿ç”¨ uv_read_start è‡ªåŠ¨åˆ†å—è¯»å–ï¼Œæ ¹æ® header æ§åˆ¶æ‹¼åŒ…ï¼›\næ³¨å†Œ WAIT æ—¶ï¼Œå°† conn ä¿å­˜åœ¨ map ä¸­ï¼Œä¸ç«‹å³å›å†™ï¼›å½“æ¡ä»¶æ»¡è¶³ï¼Œè§¦å‘ uv_write() â†’ uv_write_cb å®ç°å”¤é†’ã€‚\n\n\n4. partial-key WAIT æœºåˆ¶\n\nå®¢æˆ·ç«¯å¯ä»¥æ‰§è¡Œ store.wait([\"kA\", \"kB\"])ï¼›\nMaster å°†æ­¤ç­‰å¾…ç™»è®°è‡³ MultiWaitRegistryï¼›\nå½“ æ‰€æœ‰ç›¸å…³ key å‡è¢« SET åï¼Œæ‰ç»Ÿä¸€å‘è¯¥è¿æ¥å†™ READYï¼Œè§¦å‘å”¤é†’ã€‚\n\n\n5. â€œå¹¿æ’­ READYâ€ çš„å®ç°æœºåˆ¶\n\nä¸æ˜¯é€šè¿‡ NCCL/Gloo broadcast ç®—å­ï¼›\nMaster éå†æŒ‚èµ·çš„ WAIT socketsï¼Œé€ä¸ªå†™ READYï¼›\nä¸º rendezvous è¿‡ç¨‹è‡ªèº«æä¾›åŒæ­¥æœºåˆ¶ï¼Œé€šä¿¡ç»„å°šæœªåˆ›å»ºã€‚\n\n\n6. æ—¶é—´çº¿æ¦‚è§ˆ\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ SET/WAIT via TCP Store   â”‚  # rendezvous é˜¶æ®µâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â†“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ recv READY â†’ wait returnsâ”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â†“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ NCCL Init                â”‚  # è°ƒç”¨ ncclUniqueId, CommInitRankâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â†“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Collective Ops (DDP)     â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… æ€»ç»“è¦ç‚¹\n\næ ‡æ³¨ rank1 / rank5 çš„æµç¨‹å›¾ï¼Œæ›´ç›´è§‚ï¼›\nSET + WAIT æ“ä½œå…¨éƒ¨å‘ç”Ÿäº rendezvous é˜¶æ®µï¼Œè§å›¾ï¼›\nMaster â€œå¹¿æ’­ READYâ€ æ˜¯ socket å†™æ“ä½œï¼Œä¸æ˜¯é€šä¿¡åº“å¹¿æ’­ï¼›\nNCCL åˆå§‹åŒ–åœ¨ rendezvous å®Œæˆåè¿›è¡Œï¼›\nlibuv backend æä¾›æ›´é«˜æ•ˆ I/O å¤„ç†åŠ message æ‹¼æ¥å¤„ç†èƒ½åŠ› (docs.pytorch.org, pytorch.org, github.com)ã€‚\n\n\n","categories":["åˆ†å¸ƒå¼åŸºç¡€"],"tags":["tcpstore"]},{"title":"lumos:Efficient Performance Modeling and Estimation for Large-scale LLM Training","url":"/2025/08/17/paper/lumos/","content":"\n\n\nä¸€å¥è¯æ€»ç»“ï¼šLumos æ˜¯ä¸€ä¸ªåŸºäºè¿è¡Œæ—¶ trace çš„å»ºæ¨¡/æ¨¡æ‹Ÿå·¥å…·ï¼Œä» PyTorch Kineto ç­‰é‡‡é›†åˆ°çš„äº‹ä»¶ä¸­è‡ªåŠ¨æ¢å¤ç²¾ç»†çš„æ‰§è¡Œå›¾ï¼ˆå«ç®—-é€šé‡å ä¸è·¨æµä¾èµ–ï¼‰ï¼Œå¹¶æ”¯æŒåœ¨ä¸é‡æ–°è·‘æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå¯¹ DP/PP/æ¨¡å‹ç»“æ„ åš â€œwhat-ifâ€ ä¿®æ”¹ä¸å¿«é€Ÿä¼°ç®—ï¼›åœ¨ 512Ã—H100 é›†ç¾¤ä¸Šå›æ”¾å¹³å‡è¯¯å·®çº¦ 3.3%ã€‚(arXiv)\n\n\n1. æ ¸å¿ƒè´¡çŒ®ä¸å®šä½\n\nç²¾ç»†æ‰§è¡Œå›¾ï¼šä»…ç”¨æ¡†æ¶å†…ç½®çš„ profilerï¼ˆå¦‚ PyTorch Kinetoï¼‰å³å¯ä» CPU/GPU äº‹ä»¶æ¢å¤å››ç±»å…³é”®ä¾èµ–ï¼ˆCPUâ†’GPUã€GPUâ†’CPUã€åŒæµé¡ºåºã€è·¨æµäº‹ä»¶ï¼‰ï¼Œç²¾å‡†è¡¨è¾¾ç®—-é€šé‡å ä¸åŒæ­¥å…³ç³»ã€‚(arXiv)\nå›¾ç¼–è¾‘ &amp; å¿«é€Ÿå¤–æ¨ï¼šåœ¨ä¸æ”¹åŠ¨æ¨¡å‹/ç³»ç»Ÿçš„å‰æä¸‹ï¼Œä»åŸå§‹ trace-graph å‡ºå‘ï¼Œå¯¹ æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ã€æµæ°´å¹¶è¡Œï¼ˆPPï¼‰ ä¸æ¨¡å‹å±‚æ•°/éšè—ç»´åº¦åšå›¾çº§æ”¹å†™ï¼Œå†ç”¨æ¨¡æ‹Ÿå™¨é‡æ”¾ä¸€æ•´ä¸ªè¿­ä»£ä¼°ç®—æ€§èƒ½ã€‚(arXiv)\né«˜ç²¾åº¦å›æ”¾ï¼šåœ¨ç”Ÿäº§é›†ç¾¤ æœ€å¤š 512Ã—H100ã€å¤šç§ GPT-3 å˜ä½“ã€ä¸åŒå¹¶è¡Œç­–ç•¥ä¸‹ï¼Œè¿­ä»£æ—¶é—´å›æ”¾å¹³å‡è¯¯å·® 3.3%ï¼Œå¹¶èƒ½å†ç°å®æµ‹çš„æ‰§è¡Œç»†åˆ†å æ¯”ã€‚(arXiv)\n\n\n2. Lumos å¦‚ä½•ä» trace æ„å»ºæ‰§è¡Œå›¾\n\näº‹ä»¶æ¥æºï¼šç›´æ¥ä½¿ç”¨ PyTorch/TensorFlow çš„å†…ç½® profilerï¼ˆå¦‚ Kinetoï¼‰ï¼Œæ— éœ€å¯¹æ¨¡å‹æˆ–æ¡†æ¶åšä¾µå…¥å¼æ”¹é€ ã€‚(arXiv)\nä¾èµ–å»ºæ¨¡ï¼ˆå››ç±»ï¼‰ï¼š\n\nCPUâ†’GPUï¼ˆlaunchï¼‰ï¼šç”¨ correlation ID ç»‘å®š CPU ç«¯çš„ cudaLaunchKernel/cudaMemsetAsync ä¸å¯¹åº”çš„ GPU kernelã€‚\nGPUâ†’CPUï¼ˆåŒæ­¥ï¼‰ï¼šcudaDeviceSync/cudaStreamSync ç­‰éœ€è¦ç­‰åˆ°ç›¸å…³ GPU kernel å®Œæˆã€‚\nåŒæµé¡ºåºï¼šåŒä¸€ CUDA stream å†…æ ¸ä¸¥æ ¼é¡ºåºã€‚\nè·¨æµäº‹ä»¶ï¼šcudaEventRecord ä¸ cudaStreamWaitEvent å½¢æˆâ€œè®°å½•â†’ç­‰å¾…â€çš„è·¨æµä¾èµ–ï¼Œè¡¨è¾¾ä¸åŒæµé—´çš„æœ‰åºæ€§ã€‚(arXiv)\n\n\n\n3. å›¾ç¼–è¾‘ï¼šæ”¯æŒå“ªäº› â€œwhat-ifâ€ æ”¹åŠ¨\n\næ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ï¼šåªéœ€è°ƒæ•´é€šä¿¡ä»»åŠ¡ï¼ˆå¦‚æ¢¯åº¦è§„çº¦ç±»ï¼‰çš„æ‰§è¡Œæ—¶é—´ï¼›æœ¬åœ°è®¡ç®—ä¸å˜ã€‚(arXiv)\næµæ°´å¹¶è¡Œï¼ˆPPï¼‰ï¼š\n\nå…ˆæŒ‰æ‰€é€‰è°ƒåº¦ï¼ˆå¦‚ 1F1Bï¼‰æ›´æ–°å„å¾®æ‰¹çš„å‰åå‘é¡ºåºï¼›\nå°†åŸå›¾ä¸­ä»»åŠ¡æŒ‰å±‚èšç±»åé‡åˆ†é…åˆ°æ–° stageï¼›\nåœ¨ stage è¾¹ç•Œæ’å…¥/é‡è¿æ¿€æ´»ä¸æ¢¯åº¦çš„ send/recvï¼›\nä¿ç•™åŸ trace ä¸­çš„ä¾èµ–æ¨¡å¼ä»¥ä¿è¯å¯é‡æ”¾æ­£ç¡®æ€§ã€‚(arXiv)\n\næ¨¡å‹ç»“æ„ï¼š\n\néšè—ç»´åº¦å˜æ›´ï¼šé‡å†™ç›¸å…³ç®—å­/å†…æ ¸çš„è¾“å…¥å¼ é‡ç»´åº¦å¹¶é‡ä¼°æ—¶é•¿ï¼›\nå±‚æ•°å˜æ›´ï¼šå¤åˆ¶/åˆ å‡å±‚å—å¹¶é‡è¿ä¾èµ–ä¸é€šä¿¡ã€‚(arXiv)\n\næš‚ä¸æ”¯æŒï¼šä¿®æ”¹ Tensor Parallelismï¼ˆTPï¼‰ï¼ˆé€šå¸¸å—é™äºå•æœºä¸”é€šä¿¡é‡ï¼Œç•™ä½œæœªæ¥å·¥ä½œï¼‰ã€‚(arXiv)\n\n\n4. æ¨¡æ‹Ÿå™¨ï¼šäº‹ä»¶é©±åŠ¨æµç¨‹ï¼ˆè®ºæ–‡ç®—æ³• 1 çš„è¦ç‚¹ï¼‰\n\nç»´æŠ¤ä¸¤ä¸ªé›†åˆï¼š\n\nå›ºå®šä¾èµ–ï¼ˆåˆå§‹åŒ–é˜¶æ®µä¸€æ¬¡æ€§ç¡®å®šï¼Œå¦‚åŒçº¿ç¨‹/åŒæµé¡ºåºã€CPUâ†’GPU çš„ launch è¾¹ï¼‰ï¼›\nè¿è¡ŒæœŸä¾èµ–ï¼ˆä¾‹å¦‚ cudaStreamSync éœ€è¦ç­‰å¾…**è¯¥æµä¸Šâ€œæœ€åä¸€ä¸ª kernelâ€**å®Œæˆï¼Œè¿™ä¸ªâ€œæœ€åâ€è¦åœ¨è°ƒåº¦æ—¶æ‰èƒ½ç¡®å®šï¼‰ã€‚\n\nä¸»å¾ªç¯ï¼šä»å°±ç»ªé›†åˆå–ä»»åŠ¡ â†’ åˆ†é…åˆ°å…¶â€œå¤„ç†å™¨â€ï¼ˆCPU çº¿ç¨‹/CUDA streamï¼‰ä¸Šè¿è¡Œ â†’ æ›´æ–°å¤„ç†å™¨å¯ç”¨æ—¶é—´ä¸åç»§ä»»åŠ¡çš„æœ€æ—©å¯å¯åŠ¨æ—¶é—´ï¼›è‹¥ä»æœ‰è¿è¡ŒæœŸä¾èµ–æœªæ»¡è¶³åˆ™å»¶åã€‚(arXiv)\n\n\n5. è¯„æµ‹è®¾ç½®ä¸å…³é”®æ•°å­—\n\nè§„æ¨¡ä¸ç¯å¢ƒï¼šæœ€å¤š 512Ã—H100ï¼ˆ32 å°ä¸»æœºï¼‰ï¼ŒRoCE æ•°æ®ä¸­å¿ƒç½‘ç»œï¼ˆæ¯ä¸»æœº 8Ã—400Gbpsï¼‰ï¼ŒCUDA 12.4ï¼ŒPyTorch 2.5ï¼ŒTransformer Engine 0.12.0ï¼ŒLightning 1.9.4ã€‚(arXiv)\nå¯¹æ¯”åŸºçº¿ï¼šä¸ dPROï¼ˆtrace-driven å›æ”¾ç³»ç»Ÿï¼‰ç›¸æ¯”ï¼ŒLumos åœ¨å¤æ‚å¹¶è¡Œé…ç½®ä¸‹èƒ½æ›´å¥½æ•æ‰è·¨æµä¾èµ–ä¸ç®—-é€šé‡å ï¼Œæ˜¾è‘—é™ä½å›æ”¾è¯¯å·®ã€‚(arXiv)\nç»“æœï¼šå›æ”¾å¹³å‡è¯¯å·® ~3.3%ï¼›å¹¶å±•ç¤ºåœ¨ DP/PP/ç»“æ„å¤–æ¨æ—¶çš„ä¼°ç®—å‡†ç¡®æ€§ä¸æ‰§è¡Œç»†åˆ†ï¼ˆæš´éœ²è®¡ç®—/æš´éœ²é€šä¿¡/é‡å /å…¶ä»–ï¼‰ã€‚(arXiv)\n\n\n6. å·¥ç¨‹å®ç°ä¸ä½¿ç”¨é—¨æ§›\n\nå®ç°è§„æ¨¡ï¼šçº¦ 5,200 è¡Œ Pythonã€‚(arXiv)\næ¥å…¥æˆæœ¬ï¼šåœ¨è®­ç»ƒä»£ç é‡Œæ’å…¥ ~10 è¡Œ profiler hook é‡‡é›† Kineto traceï¼Œéšåèµ°è‡ªåŠ¨åŒ–æµç¨‹ï¼šå»ºå›¾ â†’ å›¾ç¼–è¾‘ â†’ æ¨¡æ‹Ÿä¼°ç®—ã€‚(arXiv)\n\n\n7. é€‚ç”¨/ä¸é€‚ç”¨åœºæ™¯\n\né€‚ç”¨ï¼š\n\néœ€è¦åœ¨çœŸå®æœºç¾¤å¤–å¿«é€Ÿæ¯”è¾ƒå¹¶è¡Œ/ç»“æ„é…ç½®ï¼ˆDP/PP/å±‚æ•°/éšè—ç»´ï¼‰å¹¶ä¼°ç®—æ”¶ç›Šï¼›\néœ€è¦é«˜ä¿çœŸå›æ”¾æ¥å®šä½ç®—-é€šé‡å ä¸è·¨æµåŒæ­¥å¤„çš„æ€§èƒ½ç“¶é¢ˆã€‚\n\nå½“å‰ä¸é€‚ç”¨/æ³¨æ„ï¼š\n\nä¿®æ”¹ TP çš„å¤–æ¨ï¼ˆè®ºæ–‡æš‚æœªæ”¯æŒï¼‰ï¼›\nè¿½æ±‚ FLOPsã€å†…å­˜ã€å¸¦å®½ã€èƒ½è€—ç­‰ç³»ç»Ÿçº§æŒ‡æ ‡ï¼ˆè®ºæ–‡ç§°ä¸ºåç»­è®¡åˆ’ï¼‰ï¼›\nä¼°ç®—å‡è®¾æ–°é…ç½®å¯æ­£å¸¸è¿è¡Œï¼ˆä¸è€ƒè™‘ OOM ç­‰å¤±æ•ˆæƒ…å½¢ï¼‰ã€‚(arXiv)\n\n\n\n8. ä¸æ—¢æœ‰å·¥ä½œçš„å…³ç³»ï¼ˆç¤ºä¾‹ï¼šdPROï¼‰\n\ndPRO åŒæ ·æ˜¯ trace-driven çš„æ€§èƒ½è¯Šæ–­/å›æ”¾ç³»ç»Ÿï¼Œä½†åœ¨å¤æ‚ LLM å¹¶è¡Œä¸‹ï¼Œè·¨æµä¾èµ–ä¸é‡å çš„ç²¾ç»†å»ºæ¨¡æ›´å›°éš¾ï¼Œå®¹æ˜“å¯¼è‡´è¿‡åº¦ä¹è§‚çš„å¹¶è¡Œé¢„æµ‹ï¼›Lumos åœ¨è¿™äº›æ–¹é¢åšäº†ç³»ç»Ÿå¢å¼ºå¹¶æ˜¾è‘—é™ä½è¯¯å·®ã€‚(arXiv)\n\n\n9. è®ºæ–‡ä¸ä¼šè®®ä¿¡æ¯ï¼ˆå¯å¼•ç”¨ï¼‰\n\nè®ºæ–‡ï¼ˆarXivï¼‰ï¼šâ€œLumos: Efficient Performance Modeling and Estimation for Large-scale LLM Trainingâ€ï¼ˆ2025-04-12 é¦–æ¬¡æäº¤ï¼‰ã€‚(arXiv)\nPDFï¼ˆä½œè€…ä¸»é¡µé•œåƒ/MLSys è®ºæ–‡ï¼‰ï¼šå¯ä¸‹è½½å…¨æ–‡ã€‚(mingyu-liang.github.io)\nMLSys 2025 æ¥æ”¶ä¸æ—¥ç¨‹é¡µé¢ï¼ˆå«æŠ¥å‘Š/å½•æ’­å…¥å£ï¼‰ã€‚(mlsys.org)\n\n\n10. ä»£ç ä¸å¼€æºçŠ¶æ€ï¼ˆæˆªè‡³ 2025-08-15ï¼‰\n\næœªè§å®˜æ–¹ä»£ç åº“é“¾æ¥ï¼ˆarXiv/MLSys é¡µé¢ä¸ä½œè€… PDF ä¸­å‡æœªæä¾›ï¼‰ï¼Œç¤¾åŒºé‡Œå­˜åœ¨åŒåä½†æ— å…³çš„ â€œLumosâ€ é¡¹ç›®ï¼ˆå¦‚ Agent/è§†é¢‘ç”Ÿæˆ/è§†è§‰ç­‰ï¼‰ï¼Œæ³¨æ„åŒºåˆ†ã€‚(arXiv, mlsys.org, GitHub)\n\n\n11. å¿«é€Ÿä¸Šæ‰‹ï¼ˆç¤ºæ„ï¼‰\n\né‡‡é›† Kineto trace â†’ æ„å»ºæ‰§è¡Œå›¾ â†’ åœ¨å›¾ä¸Šç¼–è¾‘ï¼ˆDP/PP/ç»“æ„ï¼‰â†’ æ¨¡æ‹Ÿå›æ”¾/ä¼°ç®—ã€‚ è®ºæ–‡æ­£æ–‡ç»™å‡ºäº†å…¸å‹çš„ PyTorch profiler ç”¨æ³•ç¤ºæ„ä¸å…¨æµç¨‹ç¤ºæ„å›¾ã€‚(arXiv)\n\n\n12. ä½ å¯èƒ½å…³å¿ƒçš„ç»†èŠ‚ï¼ˆç²¾ç‚¼ç‰ˆï¼‰\n\nä¸ºä»€ä¹ˆæ›´å‡†ï¼Ÿ\n\nç”¨ correlation ID ä¸²èµ· CPU launch ä¸ GPU kernelï¼›\næ˜¾å¼æ¢å¤ è·¨æµäº‹ä»¶ï¼ˆRecord/Waitï¼‰ä¸åŒæ­¥ï¼ˆStream/Device Syncï¼‰ï¼›\nåœ¨æ¨¡æ‹Ÿå™¨ä¸­å°†ä¾èµ–åˆ†ä¸ºå›ºå®šä¸è¿è¡ŒæœŸï¼Œç¡®ä¿åƒ â€œç­‰å¾…è¯¥æµæœ€åä¸€ä¸ª kernelâ€ è¿™ç±»è¯­ä¹‰è¢«æ­£ç¡®è¡¨è¾¾ã€‚(arXiv)\n\næ”¹ DP/PP æ€ä¹ˆç®—ï¼Ÿ\n\nDPï¼šåªé‡èµ‹é€šä¿¡ä»»åŠ¡æ—¶é•¿ï¼›\nPPï¼šæ›´æ–°è°ƒåº¦ï¼ˆå¦‚ 1F1Bï¼‰â†’ ä»»åŠ¡æŒ‰å±‚åˆ†ç»„å¹¶é‡åˆ† stage â†’ åœ¨è¾¹ç•Œæ’å…¥ send/recv â†’ ä¿æŒä¾èµ–é—­åˆã€‚(arXiv)\n\n\n\nå‚è€ƒæ–‡çŒ® / é“¾æ¥\n\nLumos è®ºæ–‡ï¼ˆarXiv é¡µé¢ä¸ PDFï¼‰ï¼š(arXiv)\nLumosï¼ˆMLSys 2025 ä¼šè®®é¡µé¢/æ—¥ç¨‹/å½•æ’­ï¼‰ï¼š(mlsys.org)\ndPROï¼ˆtrace-driven å›æ”¾åŸºçº¿è®ºæ–‡ï¼‰ï¼š(arXiv)\n\n\n\næ³¨ï¼šæœ¬æ–‡æ¡£åªæ‘˜å–å¯¹å·¥ç¨‹è½åœ°æœ€å…³é”®çš„äº‹å®ä¸æ–¹æ³•ï¼Œæ›´å¤šå›¾ä¾‹ï¼ˆå¦‚ PPÃ—TP å¾®æ‰¹è°ƒåº¦ç¤ºä¾‹ï¼‰ä¸å®Œæ•´ç®—æ³•ç»†èŠ‚è¯·å‚é˜…åŸè®ºæ–‡æ­£æ–‡ä¸é™„å›¾ã€‚(arXiv)\n\n\n","categories":["è®ºæ–‡é˜…è¯»"],"tags":["paper"]},{"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism","url":"/2025/11/22/paper/megatron_lm/","content":"\n\nä¸€ã€è®ºæ–‡é€Ÿè§ˆ\nã€ŠMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelismã€‹èšç„¦çš„é—®é¢˜æ˜¯ï¼šåœ¨å•å¡æ˜¾å­˜è¿œè¿œä¸å¤Ÿçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•åœ¨ GPU é›†ç¾¤ä¸Šé«˜æ•ˆè®­ç»ƒå¤šäº¿åˆ°å‡ åäº¿å‚æ•°è§„æ¨¡çš„ Transformer è¯­è¨€æ¨¡å‹ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ä¸“é—¨ä¸º Transformer è®¾è®¡çš„ å±‚å†…æ¨¡å‹å¹¶è¡Œï¼ˆintra-layer model parallelism / tensor parallelismï¼‰ï¼šå°†è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œä¸­çš„å¤§çŸ©é˜µæŒ‰è¡Œ/åˆ—æ‹†åˆ†åˆ°å¤šå— GPU ä¸Šï¼Œå¹¶åœ¨å…³é”®ä½ç½®æ’å…¥å°‘é‡ all-reduce / all-gather é€šä¿¡ä»¥æ‹¼åˆç»“æœï¼Œä»è€Œçªç ´å•å¡æ˜¾å­˜é™åˆ¶ã€‚è¯¥æ–¹æ¡ˆå®Œå…¨åŸºäºåŸç”Ÿ PyTorchï¼Œä¸éœ€è¦æ–°çš„ç¼–è¯‘å™¨æˆ– DSLï¼Œå¹¶ä¸”å¯ä»¥ä¸æ•°æ®å¹¶è¡Œã€æµæ°´å¹¶è¡Œç»„åˆä½¿ç”¨ï¼›åœ¨ 512 å¼  V100 ä¸Šï¼Œä½œè€…è®­ç»ƒäº† 8.3B GPT-2 ç±»æ¨¡å‹å’Œ 3.9B BERT ç±»æ¨¡å‹ï¼Œè¾¾åˆ°äº† 15.1 PFLOPsã€çº¦ 76% å¼ºæ‰©å±•æ•ˆç‡ï¼Œå¹¶åœ¨ WikiText-103ã€LAMBADAã€RACE ç­‰ä»»åŠ¡ä¸Šå–å¾—å½“æ—¶çš„ SOTAã€‚(arXiv)\näºŒã€è®ºæ–‡ç»“æ„æ¦‚è§ˆ\n\nIntroduction è¯´æ˜å¤§è§„æ¨¡ Transformer è¯­è¨€æ¨¡å‹åœ¨æ•ˆæœä¸Šçš„ä¼˜åŠ¿ï¼Œä»¥åŠæ˜¾å­˜èµ„æºå¯¹è®­ç»ƒçš„åˆ¶çº¦ï¼ŒæŒ‡å‡ºä»…é æ•°æ®å¹¶è¡Œå·²ç»éš¾ä»¥ç»§ç»­æ‰©å±•æ¨¡å‹è§„æ¨¡ã€‚å¼•å‡º Megatron-LM çš„æ ¸å¿ƒæ€è·¯ï¼šé€šè¿‡ç®€å•ã€é«˜æ•ˆçš„å±‚å†…æ¨¡å‹å¹¶è¡Œï¼ŒæŠŠæ¯ä¸€å±‚çš„å¤§çŸ©é˜µæ‹†åˆ°å¤šå¡ä¸Šè®¡ç®—ï¼Œå¹¶ç»™å‡º 8.3B / 3.9B æ¨¡å‹çš„ä¸»è¦å®éªŒç»“æœã€‚(arXiv)\nRelated Work å›é¡¾ GPT-2ã€BERT ç­‰å…¸å‹å¤§æ¨¡å‹ï¼Œä»¥åŠå¸¸è§çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹å¼ï¼ˆæ•°æ®å¹¶è¡Œã€ä¼ ç»ŸæŒ‰å±‚æ¨¡å‹å¹¶è¡Œã€GPipe/PipeDream è¿™ç±»æµæ°´å¹¶è¡Œï¼‰ï¼ŒæŒ‡å‡ºè¿™äº›æ–¹æ³•åœ¨æ˜¾å­˜åˆ©ç”¨ã€é€šä¿¡å¼€é”€ã€å¤šæœºæ‰©å±•æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡è¿™éƒ¨åˆ†ï¼Œè¯»è€…å¯ä»¥æŠŠ Megatron-LM æ”¾åœ¨â€œå¤§æ¨¡å‹å¹¶è¡Œè®­ç»ƒæ–¹æ³•è°±ç³»â€ä¸­æ¥ç†è§£ã€‚(arXiv)\nModel Parallel Transformer å…¨æ–‡æŠ€æœ¯æ ¸å¿ƒï¼šè¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨ Transformer è‡ªæ³¨æ„åŠ›å’Œ MLP ä¸¤ä¸ªå­å±‚ä¸­å¯¹æƒé‡çŸ©é˜µè¿›è¡ŒæŒ‰è¡Œ/æŒ‰åˆ—åˆ‡åˆ†ï¼Œå¦‚ä½•å®‰æ’å‰å‘å’Œåå‘ä¸­çš„ all-reduce / all-gatherï¼Œä»¥åŠå¦‚ä½•å¤„ç† embedding / è¾“å‡ºå±‚ç­‰ç‰¹æ®Šæ¨¡å—ã€‚è¿™ä¸€éƒ¨åˆ†å®é™…ä¸Šå®šä¹‰äº†åæ¥å¹¿æ³›ä½¿ç”¨çš„ â€œMegatron å¼ Tensor Parallelismâ€ çš„å…·ä½“è¯­ä¹‰ã€‚(arXiv)\nImplementation Details ä»‹ç»åœ¨å¤šæœºå¤šå¡é›†ç¾¤ä¸Šçš„éƒ¨ç½²ç»†èŠ‚ï¼ŒåŒ…æ‹¬ï¼šç¡¬ä»¶æ‹“æ‰‘ï¼ˆ32 å° DGX-2Hï¼Œ512 å¼  V100ï¼ŒNVSwitch+InfiniBandï¼‰ã€é€šä¿¡åç«¯ï¼ˆNCCLï¼‰ã€æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16+loss scalingï¼‰ã€ä¼˜åŒ–å™¨é…ç½®ç­‰ã€‚è¿™é‡Œç»™å‡ºäº†ä¸€äº›å½±å“å®é™…ååçš„å…³é”®å·¥ç¨‹é€‰æ‹©ï¼Œå¯¹å¸Œæœ›åœ¨è‡ªå·±ç³»ç»Ÿé‡Œå¤ç°å®éªŒæˆ–ç§»æ¤æ€æƒ³çš„è¯»è€…éå¸¸æœ‰å‚è€ƒä»·å€¼ã€‚(arXiv)\nExperiments ç³»ç»Ÿè¯„ä¼° Megatron-LM çš„æ‰©å±•æ€§å’Œä»»åŠ¡æ€§èƒ½ï¼šåœ¨ GPT-2 ç±»ï¼ˆ1.2Bâ€“8.3Bï¼‰å’Œ BERT ç±»ï¼ˆ0.3Bâ€“3.9Bï¼‰æ¨¡å‹ä¸Šï¼ŒæŠ¥å‘Š FLOPs åˆ©ç”¨ç‡ã€å¼º/å¼±æ‰©å±•æ•ˆç‡ï¼Œä»¥åŠ WikiText-103ã€LAMBADAã€RACE ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„ SOTA ç»“æœã€‚åŒæ—¶åˆ†æ BERT æ¶æ„ä¸­ LayerNorm æ”¾ç½®ä½ç½®å¯¹å¤§æ¨¡å‹è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆæœçš„å½±å“ã€‚(arXiv)\nConclusion æ€»ç»“æå‡ºçš„ intra-layer æ¨¡å‹å¹¶è¡Œæ–¹æ¡ˆåŠå…¶å®éªŒç»“æœï¼Œå¼ºè°ƒè¯¥æ–¹æ³•ç®€å•ã€é«˜æ•ˆã€æ˜“ä¸æ•°æ®å¹¶è¡Œå’Œæµæ°´å¹¶è¡Œç»“åˆï¼Œå¹¶æŒ‡å‡ºè¿™ä¸ºåç»­æ›´å¤§è§„æ¨¡ï¼ˆç”šè‡³ trillion å‚æ•°çº§ï¼‰çš„æ¨¡å‹è®­ç»ƒé“ºå¹³äº†é“è·¯ã€‚(arXiv)\n\n\næ ¸å¿ƒæ€æƒ³ï¼š Megatron-LM æå‡ºäº†ä¸€å¥—ä¸“ä¸º Transformer è®¾è®¡çš„å±‚å†…å¼ é‡å¹¶è¡Œæ–¹æ¡ˆï¼Œé€šè¿‡å¯¹æ¯å±‚ä¸­çš„å¤§çŸ©é˜µæŒ‰è¡Œ/åˆ—è§„åˆ™åˆ‡åˆ†å¹¶åœ¨å›ºå®šä½ç½®æ’å…¥å°‘é‡é›†ä½“é€šä¿¡ï¼Œåœ¨ä¸æ”¹å˜æ¨¡å‹ç»“æ„å’Œæ¡†æ¶çš„å‰æä¸‹ï¼Œå°† GPT/BERT ç±»è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°å¤šäº¿è‡³å‡ åäº¿å‚æ•°è§„æ¨¡ï¼Œå¹¶åœ¨å¤šæœºå¤šå¡é›†ç¾¤ä¸Šå®ç°äº†é«˜ FLOPs åˆ©ç”¨ç‡ä¸å®æµ‹ä»»åŠ¡æ€§èƒ½çš„åŒæ­¥æå‡ã€‚\n\n\nä¸‰ã€æ–¹æ³•ä¸ç³»ç»Ÿè®¾è®¡æ‹†è§£\nMegatron-LM çš„æ–¹æ³•éƒ¨åˆ†é’ˆå¯¹çš„æ˜¯ä¸€ä¸ªå¾ˆç›´æ¥çš„ç³»ç»Ÿé—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸é‡å†™æ¡†æ¶ã€ä¸å¼•å…¥æ–°ç¼–è¯‘å™¨çš„å‰æä¸‹ï¼ŒæŠŠå•å±‚å†…éƒ¨çš„å¤§çŸ©é˜µæ‹†ç»™å¤šå¼  GPU åšï¼ŒåˆæŠŠé€šä¿¡æ¬¡æ•°å‹åˆ°å¾ˆå°‘ï¼Ÿ ä½œè€…å›´ç»• Transformer çš„ MLPã€è‡ªæ³¨æ„åŠ›å’Œ Embedding ä¸‰ä¸ªâ€œç®—åŠ›ä¸æ˜¾å­˜å¤§å¤´â€ï¼Œè®¾è®¡äº†ä¸€æ•´å¥—å¼ é‡å¹¶è¡Œï¼ˆtensor parallelï¼‰ç­–ç•¥ï¼Œå¹¶é€šè¿‡æœ‰é™æ¬¡ all-reduce / all-gather å®Œæˆå¿…è¦çš„åŒæ­¥ã€‚(arXiv)\nä¸»è¦åœ¨è§£å†³çš„å­é—®é¢˜åŒ…æ‹¬ï¼š\n\nå¦‚ä½•åœ¨ MLP ä¸¤ä¸ªå¤§çº¿æ€§å±‚ ä¸Šè¿›è¡Œå¼ é‡åˆ‡åˆ†ï¼Œæ—¢é™ä½å•å¡æ˜¾å­˜ï¼Œåˆæ§åˆ¶åŒæ­¥æ¬¡æ•°åœ¨å¯æ¥å—èŒƒå›´ï¼Ÿ(arXiv)\nå¦‚ä½•åœ¨ è‡ªæ³¨æ„åŠ›ï¼ˆQ/K/V æŠ•å½± + è¾“å‡ºçº¿æ€§å±‚ï¼‰ ä¸Šå¹¶è¡Œï¼Œè®©æ¯å¼  GPU è´Ÿè´£ä¸€éƒ¨åˆ† headï¼ŒåŒæ—¶ä¿è¯æœ€ç»ˆ attention è¾“å‡ºæ­£ç¡®ï¼Ÿ(arXiv)\nå¦‚ä½•å¯¹ è¾“å…¥/è¾“å‡º embedding åšå¹¶è¡Œï¼Œé¿å… batch Ã— seq Ã— vocab çº§åˆ«çš„å·¨å¤§é€šä¿¡ï¼ŒåŒæ—¶å…¼é¡¾æƒé‡å…±äº«å’Œ cross-entropy è®¡ç®—ï¼Ÿ(arXiv)\nå¦‚ä½•å°†ä¸Šè¿°å±‚å†…å¹¶è¡Œä¸ æ•°æ®å¹¶è¡Œ ç»„åˆï¼ŒæŠŠæ¨¡å‹è§„æ¨¡ä¸ååé‡ä¸€èµ·æ‰©å±•åˆ°æ•°ç™¾ GPUï¼Ÿ(arXiv)\n\n3.1 æ ¸å¿ƒæ¨¡å—ä¸€è§ˆ\n\nMLPï¼šåˆ—åˆ‡åˆ†çš„ç¬¬ä¸€å±‚çº¿æ€§ï¼ˆColumn Parallel Linearï¼‰ ç¬¬ä¸€å±‚çº¿æ€§å±‚ (X W_1) çš„æƒé‡ (W_1) æŒ‰åˆ—åˆ‡åˆ†ä¸º (W_{1,1}, W_{1,2}, â€¦)ï¼Œä¸åŒ GPU è´Ÿè´£è¾“å‡º hidden çš„ä¸åŒåˆ‡ç‰‡ï¼›GeLU åœ¨å„ GPU æœ¬åœ°è®¡ç®—ï¼Œåç»­æ ¹æ®éœ€è¦å†åš all-reduce / all-gatherã€‚è¿™ç§è®¾è®¡é¿å…äº†åœ¨æ¿€æ´»å‰å°±åŒæ­¥ï¼Œä»è€Œå‡å°‘é€šä¿¡ã€‚(arXiv)\nMLPï¼šè¡Œåˆ‡åˆ†çš„ç¬¬äºŒå±‚çº¿æ€§ï¼ˆRow Parallel Linearï¼‰ ç¬¬äºŒå±‚çº¿æ€§å±‚ (Y W_2) ä¸­ï¼Œæƒé‡ (W_2) æŒ‰è¡Œåˆ‡åˆ†ï¼Œå„ GPU ä»…æŒæœ‰ä¸€éƒ¨åˆ†è¡Œã€‚è¾“å…¥ (Y) å·²ç»æ˜¯æŒ‰åˆ—åˆ‡å¥½çš„å±€éƒ¨å¼ é‡ï¼Œæ¯ä¸ª GPU ç‹¬ç«‹è®¡ç®—å±€éƒ¨è¾“å‡ºï¼Œæœ€åé€šè¿‡ä¸€æ¬¡ all-reduce ç´¯åŠ æ±‚å’Œå¾—åˆ°å®Œæ•´è¾“å‡ºã€‚è¿™æ · MLP ä¸¤å±‚æ•´ä½“åªéœ€å°‘é‡åŒæ­¥ã€‚(arXiv)\nè‡ªæ³¨æ„åŠ›ï¼šæŒ‰ head/åˆ—åˆ‡åˆ†çš„å¹¶è¡Œå¤šå¤´æ³¨æ„åŠ› Q/K/V æŠ•å½±çŸ©é˜µæŒ‰åˆ—åˆ‡åˆ†ï¼Œå„ GPU è´Ÿè´£ä¸€éƒ¨åˆ† heads çš„è®¡ç®—ï¼›attention å®Œæˆåï¼Œè¾“å‡ºçº¿æ€§å±‚æŒ‰è¡Œåˆ‡åˆ†ï¼Œæ¯ä¸ª GPU å¾—åˆ°å±€éƒ¨è¾“å‡ºï¼Œå†é€šè¿‡ all-reduce å¾—åˆ°å®Œæ•´ hidden è¡¨ç¤ºã€‚æ•´ä½“ä¸Šï¼Œè‡ªæ³¨æ„åŠ›æ¨¡å—çš„å‰å‘+åå‘åªå¼•å…¥å›ºå®šå‡ æ¬¡é›†ä½“é€šä¿¡ã€‚(arXiv)\nI/O Embeddingï¼šè¯è¡¨ç»´åº¦ä¸Šçš„å¹¶è¡Œ + å¹¶è¡Œ cross-entropy embedding çŸ©é˜µåœ¨ vocab ç»´åº¦ä¸Šåˆ‡åˆ†ï¼š\n\nè¾“å…¥ç«¯ï¼šæ¯ GPU å­˜ä¸€éƒ¨åˆ† vocabï¼Œå¯¹åº” tokens æŸ¥è¡¨åï¼Œç”¨ all-reduce åˆæˆå®Œæ•´ embeddingï¼›\nè¾“å‡ºç«¯ï¼šå°† all-gather å’Œ cross-entropy loss è®¡ç®—èåˆæˆâ€œå¹¶è¡Œ cross-entropyâ€ï¼ŒæŠŠåŸæœ¬ batch Ã— seq Ã— vocab çš„é€šä¿¡ï¼Œç¼©å‡ä¸º batch Ã— seq è§„æ¨¡çš„æ•°æ®äº¤æ¢ã€‚(Minjia Zhang)\n\nå¼ é‡å¹¶è¡Œ Ã— æ•°æ®å¹¶è¡Œçš„ç»„åˆ å°†ä¸Šè¿°å¼ é‡å¹¶è¡Œè§†ä¸ºâ€œæ¨¡å‹å¹¶è¡Œç»„â€å†…éƒ¨çš„äº‹ï¼Œå†åœ¨å…¶å¤–å±‚å¥—ä¸€åœˆæ•°æ®å¹¶è¡Œã€‚æ¯ä¸ªæ•°æ®å¹¶è¡Œç»„å†…æ˜¯ä¸€ç»„åš TP çš„ GPUï¼Œç»„é—´é€šè¿‡æ•°æ®å¹¶è¡ŒåŒæ­¥å‚æ•°/æ¢¯åº¦ï¼Œå®ç°æ¨¡å‹è§„æ¨¡å’Œ batch å°ºåº¦çš„åŒå‘æ‰©å±•ã€‚(arXiv)\n\n3.2 æ•°æ®æµä¸æ§åˆ¶æµç¤ºæ„ï¼ˆæ–‡å­—ç‰ˆï¼‰\nå¯ä»¥ç”¨ä»¥ä¸‹â€œä»è¾“å…¥åˆ°è¾“å‡ºâ€çš„æµç¨‹æ¥ç†è§£ Megatron-LM çš„å¼ é‡å¹¶è¡Œæ•°æ®æµä¸æ§åˆ¶æµï¼š\n\nè¾“å…¥é˜¶æ®µï¼šå¹¶è¡Œ Embedding\n\nè¾“å…¥ tokens åˆ†å‘åˆ°æ¯å¼  GPUï¼›\næ¯å¼  GPU æŒæœ‰ embedding çš„ä¸€éƒ¨åˆ†åˆ—ï¼Œå±€éƒ¨æŸ¥è¡¨å¾—åˆ°å±€éƒ¨ embeddingï¼›\nä¸€æ¬¡ all-reduce æŠŠå±€éƒ¨ embedding èšåˆä¸ºå®Œæ•´çš„ hidden è¡¨ç¤º ()ã€‚(arXiv)\n\nTransformer å±‚ â€“ è‡ªæ³¨æ„åŠ›å—\n\nQ/K/V æŠ•å½±ï¼š\n\næƒé‡æŒ‰åˆ—åˆ‡åˆ†ï¼Œå„ GPU è®¡ç®—è‡ªå·±é‚£éƒ¨åˆ† heads å¯¹åº”çš„ Q/K/Vï¼›\n\nåœ¨æ¯å¼  GPU ä¸Šç‹¬ç«‹è®¡ç®—æœ¬åœ° heads çš„è‡ªæ³¨æ„åŠ›ï¼ˆQÂ·Káµ€ / âˆšd â†’ softmax â†’ Ã—Vï¼‰ï¼›\nè¾“å‡ºçº¿æ€§å±‚æŒ‰è¡Œåˆ‡åˆ†ï¼Œå„ GPU å¾—åˆ°å±€éƒ¨è¾“å‡ºï¼Œæœ€åé€šè¿‡ä¸€æ¬¡ all-reduce å¾—åˆ°å®Œæ•´ hidden è¡¨ç¤ºã€‚(arXiv)\n\nMLP å—\n\nç¬¬ä¸€å±‚çº¿æ€§ï¼šåˆ—åˆ‡åˆ†\n\næ¯å¼  GPU è®¡ç®— (Y_i = X W_{1,i})ï¼Œå¹¶æœ¬åœ°æ‰§è¡Œ GeLUï¼›\n\nç¬¬äºŒå±‚çº¿æ€§ï¼šè¡Œåˆ‡åˆ†\n\næ¯å¼  GPU è®¡ç®— (Z_i = Y_i W_{2,i})ï¼Œé€šè¿‡ all-reduce å°†å„ GPU å±€éƒ¨è¾“å‡ºç›¸åŠ ï¼Œå¾—åˆ°æ•´ä½“è¾“å‡ºã€‚(arXiv)\n\n\nåå‘ä¼ æ’­ä¸­çš„é€šä¿¡\n\nå¯¹åˆ—åˆ‡åˆ†å±‚ï¼šå‰å‘å¯ä¸ all-gatherï¼Œåå‘éœ€è¦åˆå¹¶æ¢¯åº¦ï¼›\nå¯¹è¡Œåˆ‡åˆ†å±‚ï¼šå‰å‘éœ€è¦ä¸€æ¬¡ all-reduceï¼Œåå‘æ¢¯åº¦å¯ä»¥å±€éƒ¨æ›´æ–°ï¼›\nç»“åˆæ³¨æ„åŠ›ä¸ MLPï¼Œä¸€ä¸ªå®Œæ•´çš„å‰å‘+åå‘ä¸­ï¼Œæ¯å±‚æ€»å…±çº¦ 4 æ¬¡é›†ä½“é€šä¿¡ã€‚(Minjia Zhang)\n\nè¾“å‡ºå±‚ä¸ loss\n\nè¾“å‡º embedding ä¸è¾“å…¥ embedding å…±äº«æƒé‡ï¼Œå¹¶åœ¨ vocab ç»´åº¦åˆ‡åˆ†ï¼›\né€šè¿‡å¹¶è¡Œ cross-entropyï¼Œå°† logits çš„åˆå¹¶ä¸ loss è®¡ç®—èåˆï¼Œé€šä¿¡è§„æ¨¡å¤§å¹…é™ä½ã€‚(Minjia Zhang)\n\nå¤–å±‚æ•°æ®å¹¶è¡Œ\n\nTP ç»„å†…éƒ¨å®Œæˆä¸Šè¿°æ‰€æœ‰æ“ä½œåï¼ŒæŒ‰å¸¸è§„æ•°æ®å¹¶è¡Œæ–¹å¼åœ¨ç»„é—´åšæ¢¯åº¦ all-reduceï¼›\nä»å…¨å±€çœ‹ï¼Œæ˜¯â€œç»„å†…æ¨¡å‹å¹¶è¡Œ + ç»„é—´æ•°æ®å¹¶è¡Œâ€çš„åŒå±‚ç»“æ„ã€‚(arXiv)\n\n\n3.3 å…³é”®å‡è®¾ä¸é€‚ç”¨èŒƒå›´\n\nå‡è®¾ä¸€ï¼šTransformer ç»“æ„è¶³å¤Ÿè§„åˆ™ã€å±‚é—´åŒæ„\n\næ‰€æœ‰å±‚éƒ½ç”±â€œå¤šå¤´æ³¨æ„åŠ› + ä¸¤å±‚ MLPâ€å †å ï¼Œhidden size åŸºæœ¬ä¸€è‡´ï¼›\nè¿™ä½¿å¾—ç®€å•çš„â€œè¡Œåˆ‡/åˆ—åˆ‡ + å›ºå®šé€šä¿¡æ¨¡å¼â€åœ¨æ‰€æœ‰å±‚ä¸Šéƒ½èƒ½å¤ç”¨ã€‚\nå¯¹é«˜åº¦å¼‚æ„æˆ–å¸¦å¤§é‡åŠ¨æ€åˆ†æ”¯çš„ç»“æ„ï¼Œç›´æ¥å¥—ç”¨ä¼šå˜å¾—å¤æ‚ã€‚(arXiv)\n\nå‡è®¾äºŒï¼šç»„å†…ç½‘ç»œå¸¦å®½å’Œå»¶è¿Ÿè¶³å¤Ÿå¥½\n\né»˜è®¤ TP ç»„å†… GPU ä¹‹é—´é€šè¿‡ NVLink/NVSwitch é«˜é€Ÿäº’è”ï¼Œè·¨èŠ‚ç‚¹é€šè¿‡å¤šæ¡ InfiniBand è¿æ¥ï¼›\nåœ¨è¿™æ ·çš„æ‹“æ‰‘ä¸‹ï¼Œæ¯å±‚ 4 æ¬¡å·¦å³çš„é›†ä½“é€šä¿¡æ˜¯å¯ä»¥æ¥å—çš„ã€‚\nåœ¨å¸¦å®½è¾ƒä½æˆ–æ‹“æ‰‘å¤æ‚çš„ç¯å¢ƒä¸‹ï¼Œé€šä¿¡å¯èƒ½ä¼šæˆä¸ºä¸»è¦ç“¶é¢ˆã€‚(arXiv)\n\nå‡è®¾ä¸‰ï¼šåˆ‡åˆ†åè´Ÿè½½åœ¨ä¸åŒ GPU ä¹‹é—´å¤§è‡´å‡è¡¡\n\næŒ‰è¡Œ/åˆ—å‡åŒ€åˆ‡åˆ†çŸ©é˜µæ—¶ï¼Œå‡å®šå„ shard çš„ FLOPs å’Œæ˜¾å­˜å ç”¨ç›¸è¿‘ï¼›\nè‹¥ç¡¬ä»¶å¼‚æ„æˆ–æœ‰é¢å¤–ä»»åŠ¡æ‰“æ‰°ï¼Œç®€å•å‡åŒ€åˆ‡åˆ†å¯èƒ½å‡ºç°ä¸¥é‡çš„ straggler é—®é¢˜ã€‚\n\nå‡è®¾å››ï¼šé€šä¿¡åŸè¯­å®ç°é«˜æ•ˆç¨³å®š\n\nå¼ºä¾èµ– NCCL ç­‰åº“é«˜æ•ˆå®ç° all-reduce / all-gather ç­‰æ“ä½œï¼›\nè‹¥é€šä¿¡åº“è°ƒä¼˜ä¸è¶³æˆ–åº•å±‚ç½‘ç»œæ ˆå­˜åœ¨é—®é¢˜ï¼Œå³ä¾¿ç®—æ³•ç»“æ„ç›¸åŒï¼Œå®é™…æ€§èƒ½ä¹Ÿå¯èƒ½å¤§æ‰“æŠ˜æ‰£ã€‚(arXiv)\n\nå‡è®¾äº”ï¼šæ¨¡å‹è§„æ¨¡å’Œ batch è§„æ¨¡è¶³å¤Ÿå¤§ï¼Œå¯ä»¥æ‘Šè–„é€šä¿¡æˆæœ¬\n\nTP çš„ä¼˜åŠ¿æ›´æ˜æ˜¾åœ°ä½“ç°åœ¨ multi-billion å‚æ•°ã€å¤§ batch çš„ regimeï¼›\nå¯¹ä¸­å°æ¨¡å‹æˆ–æå° batchï¼Œé€šä¿¡å¼€é”€å¯èƒ½ä¼šå‹è¿‡è®¡ç®—ï¼Œå¯¼è‡´æ•´ä½“æ•ˆç‡ä¸ä½³ã€‚\n\n\nä¸å¸¸è§è®­ç»ƒæ ˆçš„å¯¹åº”å…³ç³»\n\nDataLoader / æ•°æ®é€šè·¯ï¼šåŸºæœ¬ä¸æ”¹æ•°æ®æ ¼å¼ï¼Œä¸»è¦éœ€è¦åœ¨ embedding/è¾“å‡ºå±‚é€‚é…åˆ‡åˆ†åçš„å¼ é‡å¸ƒå±€ã€‚\nå¹¶è¡Œè°ƒåº¦ï¼šåœ¨æ•°æ®å¹¶è¡Œä¹‹å¤–å¢åŠ â€œæ¨¡å‹å¹¶è¡Œç»„â€çš„æ¦‚å¿µï¼Œå½¢æˆç»„å†… TP + ç»„é—´ DP çš„åŒå±‚è°ƒåº¦ã€‚\næ¨¡å‹å¹¶è¡Œï¼ˆTPï¼‰ï¼šè¡Œåˆ‡/åˆ—åˆ‡çº¿æ€§å±‚ã€æŒ‰ head çš„è‡ªæ³¨æ„åŠ›å¹¶è¡Œã€embedding å¹¶è¡Œï¼Œæ„æˆç°ä»£ TP çš„åŸå‹ã€‚\nkernel ä¸ç®—å­ï¼šå¤§å¤šæ•°ç®—å­ä»æ˜¯æ ‡å‡† GEMM/attentionï¼Œåªæ˜¯å¼ é‡å½¢çŠ¶å’Œ layout æŒ‰ TP æ–¹å‘å˜åŒ–ï¼›å¹¶è¡Œ cross-entropy æ˜¯é€šä¿¡ä¸ç®—å­èåˆçš„ä»£è¡¨ã€‚(arXiv)\né€šä¿¡ backendï¼šé«˜åº¦ä¾èµ– NCCL ç­‰åº“é«˜æ•ˆå®ç°é›†ä½“æ“ä½œï¼Œæ˜¯ TP æ€§èƒ½çš„åŸºç¡€ã€‚\né…ç½®ä¸è‡ªåŠ¨è°ƒå‚ï¼šTP sizeã€DP size ç­‰ç›´æ¥å†³å®šå•å¡æ˜¾å­˜å ç”¨å’Œé€šä¿¡/è®¡ç®—æ¯”ä¾‹ï¼Œæ˜¯ç³»ç»Ÿé…ç½®çš„å…³é”®ç»´åº¦ã€‚\n\n\nå››ã€å»ºæ¨¡æ–¹å¼ä¸è¯„ä¼°æŒ‡æ ‡\n4.1 é—®é¢˜æ˜¯å¦‚ä½•å½¢å¼åŒ–çš„ï¼Ÿ\nMegatron-LM å¹¶æ²¡æœ‰ç»™å‡ºä¸¥æ ¼çš„æ•°å­¦è§„åˆ’å½¢å¼ï¼Œè€Œæ˜¯ä»â€œç³»ç»Ÿ + æ€§èƒ½â€çš„è§’åº¦å»å½¢å¼åŒ–é—®é¢˜ï¼šåœ¨å›ºå®š GPU æ•°é‡ã€å•å¡æ˜¾å­˜ã€ç½‘ç»œå¸¦å®½æ¡ä»¶ä¸‹ï¼Œå¦‚ä½•è®¾è®¡ä¸€ç§å±‚å†…æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼Œä½¿å¾—ï¼š\n\næ¨¡å‹å‚æ•°æ€»é‡ (P) å°½å¯èƒ½å¤§ï¼ˆæ‰©å±•åˆ° multi-billion å‚æ•°ï¼‰ï¼›\nå•æ­¥è®­ç»ƒæ—¶é—´ (T) å°½å¯èƒ½å°ï¼ŒFLOPs åˆ©ç”¨ç‡å°½å¯èƒ½é«˜ï¼›\né€šä¿¡å¼€é”€ç›¸å¯¹å¯æ§ï¼Œå¯ä»¥åœ¨å®é™…é›†ç¾¤ä¸Šé«˜æ•ˆè¿è¡Œã€‚(arXiv)\n\nå¯ä»¥ç†è§£ä¸ºéšå«çš„ç›®æ ‡ï¼š\n\nç›®æ ‡ 1ï¼šæ˜¾å­˜çº¦æŸä¸‹æœ€å¤§åŒ–å¯è®­ç»ƒå‚æ•°è§„æ¨¡\n\nå•å¡æ˜¾å­˜ä¸Šé™ä¸º Mï¼Œæ¨¡å‹å¹¶è¡Œåº¦ä¸º Nï¼Œå¸Œæœ›è¾¾åˆ° (P O(M N)) é‡çº§çš„æ¨¡å‹ã€‚\n\nç›®æ ‡ 2ï¼šåœ¨ç»™å®šç½‘ç»œæ¡ä»¶ä¸‹æœ€å°åŒ–é€šä¿¡å¼€é”€ç›¸å¯¹å æ¯”\n\næ¯å±‚åªåšæœ‰é™æ¬¡ã€æ¨¡å¼å›ºå®šçš„ all-reduce / all-gatherï¼Œå¹¶å°½é‡è®©é€šä¿¡çš„æ•°æ®é‡ä¸å‚æ•°åˆ‡ç‰‡å¤§å°ç›¸å…³ï¼Œè€Œä¸æ˜¯ä¸ batch Ã— seq Ã— vocab è¿™ç§å¤§å¼ é‡ç›´æ¥æˆæ­£æ¯”ã€‚(arXiv)\n\nç›®æ ‡ 3ï¼šæ”¾å¤§æ¨¡å‹è§„æ¨¡å¸¦æ¥çš„æ€§èƒ½æ”¶ç›Šï¼Œè€Œä¸æ˜¯â€œåªå †å‚æ•°â€\n\né€šè¿‡åœ¨ WikiText-103ã€LAMBADAã€RACE ç­‰ä»»åŠ¡ä¸ŠæŠ¥å‘Š SOTAï¼Œè¯æ˜æ¨¡å‹å˜å¤§æœ‰å®è´¨ä»»åŠ¡æ”¶ç›Šã€‚(arXiv)\n\n\n4.2 æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡\nè®ºæ–‡ä¸»è¦ä½¿ç”¨äº†ä»¥ä¸‹å‡ ç±»æŒ‡æ ‡æ¥è¯„ä¼°æ–¹æ³•æ•ˆæœï¼š(arXiv)\n\næ¨¡å‹å‚æ•°è§„æ¨¡ï¼ˆ#Paramsï¼‰\n\nGPT-2 ç±»æ¨¡å‹ï¼šä» 1.2B æ‰©å±•åˆ° 8.3Bï¼›\nBERT ç±»æ¨¡å‹ï¼šä» 0.3B æ‰©å±•åˆ° 3.9Bã€‚\nè¿™æ˜¯æœ€ç›´æ¥ä½“ç°â€œæ¨¡å‹åˆ°åº•æœ‰å¤šå¤§â€çš„æŒ‡æ ‡ï¼Œä¹Ÿæ˜¯å±‚å†…æ¨¡å‹å¹¶è¡Œå­˜åœ¨çš„å‰æã€‚\n\nFLOPs åˆ©ç”¨ç‡ / Sustained FLOPs\n\nå•å¡åŸºçº¿ï¼š1.2B æ¨¡å‹åœ¨å• V100 ä¸Šèƒ½ sustain 39 TFLOPsï¼Œçº¦ä¸ºç†è®ºå³°å€¼çš„ 30%ã€‚(arXiv)\nå¤šå¡è®­ç»ƒï¼š8.3B æ¨¡å‹åœ¨ 512 å¼  V100 ä¸Šèƒ½è¾¾åˆ° 15.1 PFLOPsï¼Œçº¦ 76% å¼ºæ‰©å±•æ•ˆç‡ã€‚(arXiv)\nè¯¥æŒ‡æ ‡ä½“ç°â€œGPU è¢«ç”¨å¾—æœ‰å¤šæ»¡â€ï¼Œæ˜¯ç³»ç»Ÿæ€§èƒ½çš„ç¡¬æŒ‡æ ‡ã€‚\n\næ‰©å±•æ•ˆç‡ï¼ˆScaling Efficiencyï¼‰\n\nå¼ºæ‰©å±•ï¼šä¿æŒæ¨¡å‹å’Œ batch ä¸å˜ï¼Œå¢åŠ  GPU æ•°ï¼Œè§‚å¯Ÿååå˜åŒ–å’Œæ•ˆç‡ï¼›\nå¼±æ‰©å±•ï¼šéš GPU æ•°å¢åŠ æˆæ¯”ä¾‹åœ°å¢å¤§æ¨¡å‹è§„æ¨¡æˆ– batchï¼Œçœ‹ per-GPU åˆ©ç”¨ç‡æ˜¯å¦ç¨³å®šã€‚(arXiv)\nMegatron-LM æŠ¥å‘Šï¼Œåœ¨ 1â€“8 å¡æ¨¡å‹å¹¶è¡Œ + 64-way æ•°æ®å¹¶è¡Œçš„ 512 GPU åœºæ™¯ä¸‹ï¼Œå¼ºæ‰©å±•æ•ˆç‡çº¦ä¸º 74â€“76%ã€‚\n\nä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼ˆNLP Benchmarkï¼‰\n\nGPT-2 ç±»æ¨¡å‹ï¼š\n\nWikiText-103 perplexityï¼š10.8ï¼ˆç›¸å¯¹ 15.8 çš„ SOTA æœ‰æ˜æ˜¾æå‡ï¼‰ï¼›\nLAMBADA å‡†ç¡®ç‡ï¼š66.5%ï¼ˆä¼˜äº 63.2% çš„ SOTAï¼‰ã€‚(arXiv)\n\nBERT ç±»æ¨¡å‹ï¼š\n\nRACE å‡†ç¡®ç‡ï¼š90.9%ï¼ˆä¼˜äº 89.4% çš„ SOTAï¼‰ã€‚(arXiv)\n\nè¿™äº›æŒ‡æ ‡è¯æ˜å¤§æ¨¡å‹ä¸ä»…â€œèƒ½è®­å‡ºæ¥â€ï¼Œè€Œä¸”â€œèƒ½å¸¦æ¥æ›´å¥½çš„ä»»åŠ¡æ•ˆæœâ€ã€‚\n\nè®­ç»ƒç¨³å®šæ€§ç›¸å…³æŒ‡æ ‡ï¼ˆå¦‚æ”¶æ•›æ›²çº¿ï¼‰\n\nå°¤å…¶åœ¨ BERT å¤§æ¨¡å‹ä¸­ï¼Œå¯¹æ¯”ä¸åŒ LayerNorm å¸ƒå±€ä¸‹çš„è®­ç»ƒ loss æ”¶æ•›æƒ…å†µï¼Œå±•ç¤ºäº†éƒ¨åˆ†æ¶æ„é€‰æ‹©åœ¨å¤§è§„æ¨¡ä¸‹ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚(arXiv)\n\n\n\näº”ã€ä¸»è¦å®éªŒå‘ç°\nç»¼åˆæ¥çœ‹ï¼ŒMegatron-LM åœ¨å®éªŒéƒ¨åˆ†ä¼ é€’çš„ç»“è®ºå¯ä»¥ç”¨å‡ æ¡å·¥ç¨‹åŒ–çš„è¯´æ³•æ¦‚æ‹¬ï¼š(arXiv)\n\né‡‡ç”¨å±‚å†…å¼ é‡å¹¶è¡Œï¼Œå¯ä»¥åœ¨ 512 å¼  V100 GPU ä¸Šè®­ç»ƒ 8.3B GPT-2 ç±»æ¨¡å‹ å’Œ 3.9B BERT ç±»æ¨¡å‹ï¼Œçªç ´äº†å½“æ—¶é€šç”¨ GPU ä¸Šçš„æ¨¡å‹è§„æ¨¡æé™ã€‚\nåœ¨å¼ºåŸºçº¿ï¼ˆ1.2B å•å¡ 39 TFLOPs â‰ˆ 30% å³°å€¼ï¼‰çš„åŸºç¡€ä¸Šï¼Œ8.3B æ¨¡å‹åœ¨ 512 å¡ä¸Šä¾ç„¶èƒ½ sustain 15.1 PFLOPsï¼Œå¯¹åº”çº¦ 76% çš„å¼ºæ‰©å±•æ•ˆç‡ã€‚\næ¨¡å‹å˜å¤§åï¼Œè¯­è¨€å»ºæ¨¡å’Œé˜…è¯»ç†è§£ä»»åŠ¡çš„æ€§èƒ½éƒ½æœ‰æ˜¾è‘—æå‡ï¼šWikiText-103 perplexity ä» 15.8 é™åˆ° 10.8ï¼›LAMBADA å‡†ç¡®ç‡ä» 63.2% å‡åˆ° 66.5%ï¼›RACE å‡†ç¡®ç‡ä» 89.4% å‡åˆ° 90.9%ã€‚\nå¯¹ BERT æ¶æ„åšäº†ç³»ç»Ÿçš„ LayerNorm ä½ç½®å¯¹æ¯”ï¼Œå‘ç°åŸå§‹ Post-LN è®¾è®¡åœ¨ 3.9B è§„æ¨¡ä¸‹ä¸ç¨³å®šï¼Œè€Œæ”¹æˆ Pre-LN åå¯ä»¥æ˜¾è‘—æ”¹å–„è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆæ€§èƒ½ã€‚\n\n5.1 å…³é”®å›¾è¡¨è§£è¯»\n\næ‰©å±•æ€§å›¾ï¼šåå/æ•ˆç‡ vs GPU æ•°\n\nå±•ç¤ºäº†ä»å• GPU åˆ° 512 GPU çš„å¼ºã€å¼±æ‰©å±•æ›²çº¿ï¼šéšç€ GPU æ•°å¢åŠ ï¼Œååæ¥è¿‘çº¿æ€§æå‡ï¼Œå¼ºæ‰©å±•æ•ˆç‡ä»ä¿æŒåœ¨ ~74â€“76%ã€‚(arXiv)\nè¿™ä¸ªå›¾æ”¯æ’‘äº†â€œTP åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸Šæ˜¯å¯æ‰©å±•çš„â€è¿™ä¸€ä¸»å¼ ã€‚\n\nä»»åŠ¡æ€§èƒ½ vs æ¨¡å‹è§„æ¨¡æ›²çº¿\n\nä¾‹å¦‚ GPT-2 æ¨¡å‹åœ¨ WikiText-103ã€LAMBADA ä¸Šçš„æ€§èƒ½éšå‚æ•°è§„æ¨¡çš„å˜åŒ–ï¼›\nè¶‹åŠ¿æ˜¯ï¼šæ¨¡å‹è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ï¼Œ8.3B æ¨¡å‹æ˜æ˜¾ä¼˜äºä¹‹å‰çš„ SOTAã€‚(arXiv)\nè¿™è¯´æ˜â€œæ‰©å±•æ¨¡å‹ä¸æ˜¯çº¯ç‚«æŠ€ï¼Œè€Œæ˜¯èƒ½æ¢æ¥çœŸå®çš„ä»»åŠ¡æ”¶ç›Šâ€ã€‚\n\nBERT æ¶æ„å˜ä½“çš„æ”¶æ•›æ›²çº¿ï¼ˆä¸åŒ LN å¸ƒå±€å¯¹æ¯”ï¼‰\n\nå›¾ä¸­å¯¹æ¯”äº†ä¸åŒ LayerNorm æ”¾ç½®æ–¹å¼ä¸‹çš„è®­ç»ƒ loss éš step å˜åŒ–ï¼›\nå¯ä»¥çœ‹å‡ºæŸäº›å¸ƒå±€åœ¨ 3.9B æ¨¡å‹ä¸Šéš¾ä»¥æ”¶æ•›ï¼Œè€Œ Pre-LN æ›´ç¨³å®šã€æ•ˆæœæ›´å¥½ã€‚(arXiv)\nè¿™å¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡ regime ä¸‹æ¶æ„ç»†èŠ‚çš„é‡è¦æ€§ã€‚\n\n\nç»“æœè§£è¯»ä¸è¾¹ç•Œ\nä»ç»“æœæ¥çœ‹ï¼ŒMegatron-LM åœ¨å®ƒå…³æ³¨çš„æ ¸å¿ƒé—®é¢˜ä¸Šç»™å‡ºäº†æ¯”è¾ƒå®Œæ•´çš„è¯æ®é“¾ï¼š(arXiv)\n\nç³»ç»Ÿå±‚é¢ï¼šæœ‰å•å¡å¼ºåŸºçº¿ï¼Œæœ‰ 512 å¡æ‰©å±•ç»“æœï¼Œæœ‰ FLOPs åˆ©ç”¨ç‡å’Œæ‰©å±•æ•ˆç‡æŒ‡æ ‡ï¼›\næ¨¡å‹æ•ˆæœå±‚é¢ï¼šåœ¨å¤šä¸ªå…¬å¼€åŸºå‡†ä¸Šè¾¾åˆ°æˆ–è¶…è¶Š SOTAï¼Œä¸”æ¸…æ™°å±•ç¤ºäº†â€œæ¨¡å‹è§„æ¨¡ â†’ æ€§èƒ½â€çš„æ”¶ç›Šæ›²çº¿ï¼›\nç¨³å®šæ€§å±‚é¢ï¼šå¯¹ BERT æ¶æ„çš„ LN å¸ƒå±€åšäº†å¯è§†åŒ–å¯¹æ¯”ï¼Œå¼ºè°ƒäº†ç»“æ„è®¾è®¡å¯¹å¤§æ¨¡å‹è®­ç»ƒçš„å½±å“ã€‚\n\nä½†ä¹Ÿæœ‰ä¸€äº›è¾¹ç•Œå’Œæœªè¦†ç›–çš„ç‚¹éœ€è¦è¯»è€…è‡ªå·±è„‘è¡¥ï¼š\n\nå®éªŒä»…åœ¨ NVIDIA è‡ªå®¶ç¡¬ä»¶å’Œæ ˆä¸Šå®Œæˆï¼Œå¯¹ä¸åŒç¡¬ä»¶/é€šä¿¡åº“çš„æ³›åŒ–éœ€è¦è‡ªå·±è¯„ä¼°ï¼›\næ²¡æœ‰ä¸å½“æ—¶çš„å…¶ä»–å¹¶è¡Œæ–¹æ¡ˆï¼ˆå¦‚ GPipe/PipeDream ç­‰ï¼‰è¿›è¡Œæ›´ç»†è‡´çš„ç³»ç»Ÿ head-to-head å¯¹æ¯”ï¼›(Minjia Zhang)\nå¯¹é€šä¿¡å¼€é”€çš„ç†è®ºå»ºæ¨¡å’Œå¤æ‚æ‹“æ‰‘ä¸‹çš„è¡¨ç°ï¼Œåªæœ‰ç»éªŒæ€§è¯´æ˜ï¼Œæ²¡æœ‰ç»™å‡ºæ›´ formal çš„åˆ†æã€‚\n\n\nå…­ã€ä¼˜ç‚¹ä¸å±€é™\näº®ç‚¹ï¼ˆStrengthsï¼‰\n\né—®é¢˜é€‰æ‹©åˆ°ä½ï¼šç›´æ¥ç„å‡†â€œå¤šäº¿å‚æ•° Transformer æ€ä¹ˆè®­â€è¿™ä¸ªæ ¸å¿ƒç—›ç‚¹ æŠŠæ˜¾å­˜ç“¶é¢ˆå’Œæ•°æ®å¹¶è¡Œçš„å±€é™è®²æ¸…æ¥šï¼Œä»¥ GPT-2/BERT ä¸ºä¾‹è¯´æ˜â€œå¤§æ¨¡å‹ç¡®å®æœ‰ç”¨â€ï¼Œé—®é¢˜åŠ¨æœºéå¸¸è‡ªç„¶ä¸”é‡è¦ã€‚(arXiv)\næ–¹æ³•è§„åˆ™ã€ç®€å•ã€å·¥ç¨‹å‹å¥½ è¡Œåˆ‡/åˆ—åˆ‡ + å°‘é‡å›ºå®šä½ç½®çš„é›†ä½“é€šä¿¡ï¼Œå‡ ä¹å¯ä»¥è¢«æŠ½è±¡ä¸ºä¸€å¥—â€œæ¨¡æ¿â€ï¼š\n\nçº¿æ€§å±‚è¦ä¹ˆæŒ‰åˆ—åˆ‡ï¼Œè¦ä¹ˆæŒ‰è¡Œåˆ‡ï¼›æ³¨æ„åŠ›æŒ‰ head åˆ‡ï¼›æ¯å±‚å‰åå„å‡ æ¬¡ all-reduce/all-gatherã€‚ ä¸ä¾èµ–æ–°æ¡†æ¶æˆ–ç‰¹æ®Šç¼–è¯‘å™¨ï¼Œå®Œå…¨å¯ä»¥åœ¨åŸç”Ÿ PyTorch ä¸Šå®ç°ï¼Œå¯¹äºå·¥ç¨‹å›¢é˜Ÿæ¥è¯´æˆæœ¬ä½ä¸”å¯ç»´æŠ¤ã€‚(arXiv)\n\nç³»ç»Ÿæ€§èƒ½ä¸ä»»åŠ¡æ€§èƒ½å…¼é¡¾ åŒæ—¶ç»™å‡º FLOPs åˆ©ç”¨ç‡ã€æ‰©å±•æ•ˆç‡å’Œæ ‡å‡† benchmark ä¸Šçš„ SOTA æŒ‡æ ‡ï¼Œè¯´æ˜å®ƒæ—¢æ˜¯ç³»ç»Ÿä¼˜åŒ–ï¼Œåˆæœ‰å®è´¨çš„æ¨¡å‹æ•ˆæœæå‡ã€‚(arXiv)\nå¼ºè°ƒä¸å…¶ä»–å¹¶è¡Œæ–¹å¼çš„â€œæ­£äº¤å¯ç»„åˆâ€ç‰¹æ€§ è®ºæ–‡æ¸…æ¥šæŒ‡å‡ºå±‚å†…æ¨¡å‹å¹¶è¡Œå¯ä¸æ•°æ®å¹¶è¡Œã€æµæ°´å¹¶è¡Œå åŠ ä½¿ç”¨ï¼Œè¿™ä¸€ç‚¹åæ¥åœ¨ 3D/4D å¹¶è¡Œä¸­è¢«å¹¿æ³›ç»§æ‰¿ã€‚(arXiv)\nå¯¹ BERT æ¶æ„çš„ LayerNorm ä½ç½®ç»™å‡ºå¤§è§„æ¨¡ä¸‹çš„ç»éªŒç»“è®º å±•ç¤ºäº† Pre-LN å¯¹å¤§è§„æ¨¡æ¨¡å‹ç¨³å®šæ€§çš„å¥½å¤„ï¼Œå¯¹åç»­ Transformer ç»“æ„è®¾è®¡æœ‰æ·±è¿œå½±å“ã€‚(arXiv)\n\nå±€é™ï¼ˆLimitationsï¼‰\n\nå¯¹ç½‘ç»œæ‹“æ‰‘ã€å¼‚æ„é›†ç¾¤ç­‰å¤æ‚ç¯å¢ƒçš„è€ƒè™‘è¾ƒå°‘ å‡å®šçš„æ˜¯é«˜é€Ÿ NVSwitch + InfiniBand çš„â€œå¥½ç¯å¢ƒâ€ï¼Œç®€å• TP åˆ‡æ³•åœ¨å¼±ç½‘ç»œæˆ–å¼‚æ„ç¯å¢ƒä¸‹å¯èƒ½è¡¨ç°ä¸ä½³ã€‚(arXiv)\nå®éªŒä¸»è¦åŸºäºè‡ªå®¶ç¡¬ä»¶æ ˆï¼Œæ³›åŒ–åˆ°å…¶ä»–å¹³å°éœ€è¦é¢å¤–å·¥ä½œ æœªç»™å‡ºåœ¨å…¶ä»–ç¡¬ä»¶ï¼ˆå¦‚ TPU æˆ–ä¸åŒå‚å•† GPUï¼‰ä¸Šçš„éªŒè¯ï¼Œå¯¹è·¨ç”Ÿæ€è¿ç§»çš„è¯»è€…ä¸å¤Ÿå‹å¥½ã€‚(arXiv)\nç†è®ºåˆ†æåè–„ï¼Œæ›´å¤šä¾èµ–ç»éªŒå’Œå®æµ‹ ä¾‹å¦‚è¡Œåˆ‡/åˆ—åˆ‡ç»„åˆä¸ºä»€ä¹ˆâ€œå¥½ç”¨â€ï¼Œåœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹æ¥è¿‘æœ€ä¼˜ï¼Œç¼ºä¹æ›´ formal çš„æ¨¡å‹å’Œä¿è¯ï¼› é€šä¿¡æˆæœ¬å’Œæ€§èƒ½è¾¹ç•Œä¹Ÿä¸»è¦æ˜¯é€šè¿‡å®æµ‹è¯´æ˜ï¼Œè€Œä¸æ˜¯å»ºç«‹ç»Ÿä¸€çš„åˆ†ææ¡†æ¶ã€‚(Minjia Zhang)\nå¯¹æœªæ¥å¤æ‚æ¨¡å‹ç»“æ„ï¼ˆMoEã€ç¨€ç–æ³¨æ„åŠ›ç­‰ï¼‰çš„é€‚é…æœªè®¨è®º å¯ä»¥ç†è§£ä¸ºæ—¶ä»£é™åˆ¶ï¼Œä½†ä»ä»Šå¤©å›çœ‹ï¼Œå®ƒä¸»è¦è¦†ç›– dense Transformerï¼Œå¯¹åç»­æ›´å¤æ‚æ¶æ„æä¾›çš„æ˜¯å¯å‘è€Œéç›´æ¥æ–¹æ¡ˆã€‚\n\n\nä¸ƒã€ä¸šå†…ç›¸å…³å·¥ä½œå¯¹æ¯”\nè¿™é‡Œé€‰å‡ ä¸ªä¸ä½ ç»å¸¸æ¥è§¦ã€ä¸”å’Œ Megatron-LM å¼ºç›¸å…³çš„å·¥ä½œåšç®€è¦å¯¹æ¯”ï¼š(Minjia Zhang)\n\nGPT-2ï¼šLanguage Models are Unsupervised Multitask Learners\n\né‡ç‚¹æ˜¯â€œscale â†’ zero-shot èƒ½åŠ›â€ï¼Œä¸»è¦ç”¨å•æœº + æ•°æ®å¹¶è¡Œã€‚\næ²¡æœ‰æå‡ºç³»ç»Ÿå±‚é¢çš„æ–°å¹¶è¡Œæ–¹æ³•ã€‚\n\nGPipe / PipeDreamï¼šåŸºäºæµæ°´å¹¶è¡Œçš„å·¨å‹ç½‘ç»œè®­ç»ƒ\n\nå°†ç½‘ç»œæŒ‰å±‚åˆ‡æˆä¸åŒ stageï¼Œé€šè¿‡ micro-batch æµæ°´ä¼ é€’å®ç°æµæ°´å¹¶è¡Œï¼›\nå…³æ³¨çš„æ˜¯â€œæ·±åº¦æ–¹å‘â€çš„å¹¶è¡Œä¸æ˜¾å­˜æ‰©å±•ã€‚\n\nåç»­å¤šç»´å¹¶è¡Œå·¥ä½œï¼ˆå¦‚æ›´åæ¥çš„ Megatron-DeepSpeedã€3D/4D å¹¶è¡Œæ¡†æ¶ï¼‰(ACM Digital Library)\n\nåœ¨ Megatron TP çš„åŸºç¡€ä¸Šå åŠ æµæ°´å¹¶è¡Œã€ZeRO ç­‰æŠ€æœ¯ï¼Œå®ç° DP+TP+PP çš„è”åˆæœç´¢ä¸è°ƒåº¦ï¼›\nè¿›ä¸€æ­¥è§£å†³ trillion çº§æ¨¡å‹è®­ç»ƒä¸­çš„é€šä¿¡ã€å†…å­˜å’Œè°ƒåº¦é—®é¢˜ã€‚\n\n\né—®é¢˜å®šä¹‰ç»´åº¦ï¼š\n\nGPT-2ï¼šèƒ½å¦é€šè¿‡å•æœº/å°‘æœºè®­ç»ƒæ›´å¤§çš„è¯­è¨€æ¨¡å‹ï¼Œä»è€Œè·å¾—æ›´å¼ºçš„ zero-shot èƒ½åŠ›ï¼Ÿ\nGPipe / PipeDreamï¼šå¦‚ä½•é€šè¿‡æµæ°´å¹¶è¡Œè§£å†³â€œç½‘ç»œå¾ˆæ·±â€å¸¦æ¥çš„æ˜¾å­˜ä¸ååé—®é¢˜ï¼Ÿ\nMegatron-LMï¼šå¦‚ä½•é€šè¿‡å±‚å†…å¼ é‡å¹¶è¡Œè§£å†³â€œå•å±‚çŸ©é˜µå¤ªå¤§â€å¸¦æ¥çš„æ˜¾å­˜ä¸ååé—®é¢˜ï¼Ÿ\n\næ–¹æ³•è·¯çº¿å…³ç³»ï¼š\n\nMegatron-LM ä¸ GPT-2ï¼šå‰è€…åœ¨ç³»ç»Ÿå’Œå¹¶è¡Œç­–ç•¥ä¸Šä¸º GPT-2 ç±»æ¶æ„æä¾›æ‰©å±•è·¯å¾„ï¼Œä¸¤è€…æ˜¯å¯å åŠ çš„ï¼›\nMegatron-LM ä¸ GPipe/PipeDreamï¼šä¸€ä¸ªåœ¨å±‚å†…ã€ä¸€ä¸ªåœ¨å±‚é—´ï¼Œæœ¬è´¨ä¸Šæ˜¯æ­£äº¤ç»´åº¦ï¼Œå¯ä»¥ç»„åˆæˆ 3D å¹¶è¡Œï¼›\nåç»­ 3D/4D å¹¶è¡Œå·¥ä½œï¼šæ›´å¤šæ˜¯æŠŠ Megatron æ¨¡å¼åŒ– TP å˜æˆä¸€ä¸ªç»„ä»¶ï¼ŒåŒæ—¶å¼•å…¥è‡ªåŠ¨æœç´¢ä¸æ›´å¤æ‚è°ƒåº¦ã€‚\n\n7.1 ä¸ªäººè§‚ç‚¹\nä»æ•´ä½“è®ºè¯æ–¹å¼çœ‹ï¼ŒMegatron-LM æ˜¯ä¸€ç¯‡å¾ˆâ€œå·¥ç¨‹å‘³â€çš„ç³»ç»Ÿè®ºæ–‡ï¼šåŸºçº¿å¼ºã€å®éªŒæ‰å®ã€ç»“æœæ¸…æ™°ã€‚ä½†ç«™åœ¨ä»Šå¤©çš„è§’åº¦ï¼Œä¼šè§‰å¾—æœ‰å‡ ä¸ªå¯ä»¥åŠ å¼ºçš„ç‚¹ï¼š\n\nbaseline ä¸»è¦é›†ä¸­åœ¨â€œæ—  TP çš„æ•°æ®å¹¶è¡Œâ€å’Œç®€å•çš„æ¨¡å‹å¹¶è¡Œä¸Šï¼Œå¦‚æœèƒ½æœ‰æ›´å¤šä¸å½“æ—¶å…¶å®ƒå¹¶è¡Œæ–¹æ³•ï¼ˆGPipe/PipeDreamï¼‰çš„æ­£é¢å¯¹æ¯”ï¼Œä¼šè®©â€œä¸ºä»€ä¹ˆé€‰ TPâ€è¿™ä¸€ç‚¹æ›´åŠ æœ‰è¯´æœåŠ›ã€‚\nå¯¹é€šä¿¡æˆæœ¬çš„åˆ†æåç»éªŒï¼šä¾èµ–å®æµ‹ FLOPs åˆ©ç”¨ç‡å’Œæ‰©å±•æ•ˆç‡æ›²çº¿ï¼Œå¯¹â€œç½‘ç»œæ¡ä»¶å˜åŒ–æ—¶ TP æ˜¯å¦ä»ç„¶æœ‰ä¼˜åŠ¿â€ç¼ºå°‘æ›´ç³»ç»Ÿçš„è§£é‡Šã€‚\nå…³äºæ¶æ„ç¨³å®šæ€§ï¼ˆLayerNorm ç­‰ï¼‰çš„éƒ¨åˆ†ï¼Œå¦‚æœèƒ½é…åˆä¸€äº›æ¢¯åº¦ä¼ æ’­æˆ–è°±åˆ†æï¼ŒæŠ½è±¡å‡ºæ›´ä¸€èˆ¬çš„åŸåˆ™ï¼Œä¼šæ›´å®¹æ˜“è¢«åç»­å·¥ä½œç»§æ‰¿å’Œæ¨å¹¿ã€‚\n\nå¦‚æœè®©æˆ‘åŸºäºè¿™ç¯‡è®ºæ–‡å†å†™ä¸€ç‰ˆâ€œMegatron-LM 202Xâ€ï¼Œåœ¨ä¿ç•™æ ¸å¿ƒ TP è®¾è®¡ä¸å˜çš„å‰æä¸‹ï¼Œæˆ‘ä¼šå€¾å‘äºï¼š\n\nå¢åŠ ä¸€ä¸ªç®€æ´çš„â€œé€šä¿¡ vs è®¡ç®—â€æ€§èƒ½æ¨¡å‹ï¼Œåˆ†æä¸åŒç½‘ç»œæ¡ä»¶ä¸‹ TP çš„é€‚ç”¨è¾¹ç•Œï¼›\nå’Œä»£è¡¨æ€§çš„å¤šç»´å¹¶è¡Œæ–¹æ¡ˆåš head-to-head å¯¹æ¯”ï¼ˆåŒ…æ‹¬ååã€æ˜¾å­˜ã€å¤æ‚åº¦ï¼‰ï¼›\næŠŠå…³äº LN ä½ç½®å’Œå¤§æ¨¡å‹ç¨³å®šæ€§çš„ç»éªŒï¼Œä¸Šå‡ä¸ºæ›´é€šç”¨çš„æŒ‡å¯¼åŸåˆ™ï¼Œä»¥ä¾¿è¿ç§»åˆ°å…¶ä»–æ¶æ„ä¸Šã€‚\n\n\nå…«ã€åœ¨å®é™…è®­ç»ƒæ ˆä¸­å¦‚ä½•è½åœ°ï¼Ÿ\nä»å·¥ç¨‹å®è·µçš„è§’åº¦ï¼Œå¦‚æœè¦æŠŠ Megatron-LM çš„æ€æƒ³å¼•å…¥åˆ°è‡ªå·±çš„å¤§è§„æ¨¡è®­ç»ƒæ ˆï¼ˆå¦‚ Megatron ç³»åˆ—ã€DeepSpeedã€vLLM æˆ–è‡ªç ”æ¡†æ¶ï¼‰ï¼Œå¤§è‡´éœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š(arXiv)\n1. æ•°æ®æ‰“åŒ…ä¸å‰å‘é€šè·¯\n\nä¿æŒåŸæœ‰ DataLoader åŸºæœ¬ä¸å˜ï¼Œä¸»è¦è¦æ±‚ embedding å’Œè¾“å‡ºå±‚æ”¯æŒâ€œåˆ†ç‰‡æƒé‡ + å…¨å±€å‘é‡â€æ¨¡å¼ï¼›\nåœ¨è¾“å…¥ embedding é˜¶æ®µï¼Œå¼•å…¥â€œåˆ†ç‰‡æŸ¥è¡¨ + all-reduce èšåˆâ€çš„é€»è¾‘ã€‚\n\n2. å¹¶è¡Œè°ƒåº¦ä¸è¿›ç¨‹ç»„\n\nåœ¨æ•°æ®å¹¶è¡Œä¹‹å¤–å¢åŠ â€œæ¨¡å‹å¹¶è¡Œç»„â€æ¦‚å¿µï¼›\næ˜ç¡®åˆ’åˆ† tp_group / dp_groupï¼Œå¯¹æ¯ä¸ªå¹¶è¡ŒåŸè¯­ç»‘å®šåˆ°æ­£ç¡®çš„ group ä¸Šï¼›\nè°ƒåº¦å™¨éœ€è¦ä¿è¯ï¼šç»„å†…å®Œæˆ TP é€šä¿¡ï¼Œå†è¿›è¡Œç»„é—´ DP åŒæ­¥ã€‚\n\n3. å¼ é‡å¹¶è¡Œç­–ç•¥è½åœ°\n\næŠŠ Transformer ä¸­æ‰€æœ‰çº¿æ€§å±‚æ”¹å†™ä¸ºâ€œè¡Œåˆ‡/åˆ—åˆ‡â€ç‰ˆæœ¬ï¼Œæ³¨æ„è‡ªæ³¨æ„åŠ›å’Œ MLP çš„ç‰¹å®šç»„åˆï¼›\nç¡®ä¿æ¿€æ´»å½¢çŠ¶å’Œåˆ‡åˆ†æ–¹å‘ä¸€è‡´ï¼Œé¿å… subtle çš„ shape å¯¹ä¸ä¸Šé—®é¢˜ï¼›\nå¯¹ fused kernel åšå…¼å®¹æ€§æ£€æŸ¥ï¼ˆè¾“å…¥è¾“å‡ºæ˜¯å¦æ”¯æŒåˆ†ç‰‡ï¼‰ã€‚\n\n4. kernel ä¸ç®—å­é›†æˆ\n\nå¯¹å·²æœ‰é«˜æ€§èƒ½ GEMM/attention kernelï¼Œç¡®è®¤åœ¨æ–°çš„å¼ é‡å½¢çŠ¶ä¸Šä¾ç„¶èƒ½é«˜æ•ˆè¿è¡Œï¼›\nå¯¹ embedding + cross-entropy è¿™ç±»ç»„åˆç®—å­è®¾è®¡å¹¶è¡Œç‰ˆæœ¬ï¼Œå‡å°‘é€šä¿¡é‡ï¼›\nè€ƒè™‘å°†éƒ¨åˆ†é€šä¿¡ä¸ç®—å­èåˆï¼Œå‡å°‘ä¸­é—´å†™å›ã€‚\n\n5. é€šä¿¡ backend è°ƒä¼˜\n\nåœ¨æ‹“æ‰‘å±‚é¢å°½é‡å°† TP ç»„é™åˆ¶åœ¨é«˜é€Ÿäº’è”èŒƒå›´ï¼ˆå¦‚å•æœº NVLink/NVSwitchï¼‰ï¼›\nè°ƒè¯•å’Œç›‘æ§ TP ç»„å†… all-reduce / all-gather çš„å¸¦å®½åˆ©ç”¨ã€å»¶è¿Ÿï¼Œå¿…è¦æ—¶é™å®šç®—æ³•ï¼ˆring/treeï¼‰æˆ–è®¾ç½®åˆ†ç»„ç­–ç•¥ï¼›\nç»“åˆ profiler ç¡®è®¤é€šä¿¡ä¸è®¡ç®—çš„ overlap æ˜¯å¦æœ‰æ•ˆã€‚\n\n6. é…ç½®ä¸è°ƒå‚ç­–ç•¥\n\né’ˆå¯¹ä¸åŒæ¨¡å‹è§„æ¨¡å’Œç¡¬ä»¶èµ„æºï¼Œè®¾è®¡ä¸€å¥— TP size Ã— DP size çš„æ¨èç»„åˆï¼ˆæ¯”å¦‚ TP=2/4/8ï¼ŒDP æ ¹æ® GPU æ€»æ•°è‡ªåŠ¨æ¨å¯¼ï¼‰ï¼›\né…åˆæ··åˆç²¾åº¦ã€gradient checkpointingã€ZeRO ç­‰æ–¹æ³•ï¼Œæ•´ä½“è°ƒèŠ‚æ˜¾å­˜å³°å€¼å’Œååï¼›(ACM Digital Library)\nå¯¹ä¸åŒæ¨¡å‹ä¸æ•°æ®é›†ï¼Œç§¯ç´¯ç»éªŒæ€§çš„â€œå¹³è¡¡ç‚¹â€ï¼ˆå¦‚â€œTP=8 + DP=16 æ¯” TP=4 + DP=32 æ›´åˆ’ç®—è¿˜æ˜¯ç›¸åâ€ï¼‰ã€‚\n\n\nä¹ã€å€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢çš„ç ”ç©¶æ–¹å‘\nç»“åˆ Megatron-LM çš„æ€è·¯ï¼Œä»¥åŠå½“å‰å¤§æ¨¡å‹è®­ç»ƒçš„å‘å±•ï¼Œå¯ä»¥è¡ç”Ÿå‡ºå¤šæ¡ç»§ç»­æ·±å…¥çš„æ–¹å‘ï¼š(ACM Digital Library)\næ–¹å‘ä¸€ï¼šå¤šç»´å¹¶è¡Œçš„è‡ªåŠ¨è§„åˆ’ä¸è°ƒåº¦\nåœ¨ TP ä¹‹å¤–å¼•å…¥ DPã€PPã€åºåˆ—å¹¶è¡Œã€ä¸“å®¶å¹¶è¡Œåï¼Œäººè‚‰è°ƒå‚æˆæœ¬æé«˜ã€‚å¦‚ä½•åœ¨ç»™å®šæ¨¡å‹ç»“æ„å’Œé›†ç¾¤æ‹“æ‰‘çš„å‰æä¸‹ï¼Œè‡ªåŠ¨æœç´¢ TP/PP/DP ç­‰å¤šç»´å¹¶è¡Œç»„åˆï¼Œå¹¶ç”Ÿæˆå¯æ‰§è¡Œçš„å¹¶è¡Œè®¡åˆ’ï¼Œæ˜¯ä¸€ä¸ªé«˜åº¦å·¥ç¨‹åŒ–åˆéå¸¸æœ‰ä»·å€¼çš„é—®é¢˜ã€‚åç»­çš„ä¸€äº›å·¥ä½œï¼ˆå¦‚è‡ªåŠ¨æ··åˆå¹¶è¡Œæ¡†æ¶ï¼‰å°±åœ¨å°è¯•è§£å†³è¿™ç±»é—®é¢˜ã€‚\næ–¹å‘äºŒï¼šæ‹“æ‰‘æ„ŸçŸ¥çš„å¼ é‡å¹¶è¡Œ\nMegatron-LM é»˜è®¤çš„æ˜¯è§„æ•´ã€å¸¦å®½å……è¶³çš„æ‹“æ‰‘ã€‚ç°å®ä¸­ç»å¸¸é‡åˆ°å¼‚æ„ GPUã€ç½‘ç»œä¸å¯¹ç§°ã€å¤šé›†ç¾¤ç¯å¢ƒã€‚ä¸ºæ­¤å¯ä»¥è€ƒè™‘ï¼š\n\næ ¹æ®ç‰©ç†æ‹“æ‰‘åˆ’åˆ† TP ç»„ï¼Œå°½é‡æŠŠé«˜é¢‘é€šä¿¡é™åˆ¶åœ¨å¸¦å®½æ›´å¥½çš„åŒºåŸŸï¼›\nå¯¹äºè·¨èŠ‚ç‚¹é€šä¿¡è¾ƒæ…¢çš„åœºæ™¯ï¼Œæ¢ç´¢æ–°çš„é€šä¿¡è§„çº¦æ–¹å¼æˆ–éƒ¨åˆ†é‡æ–°å¸ƒå±€æ¨¡å‹åˆ†ç‰‡ï¼›\nå°†æ‹“æ‰‘ä¿¡æ¯çº³å…¥å¹¶è¡Œè§„åˆ’ä¸­ï¼Œä½¿ TP ä¸å†â€œç›²ç›®å‡åˆ†â€ã€‚\n\næ–¹å‘ä¸‰ï¼šå¼ é‡å¹¶è¡Œä¸ç¨€ç–ç»“æ„ååŒ\né¢å¯¹ MoEã€ç¨€ç–æ³¨æ„åŠ›ã€å¤§è§„æ¨¡æ£€ç´¢å¢å¼ºæ¨¡å‹ç­‰ç»“æ„ï¼Œä¼ ç»Ÿçš„â€œdense è¡Œ/åˆ—åˆ‡åˆ†â€ä¸å†ç›´æ¥é€‚ç”¨ã€‚ç ”ç©¶å¦‚ä½•åœ¨è¿™äº›ç»“æ„ä¸­è®¾è®¡ TPï¼š\n\nå¯¹ MoEï¼Œå¯ä»¥åœ¨ä¸“å®¶ç»´åº¦å’Œ hidden ç»´åº¦ä¸ŠåŒæ—¶è€ƒè™‘åˆ‡åˆ†ä¸è´Ÿè½½å‡è¡¡ï¼›\nå¯¹ç¨€ç–æ³¨æ„åŠ›ï¼Œå¯ä»¥æ¢ç´¢â€œå—ç¨€ç– + TPâ€è”åˆè®¾è®¡ï¼Œä½¿åˆ†ç‰‡åçš„ attention ä»ç„¶é«˜æ•ˆã€‚\n\næ–¹å‘å››ï¼šé€šä¿¡ä¸è®¡ç®—çš„æ·±åº¦èåˆ\nMegatron-LM å·²ç»é€šè¿‡å¹¶è¡Œ cross-entropy å±•ç¤ºäº†â€œç®—å­ + é€šä¿¡â€èåˆçš„å¨åŠ›ã€‚è¿›ä¸€æ­¥å¯ä»¥ç ”ç©¶ï¼š\n\nå¦‚ä½•åœ¨ MLP/attention å†…éƒ¨å¼•å…¥ç®—å­çº§é€šä¿¡èåˆï¼Œå‡å°‘æ˜¾å¼ all-reduce è°ƒç”¨ï¼›\nåœ¨ç¼–è¯‘å™¨æˆ–å›¾ä¼˜åŒ–å±‚æ¬¡è‡ªåŠ¨è¯†åˆ«å’Œæ›¿æ¢â€œé€šä¿¡ + ç®—å­â€æ¨¡å¼ï¼›\nåœ¨ç¡¬ä»¶æ”¯æŒæ›´å¼ºé€šä¿¡ offload çš„æƒ…å†µä¸‹ï¼Œé‡æ–°è®¾è®¡ TP çš„é€šä¿¡è·¯å¾„ã€‚\n\næ–¹å‘äº”ï¼šä»ç»éªŒç°è±¡åˆ°ç†è®ºç†è§£\nLayerNorm ä½ç½®åœ¨å¤§æ¨¡å‹è®­ç»ƒä¸­çš„å½±å“ï¼Œç›®å‰åœ¨å®è·µä¸Šå·²ç»æœ‰å¾ˆå¤šå…±è¯†ï¼Œä½†ç†è®ºåˆ†æä»æ¯”è¾ƒç¼ºä¹ã€‚å¯ä»¥è€ƒè™‘ï¼š\n\nåŸºäºæ¢¯åº¦ä¼ æ’­å’Œæ®‹å·®ç½‘ç»œç†è®ºï¼Œåˆ†æä¸åŒ LN å¸ƒå±€å¯¹æ·±å±‚ç½‘ç»œç¨³å®šæ€§çš„å½±å“ï¼›\nç ”ç©¶åœ¨å¤§æ¨¡å‹ regime ä¸‹ï¼Œå“ªäº›æ¶æ„ä¿®æ”¹èƒ½ç³»ç»Ÿæ€§å‡å°‘æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ä¸è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼›\nå°†è¿™äº›ç†è®ºç»“æœåé¦ˆåˆ°æ–°æ¶æ„è®¾è®¡ä¸­ï¼Œå½¢æˆä¸€å¥—â€œä¸ºå¤§è§„æ¨¡è®­ç»ƒå‹å¥½â€çš„è®¾è®¡è§„èŒƒã€‚\n\n\nåã€çŸ¥è¯†å›¾è°±æ€ç»´é“¾\nä»ä¸€ä¸ªåâ€œå¤§æ¨¡å‹ç³»ç»Ÿå·¥ç¨‹ + å¹¶è¡Œè®­ç»ƒâ€çš„çŸ¥è¯†å›¾è°±è§†è§’æ¥çœ‹ï¼ŒMegatron-LM è¿™ç¯‡è®ºæ–‡ä¸»è¦å¯¹ä»¥ä¸‹å‡ ä¸ªæ¿å—æœ‰ç›´æ¥è¡¥é“¾æˆ–è¡¥å¼ºä½œç”¨ï¼š(arXiv)\n\nå¹¶è¡Œä¸è°ƒåº¦\n\næ˜ç¡®æå‡ºå¹¶éªŒè¯äº†å±‚å†…å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰çš„å…·ä½“æ‹†åˆ†æ–¹å¼ï¼›\nå°†â€œæ¨¡å‹å¹¶è¡Œç»„ + æ•°æ®å¹¶è¡Œç»„â€çš„ç»„ç»‡æ¨¡å¼å›ºåŒ–æˆä¸€ä¸ªå¯å¤ç”¨èŒƒå¼ã€‚\n\né€šä¿¡ä¸é›†ä½“æ“ä½œ\n\nå°†å±‚é—´é€šä¿¡â€œè§„æ•´åŒ–â€ä¸ºå›ºå®šæ¨¡å¼çš„ all-reduce / all-gatherï¼Œå‡å°‘äº† point-to-point é€šä¿¡ï¼›\nä¸ºåç»­é€šä¿¡ä¸è®¡ç®— overlapã€é€šä¿¡ç®—æ³•ä¼˜åŒ–æä¾›äº†ç¨³å®šçš„æ¨¡å¼åŸºç¡€ã€‚\n\nkernel ä¸ç®—å­ä¼˜åŒ–\n\né€šè¿‡çº¦æŸå¼ é‡åˆ‡åˆ†æ–¹å¼ï¼Œé—´æ¥ä¸ºå†™é«˜æ€§èƒ½ GEMM/attention/fused kernel åˆ›é€ äº†ç»Ÿä¸€çš„å½¢çŠ¶å‡è®¾ï¼›\nå¹¶è¡Œ cross-entropy æ˜¯â€œå°†æŸå¤±è®¡ç®—ä¸é€šä¿¡èåˆâ€çš„ç»å…¸ä¾‹å­ã€‚\n\næ¨¡å‹ç»“æ„ä¸æ¶æ„è®¾è®¡\n\nBERT å®éªŒé‡Œå¯¹ LayerNorm ä½ç½®çš„æ¢ç´¢ï¼Œç›´æ¥æ¨åŠ¨äº† Pre-LN Transformer çš„æ™®åŠï¼›\nè¯´æ˜åœ¨ multi-billion å‚æ•° regime ä¸‹ï¼Œç»“æ„ç»†èŠ‚ä¸å†æ˜¯â€œå° tweakâ€ï¼Œè€Œæ˜¯è®­ç»ƒæˆè´¥çš„å…³é”®å˜é‡ã€‚\n\nå†…å­˜ç®¡ç†ä¸æ˜¾å­˜ä¼˜åŒ–\n\né€šè¿‡ TP æŠŠæƒé‡ä¸æ¿€æ´»åˆ†æ‘Šåˆ°å¤šå¡ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ç§â€œç”¨å¹¶è¡Œæ¢æ˜¾å­˜â€çš„ç­–ç•¥ï¼›\nä¸åç»­ ZeROã€activation checkpointing ç­‰æŠ€æœ¯ç»„åˆåï¼Œæ„æˆå®Œæ•´çš„å¤§æ¨¡å‹æ˜¾å­˜è§£å†³æ–¹æ¡ˆã€‚\n\næ•°æ®ä¸é¢„å¤„ç†\n\nè®ºæ–‡è™½æœªè¿‡å¤šå±•å¼€æ•°æ®ç®¡çº¿ç»†èŠ‚ï¼Œä½†é€šè¿‡æ··åˆå¤šä»»åŠ¡ã€å¤šè¯­æ–™çš„å¤§è§„æ¨¡è®­ç»ƒï¼Œå±•ç¤ºäº†â€œç»Ÿä¸€å¤§æ¨¡å‹ + å¤šä»»åŠ¡è®­ç»ƒâ€çš„åŸºæœ¬èŒƒå¼ã€‚\n\n\n10.1 ä¸ªäººæ”¶è·ä¸åæ€\nå¯¹æˆ‘ä¸ªäººçš„â€œçŸ¥è¯†å›¾è°±æ€ç»´é“¾â€æ¥è¯´ï¼ŒMegatron-LM å¸¦æ¥çš„æœ€å¤§æ”¶è·æœ‰ä¸¤ç‚¹ï¼š\nç¬¬ä¸€ï¼Œå®ƒæŠŠâ€œå±‚å†…å¼ é‡å¹¶è¡Œâ€ä»ä¸€ä¸ªæ¨¡ç³Šæ¦‚å¿µï¼Œå˜æˆäº†å¯ä»¥ç›´æ¥ç¼–ç çš„è®¾è®¡æ¨¡å¼ï¼š\n\nçº¿æ€§å±‚åªåšä¸¤ç§æ‹†åˆ†ï¼šæŒ‰åˆ—åˆ‡æˆ–æŒ‰è¡Œåˆ‡ï¼›æ³¨æ„åŠ›æŒ‰ head åˆ‡ï¼›åœ¨å°‘æ•°å‡ å¤„åš all-reduce/all-gatherã€‚\n\nè¿™è®©å¤§æ¨¡å‹å¹¶è¡Œè®­ç»ƒä»â€œä¸€å † ad-hoc trickâ€å˜æˆäº†ä¸€å¥—ç›¸å¯¹è§„èŒƒã€å¯å¤ç”¨çš„å·¥ç¨‹æ¨¡æ¿ã€‚\nç¬¬äºŒï¼Œå®ƒè®©æˆ‘æ›´æ¸…æ¥šåœ°çœ‹åˆ°ï¼šç³»ç»Ÿè®¾è®¡å’Œæ¨¡å‹è®¾è®¡åœ¨å¤§æ¨¡å‹ regime ä¸‹æ˜¯é«˜åº¦è€¦åˆçš„ã€‚ å¼ é‡å¹¶è¡Œçš„ç»“æ„å†³å®šäº†é€šä¿¡æ¨¡å¼å’Œ kernel å½¢çŠ¶ï¼ŒLayerNorm çš„ä½ç½®å†³å®šäº†åœ¨è¿™ä¸ª regime ä¸‹èƒ½ä¸èƒ½è®­ç¨³ï¼›ä»»ä½•ä¸€ç«¯çš„è®¾è®¡ï¼Œéƒ½ä¸èƒ½è„±ç¦»å¦ä¸€ç«¯å­¤ç«‹åœ°çœ‹å¾…ã€‚\n\næ•´ä½“è¯„ä»·ï¼š ä»å¤§æ¨¡å‹è®­ç»ƒçš„å†å²çº¿ä¸Šçœ‹ï¼Œã€ŠMegatron-LMã€‹æ˜¯æŠŠâ€œå¤šäº¿åˆ°å‡ åäº¿å‚æ•°çš„ Transformer è®­ç»ƒâ€ä»æ¦‚å¿µå˜æˆå·¥ç¨‹ç°å®çš„ä¸€å—å…³é”®æ‹¼å›¾ã€‚å®ƒç”¨æå…¶ç®€å•ã€è§„åˆ™çš„å¼ é‡å¹¶è¡Œè®¾è®¡ï¼Œç»™å‡ºäº†ä¸€ä¸ªå¯ä»¥è¢«å¹¿æ³›ç»§æ‰¿çš„åŸºç¡€èŒƒå¼ï¼Œä¹Ÿä¸ºåç»­ 3D/4D å¹¶è¡Œå’Œ trillion çº§æ¨¡å‹è®­ç»ƒæä¾›äº†å‡ºå‘ç‚¹ã€‚å¦‚æœä½ åœ¨æ­å»ºæˆ–ç†è§£ç°ä»£å¤§æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼ŒMegatron-LM ä»ç„¶æ˜¯ä¸€ç¯‡å€¼å¾—å®Œæ•´é˜…è¯»å’Œç»†å“çš„åŸºç¡€è®ºæ–‡ã€‚\n\n","categories":["è®ºæ–‡é˜…è¯»"],"tags":["paper"]},{"title":"ubuntuå¸¸è§shellå‘½ä»¤","url":"/2025/08/17/other/shell/","content":"\n\n1. ç£ç›˜å ç”¨ä¸æ’åºï¼ˆdu/sortï¼‰\nå¸¸ç”¨å†™æ³•\n# æŒ‰â€œå½“å‰ç›®å½•çš„ç›´æ¥å­é¡¹â€æ±‡æ€»ï¼ˆäººç±»å¯è¯»ï¼‰ï¼Œå¹¶æŒ‰å¤§å°å€’åºdu -h --max-depth=1 . | sort -hr# ä»…ç»Ÿè®¡æ¯ä¸ªæ¡ç›®æ€»å¤§å°ï¼ˆä¸æ˜¾ç¤ºå­å±‚çº§ï¼‰ï¼Œå¹¶å¯¹æ¡ç›®æ’åºdu -sh -- * | sort -h\n2. æ–‡æœ¬æœç´¢ï¼ˆgrepï¼‰\nåŸºç¡€\ngrep &quot;keyword&quot; file.txt          # åœ¨å•ä¸ªæ–‡ä»¶ä¸­æŸ¥æ‰¾grep -n &quot;keyword&quot; file.txt       # æ˜¾ç¤ºè¡Œå·grep -i &quot;keyword&quot; file.txt       # å¿½ç•¥å¤§å°å†™\nç›®å½•é€’å½’ä¸ä¸Šä¸‹æ–‡\ngrep -rin --color=auto &quot;keyword&quot; .      # é€’å½’ã€å¿½ç•¥å¤§å°å†™ã€è¡Œå·ã€é«˜äº®grep -nC 3 &quot;keyword&quot; file.txt           # ä¸Šä¸‹å„ 3 è¡Œgrep -nA 2 &quot;keyword&quot; file.txt           # å 2 è¡Œgrep -nB 2 &quot;keyword&quot; file.txt           # å‰ 2 è¡Œ\nç²¾ç¡®åŒ¹é…ä¸æ­£åˆ™\ngrep -rw &quot;\\&lt;token\\&gt;&quot; .                  # æŒ‰â€œæ•´è¯â€åŒ¹é…grep -E &quot;err(or)?|fail(ed)?&quot; app.log    # æ‰©å±•æ­£åˆ™grep -rF &quot;literal*text&quot; .               # çº¯å­—ç¬¦ä¸²ï¼ˆä¸å½“æ­£åˆ™ï¼‰ï¼Œæ›´å¿«\næ’é™¤æ–‡ä»¶/ç›®å½•\ngrep -rin &quot;keyword&quot; . \\  --exclude-dir=&#123;.git,node_modules,dist&#125; \\  --exclude=&quot;*.min.js&quot;\n\n3. æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾ï¼ˆfind/locateï¼‰\nfindï¼šçµæ´»ä½†å®æ—¶æ‰«æï¼ˆæ…¢ï¼‰\n# æŒ‰æ–‡ä»¶åï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰find /path -type f -iname &quot;*name*&quot;# é™åˆ¶æœç´¢æ·±åº¦find . -maxdepth 2 -type d -name &quot;build&quot;# æŸ¥æ‰¾å¤§æ–‡ä»¶ï¼ˆ&gt; 100MBï¼‰å¹¶æŒ‰å¤§å°é™åºåˆ—å‡ºå‰ 20 ä¸ªfind /var -type f -size +100M -printf &#x27;%s\\t%p\\n&#x27; | sort -nr | head -20# æŸ¥æ‰¾æœ€è¿‘ 1 å¤©å†…ä¿®æ”¹çš„æ–‡ä»¶find . -type f -mtime -1# å¯¹ç»“æœæ‰§è¡Œå‘½ä»¤ï¼ˆå®‰å…¨å¤„ç†ç©ºæ ¼ï¼‰find . -type f -name &quot;*.log&quot; -print0 | xargs -0 gzip\n\nè·³è¿‡ç³»ç»Ÿç›®å½•ä¸”å‹åˆ¶æŠ¥é”™\n\nfind / \\( -path /proc -o -path /sys -o -path /run \\) -prune -o \\  -type f -name &quot;*.conf&quot; -print 2&gt;/dev/null\nlocate/plocateï¼šåŸºäºç´¢å¼•ï¼ˆå¿«ï¼‰\nsudo apt-get install -y plocatesudo updatedb                 # é€šå¸¸è‡ªåŠ¨å®šæ—¶æ›´æ–°locate filename_or_pattern\n\n4. å¸¸è§ç½‘ç»œå·¥å…·å®‰è£…åŒ…\n# pingsudo apt-get install -y iputils-ping# ifconfigï¼ˆè€å·¥å…·ï¼Œä»å¸¸è§ï¼‰sudo apt-get install -y net-tools# ç°ä»£æ›¿ä»£ï¼šipï¼ˆé€šå¸¸å·²è‡ªå¸¦äº iproute2ï¼‰ip addrip linkip route# killallsudo apt-get install -y psmisc\n\n5. è¿›ç¨‹æŸ¥æ€ï¼ˆkill/pkill/killallï¼‰\nps -ef | grep python3 | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9\næ›´å®‰å…¨çš„åšæ³•\n# ä¼˜é›…ç»ˆæ­¢ï¼ˆSIGTERMï¼‰ï¼›æ—  PID æ—¶ä¸æ‰§è¡Œ (-r)pgrep -f python3 | xargs -r kill# ç›´æ¥æŒ‰åç§°åŒ¹é…ï¼ˆä¼˜é›…ç»ˆæ­¢ï¼‰ï¼Œå¿…è¦æ—¶å† -9pkill -f python3pkill -9 -f python3# é¿å…åŒ¹é…åˆ° grep è‡ªèº«ps -ef | grep &#x27;[p]ython3&#x27; | awk &#x27;&#123;print $2&#125;&#x27; | xargs -r kill\n\nå»ºè®®å…ˆå°è¯• SIGTERMï¼ˆé»˜è®¤ï¼‰ï¼Œæ— å“åº”å†ç”¨ SIGKILLï¼ˆ-9ï¼‰ã€‚\n\n\n6. é«˜é¢‘å‘½ä»¤æ¸…å•ä¸ç¤ºä¾‹\nç³»ç»Ÿ/èµ„æº\ntop                     # å®æ—¶æ¦‚è§ˆhtop                    # æ›´å‹å¥½ï¼ˆéœ€ï¼šsudo apt-get install -y htopï¼‰free -h                 # å†…å­˜df -h                   # ç£ç›˜åˆ†åŒºå®¹é‡du -sh * | sort -h      # ç›®å½•å ç”¨uname -a                # å†…æ ¸ä¿¡æ¯lsb_release -a          # å‘è¡Œç‰ˆä¿¡æ¯\nè¿›ç¨‹/ç½‘ç»œ\nps aux | lesspstree -p               # è¿›ç¨‹æ ‘ï¼ˆéœ€ï¼šsudo apt-get install -y psmiscï¼‰lsof -i :8080           # ç«¯å£å ç”¨ï¼ˆéœ€ï¼šsudo apt-get install -y lsofï¼‰ss -lntp                # ç›‘å¬ç«¯å£ + è¿›ç¨‹\næ–‡æœ¬/æ—¥å¿—\nless file.logtail -f file.logwc -l file.txtsort file | uniq -c | sort -nrcut -d&#x27;,&#x27; -f1,3 file.csvsed -n &#x27;1,20p&#x27; file.txtawk -F: &#x27;&#123;print $1,$3&#125;&#x27; /etc/passwd\næ–‡ä»¶/å½’æ¡£/ä¼ è¾“\ntar -czf logs.tgz logs/        # å‹ç¼©tar -xzf logs.tgz              # è§£å‹zip -r src.zip src/            # zipï¼ˆéœ€ï¼šsudo apt-get install -y zip unzipï¼‰rsync -av --progress src/ dst/scp file user@host:/path/\næƒé™/é“¾æ¥\nchmod +x run.shchown user:group fileln -s /real/path link_name\næœåŠ¡ä¸æ—¥å¿—ï¼ˆsystemdï¼‰\nsystemctl status nginxsudo systemctl start nginxjournalctl -u nginx --since &quot;1 hour ago&quot;\nå…¶ä»–\nwhich python3command -v nodedate &quot;+%F %T&quot;nohup python3 app.py &gt;out.log 2&gt;&amp;1 &amp;tmux new -s work              # ç»ˆç«¯å¤ç”¨ï¼ˆéœ€ï¼šsudo apt-get install -y tmuxï¼‰\n\n7. å°è´´å£«ä¸å¸¸è§å‘\n\néšè—æ–‡ä»¶ï¼š* ä¸åŒ¹é…éšè—é¡¹ï¼Œå¯ç”¨ .* * ç»„åˆæˆ–å¼€å¯ dotglobã€‚\né˜²æ­¢å‚æ•°è¢«å½“ä½œé€‰é¡¹ï¼šå½“æ–‡ä»¶åä»¥ - å¼€å¤´æ—¶åŠ  --ï¼Œå¦‚ rm -- -weirdfileã€‚\nxargs å®‰å…¨ï¼šäºŒè¿›åˆ¶æ–‡ä»¶/ç©ºæ ¼ç”¨ -0 é…åˆ -print0ï¼›æ— ç»“æœæ—¶ä¸æ‰§è¡Œç”¨ -rã€‚\nä¼˜é›…åœæœåŠ¡ä¼˜å…ˆï¼škill -TERM â†’ ä¸è¡Œå† kill -KILLã€‚\næƒé™ï¼šç³»ç»Ÿç›®å½•æ“ä½œæ…ç”¨ sudoï¼Œå†™å‰å…ˆ ls/du/stat ç¡®è®¤ã€‚\ngrep æ­£åˆ™ vs å­—ç¬¦ä¸²ï¼šçº¯æ–‡æœ¬åŒ¹é…æ›´ç¨³æ›´å¿«ç”¨ -Fã€‚\nfind æ€§èƒ½ï¼šå¤§ç›®å½•ç”¨ -maxdepth é™åˆ¶å±‚çº§æˆ–æ”¹ç”¨ locate/plocateã€‚\n\n\n","categories":["å…¶å®ƒ"],"tags":["shell"]},{"title":"token ç®€ä»‹","url":"/2025/09/07/other/token/","content":"\n\nğŸ§  ä»€ä¹ˆæ˜¯ Tokenï¼Ÿ\nåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼ŒToken æ˜¯æ–‡æœ¬çš„åŸºæœ¬å•ä½ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ã€ä¸€ä¸ªè¯ã€ä¸€ä¸ªå­è¯ï¼Œç”šè‡³æ˜¯ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·ã€‚Token çš„å®šä¹‰å–å†³äºæ‰€é‡‡ç”¨çš„æ ‡è®°åŒ–ï¼ˆtokenizationï¼‰æ–¹æ³•ã€‚\n\nğŸ”„ æ–‡æœ¬å¦‚ä½•è½¬æ¢ä¸ºæ•°å­—ï¼Ÿ\nåœ¨è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶ï¼Œæ–‡æœ¬éœ€è¦è¢«è½¬æ¢ä¸ºæ•°å­—å½¢å¼ã€‚è¿™ä¸€è¿‡ç¨‹é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š\n\næ ‡è®°åŒ–ï¼ˆTokenizationï¼‰ï¼šå°†æ–‡æœ¬åˆ†è§£ä¸º tokensã€‚\næ„å»ºè¯æ±‡è¡¨ï¼ˆVocabularyï¼‰ï¼šä¸ºæ¯ä¸ª token åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•°å­— IDã€‚\næ•°å­—åŒ–ï¼ˆNumericalizationï¼‰ï¼šå°†æ–‡æœ¬ä¸­çš„ tokens æ›¿æ¢ä¸ºå¯¹åº”çš„æ•°å­— IDã€‚\n\nä¾‹å¦‚ï¼Œå¥å­ \"hello world\" å¯èƒ½è¢«æ ‡è®°åŒ–ä¸º [\"hello\", \"world\"]ï¼Œç„¶åæ ¹æ®è¯æ±‡è¡¨è½¬æ¢ä¸º [1, 2]ã€‚\n\nğŸ”¤ å¸¸è§çš„æ ‡è®°åŒ–æ–¹æ³•\n1. Word-based Tokenizationï¼ˆåŸºäºè¯çš„æ ‡è®°åŒ–ï¼‰\nå°†æ–‡æœ¬æŒ‰ç©ºæ ¼æˆ–æ ‡ç‚¹ç¬¦å·åˆ†å‰²æˆå•è¯ã€‚è¿™ç§æ–¹æ³•ç®€å•ç›´è§‚ï¼Œä½†å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š\n\nè¯æ±‡è¡¨è¿‡å¤§ï¼šéœ€è¦ä¸ºæ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ IDï¼Œå¯¼è‡´è¯æ±‡è¡¨åºå¤§ã€‚\nå¤„ç†æœªç™»å½•è¯å›°éš¾ï¼šå¯¹äºè®­ç»ƒæ•°æ®ä¸­æœªå‡ºç°çš„å•è¯ï¼Œæ¨¡å‹éš¾ä»¥å¤„ç†ã€‚\n\nç¤ºä¾‹ï¼š\n\nè¾“å…¥æ–‡æœ¬ï¼š\"I love NLP\"\næ ‡è®°åŒ–ç»“æœï¼š[\"I\", \"love\", \"NLP\"]\næ•°å­—åŒ–è¡¨ç¤ºï¼š[1, 2, 3]\n\n2. Character-based Tokenizationï¼ˆåŸºäºå­—ç¬¦çš„æ ‡è®°åŒ–ï¼‰\nå°†æ–‡æœ¬åˆ†è§£ä¸ºå•ä¸ªå­—ç¬¦ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡å°‘è¯æ±‡è¡¨å¤§å°ï¼Œä½†å¯èƒ½å¯¼è‡´ä»¥ä¸‹é—®é¢˜ï¼š\n\nä¿¡æ¯ä¸¢å¤±ï¼šå­—ç¬¦çº§åˆ«çš„è¡¨ç¤ºå¯èƒ½æ— æ³•æ•æ‰åˆ°è¯æ±‡çš„å®Œæ•´è¯­ä¹‰ã€‚\nåºåˆ—é•¿åº¦å¢åŠ ï¼šåŒä¸€æ–‡æœ¬çš„ token æ•°é‡å¢åŠ ï¼Œå¯èƒ½å½±å“æ¨¡å‹çš„å¤„ç†æ•ˆç‡ã€‚\n\nç¤ºä¾‹ï¼š\n\nè¾“å…¥æ–‡æœ¬ï¼š\"I love NLP\"\næ ‡è®°åŒ–ç»“æœï¼š[\"I\", \" \", \"l\", \"o\", \"v\", \"e\", \" \", \"N\", \"L\", \"P\"]\næ•°å­—åŒ–è¡¨ç¤ºï¼š[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nğŸ”¬ Subword-based Tokenizationï¼šBPEï¼ˆå­—èŠ‚å¯¹ç¼–ç ï¼‰\nBPE æ˜¯ä¸€ç§å­è¯çº§åˆ«çš„æ ‡è®°åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡è¯æ±‡è¡¨å¤§å°å’Œè¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ã€‚å®ƒé€šè¿‡è¿­ä»£åœ°åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹æ¥æ„å»ºå­è¯å•å…ƒï¼Œå¹¶è¢«å¹¿æ³›åº”ç”¨äº GPTã€BERT ç­‰å¤§è¯­è¨€æ¨¡å‹ã€‚BPE çš„è¿‡ç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹äº”ä¸ªé˜¶æ®µï¼š\n1. åˆå§‹åŒ–è¯æ±‡è¡¨\n\næ‹†åˆ†å­—ç¬¦ï¼šé¦–å…ˆå°†è¯­æ–™åº“æ‹†åˆ†ä¸ºæœ€å°å•ä½â€”â€”å•ä¸ªå­—ç¬¦ã€‚ä¾‹å¦‚ï¼Œå¯¹å•è¯ lower æ‹†åˆ†å¾—åˆ° l o w e r&lt;/w&gt;ï¼Œå¹¶åœ¨æ¯ä¸ªè¯å°¾æ·»åŠ ç‰¹æ®Šç»“æŸç¬¦ï¼ˆå¦‚ &lt;/w&gt;ï¼‰ï¼Œä»¥åŒºåˆ†ä¸åŒè¯ã€‚\næ„å»ºåˆå§‹è¯è¡¨ï¼šè®°å½•æ‰€æœ‰å‡ºç°è¿‡çš„å­—ç¬¦ï¼Œä½œä¸ºåˆå§‹ token é›†ã€‚\n\n2. ç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹é¢‘ç‡\néå†æ‰€æœ‰è¯ï¼Œç»Ÿè®¡æ¯ä¸ªç›¸é‚»å­—ç¬¦ï¼ˆæˆ–å·²åˆå¹¶çš„å­è¯ï¼‰å¯¹çš„å‡ºç°æ¬¡æ•°ã€‚BPE é€šè¿‡è¿™ä¸€ç»Ÿè®¡æ¥è¯†åˆ«è¯­è¨€ä¸­æœ€å¸¸è§çš„æ¨¡å¼ï¼Œä»¥å†³å®šæ¥ä¸‹æ¥è¦åˆå¹¶çš„ç¬¦å·å¯¹ã€‚\n3. åˆå¹¶æœ€é¢‘ç¹çš„ç¬¦å·å¯¹\næ‰¾åˆ°å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ï¼Œå¹¶å°†å®ƒä»¬åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­åˆå¹¶æˆæ–°çš„å­è¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ (w, e) æ˜¯æœ€é«˜é¢‘ç»„åˆï¼Œåˆ™å°†å…¶åˆå¹¶ä¸º weï¼›æ›´æ–°æ‰€æœ‰ç›¸å…³è¯ï¼Œå¹¶æŠŠæ–°å­è¯åŠ å…¥è¯æ±‡è¡¨ã€‚\n4. é‡å¤æ­¥éª¤ç›´åˆ°è¾¾åˆ°è¯è¡¨å¤§å°\nBPE æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ã€‚æ¯æ¬¡åˆå¹¶åï¼Œç»Ÿè®¡æ–°çš„ç›¸é‚»ç¬¦å·å¯¹å¹¶ç»§ç»­åˆå¹¶ï¼Œç›´åˆ°è¯æ±‡è¡¨è¾¾åˆ°é¢„è®¾å¤§å°æˆ–ä¸å†æœ‰éœ€è¦åˆå¹¶çš„ç¬¦å·å¯¹ã€‚\n5. æ„å»ºæœ€ç»ˆè¯æ±‡è¡¨å¹¶åº”ç”¨åˆ°æ–‡æœ¬\næœ€ç»ˆè¯æ±‡è¡¨æ—¢åŒ…å«åˆå§‹çš„å­—ç¬¦ï¼ŒåˆåŒ…å«æ‰€æœ‰åˆå¹¶å¾—åˆ°çš„é«˜é¢‘å­è¯ã€‚æ–°è¯å¯ä»¥é€šè¿‡è¿™äº›å­è¯ç»„åˆè¡¨ç¤ºï¼Œå› æ­¤ä»»ä½•æ–°è¯éƒ½èƒ½æ‹†è§£ä¸ºå·²çŸ¥çš„å­è¯åºåˆ—ã€‚\nç¤ºä¾‹ï¼šBPE åˆ†è¯è¿‡ç¨‹æ¼”ç¤º\nä»¥ä»¥ä¸‹è¯æ±‡é›†ä¸ºä¾‹ï¼šhuggingfaceã€huggingã€faceã€hugã€huggerã€learningã€learnerã€learnã€‚å°†æ¯ä¸ªè¯æ‹†åˆ†ä¸ºå­—ç¬¦å¹¶åŠ ä¸Šç»“æŸç¬¦ï¼Œç„¶åæ‰§è¡Œé¢‘æ¬¡ç»Ÿè®¡å’Œåˆå¹¶ã€‚ä»¥ä¸‹æ˜¯å‰å‡ æ¬¡åˆå¹¶ï¼š\n\n(h, u) â†’ hu\n(hu, g) â†’ hug\n(hug, g) â†’ hugg\n(i, n) â†’ in\n(in, g) â†’ ing\n(l, e) â†’ le\n(le, a) â†’ lea\n(lea, r) â†’ lear\n\næ‰§è¡Œ 8 æ¬¡åˆå¹¶åï¼Œè¯è¡¨æ‰©å¤§è‡³ 20 ä¸ª tokenï¼Œå…¶ä¸­åŒ…æ‹¬åŸºæœ¬å­—ç¬¦ï¼ˆh,u,g,i ç­‰ï¼‰å’Œæ–°åˆæˆçš„å­è¯ï¼ˆhug,hugg,ing,lear ç­‰ï¼‰ã€‚è¯ huggingface ç»è¿‡æ ‡è®°åŒ–åä¸º hugg ing f a c e &lt;/w&gt;ï¼Œlearning åˆ™ä¸º lear n ing &lt;/w&gt;ï¼Œå…¶ä½™è¯ä¹Ÿå¯ä»¥ç”¨ç±»ä¼¼æ–¹å¼è¡¨ç¤ºã€‚\nğŸ§ª BPE çš„ä¼˜ç¼ºç‚¹\nä¼˜ç‚¹ï¼š\n\nå¤„ç†æœªç™»å½•è¯ï¼šé€šè¿‡æŠŠè¯æ‹†è§£ä¸ºå­è¯ï¼ŒBPE å¯ä»¥ç”¨å·²æœ‰çš„å­è¯ç»„åˆæ¥è¡¨ç¤ºè®­ç»ƒé›†ä¸­æœªå‡ºç°çš„è¯æ±‡ã€‚\nå‡å°‘è¯æ±‡è¡¨å¤§å°ï¼šç›¸æ¯”åŸºäºè¯çš„æ ‡è®°åŒ–ï¼ŒBPE å¯ä»¥æ˜¾è‘—ç¼©å°è¯è¡¨è§„æ¨¡ï¼Œä½¿æ¨¡å‹æ›´é«˜æ•ˆã€‚\næå‡æ³›åŒ–èƒ½åŠ›ï¼šå­è¯çº§è¡¨ç¤ºå…è®¸æ¨¡å‹å­¦ä¹ æ›´ç»†ç²’åº¦çš„è¯­è¨€ç»“æ„ï¼Œå¯¹ä¸åŒé¢†åŸŸã€ä¸åŒè¯­è¨€å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚\n\nç¼ºç‚¹ï¼š\n\nåˆå¹¶è§„åˆ™ä¾èµ–è¯­æ–™ï¼šä¸åŒè¯­æ–™å¾—åˆ°çš„åˆå¹¶è§„åˆ™å·®å¼‚è¾ƒå¤§ï¼Œå¤šè¯­è¨€åœºæ™¯ä¸‹å¯èƒ½éœ€è¦å¤æ‚çš„å¤„ç†ã€‚\nè¯­ä¹‰å®Œæ•´æ€§å¯èƒ½å—æŸï¼šå¦‚æœåˆå¹¶è¿‡åº¦ï¼ŒæŸäº›åˆæˆè¯çš„è¯­ä¹‰ä»å¯èƒ½åˆ†å‰²ï¼Œéœ€æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„è¯è¡¨å¤§å°ã€‚\n\n\nğŸ“ æ€»ç»“\næ ‡è®°åŒ–æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„æ ¸å¿ƒæ­¥éª¤ï¼Œå®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å­—å½¢å¼ã€‚åŸºäºè¯å’ŒåŸºäºå­—ç¬¦çš„æ ‡è®°åŒ–æ–¹æ³•ç®€å•æ˜“ç†è§£ï¼Œä½†åˆ†åˆ«å­˜åœ¨è¯æ±‡è¡¨è¿‡å¤§å’Œåºåˆ—è¿‡é•¿çš„é—®é¢˜ã€‚BPE ä½œä¸ºä¸€ç§å­è¯çº§æ ‡è®°åŒ–ç®—æ³•ï¼Œä»¥åˆå§‹åŒ–å­—ç¬¦é›†ä¸ºåŸºç¡€ï¼Œé€šè¿‡è¿­ä»£åˆå¹¶é«˜é¢‘ç¬¦å·å¯¹æ„å»ºæ–°çš„å­è¯å•å…ƒã€‚è¿™ç§æ–¹æ³•å…¼é¡¾äº†è¯è¡¨å¤§å°å’Œè¯­ä¹‰è¡¨è¾¾èƒ½åŠ›ï¼Œæ—¢èƒ½å¤„ç†æœªç™»å½•è¯ï¼Œä¹Ÿèƒ½ä¿ç•™è¶³å¤Ÿçš„è¯­ä¹‰ä¿¡æ¯ã€‚å› æ­¤ï¼Œç°ä»£å¤§è¯­è¨€æ¨¡å‹é€šå¸¸é‡‡ç”¨ BPE æˆ–å…¶å˜ç§ï¼ˆå¦‚ WordPieceã€SentencePieceï¼‰ä½œä¸ºé»˜è®¤çš„æ ‡è®°åŒ–æ–¹æ¡ˆã€‚\n","categories":["å…¶å®ƒ"],"tags":["token"]},{"title":"ubuntuæ­å»ºæŠ€æœ¯åšå®¢æŒ‡å—","url":"/2025/06/14/other/web_init/","content":"\n\n1. å®‰è£… Hexo ç¯å¢ƒ\n2. é€‰æ‹©ä¸é…ç½® Hexo ä¸»é¢˜\n3. æ’°å†™ä¸ç®¡ç†åšå®¢å†…å®¹\n4. SEO ä¼˜åŒ–\n5. åšå®¢éƒ¨ç½²\n6. ç»´æŠ¤ä¸ä¼˜åŒ–\n\næœ¬æŒ‡å—è¯¦ç»†ä»‹ç»äº†å¦‚ä½•åœ¨ Ubuntu æœåŠ¡å™¨ä¸Šæ­å»ºå¹¶éƒ¨ç½²ä¸€ä¸ª Hexo æŠ€æœ¯åšå®¢ï¼ŒåŒ…æ‹¬ä»ç¯å¢ƒå®‰è£…åˆ°åæœŸç»´æŠ¤çš„å®Œæ•´æ­¥éª¤ã€‚\n1. å®‰è£… Hexo ç¯å¢ƒ\næ­å»º Hexo åšå®¢é¦–å…ˆéœ€è¦å®‰è£… Node.jsï¼ˆHexo åŸºäº Node.jsï¼‰ã€npmã€Git ä»¥åŠ Hexo CLI å·¥å…·ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤é…ç½®ç¯å¢ƒï¼š\nå®‰è£… Node.js å’Œ npmï¼š\nåœ¨ Ubuntu ä¸Šï¼Œé€šè¿‡åŒ…ç®¡ç†å™¨æˆ– Node å®˜æ–¹ä»“åº“å®‰è£… Node.jsã€‚å»ºè®®å®‰è£… LTS ç‰ˆæœ¬ï¼ˆå¦‚ Node 14+ï¼‰ã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ·»åŠ  NodeSource ä»“åº“å¹¶å®‰è£… Node.jsï¼š\ncurl -sL https://deb.nodesource.com/setup_18.x | sudo -E bash -sudo apt-get install -y nodejs\nå®‰è£…å®Œæˆåï¼Œæ£€æŸ¥ç‰ˆæœ¬ä»¥ç¡®ä¿ Node æ­£å¸¸å¯ç”¨ï¼š\nnode -v  # åº”è¿”å›ç±»ä¼¼ v18.20.6 çš„ç‰ˆæœ¬å·npm -v   # éªŒè¯ npm æ˜¯å¦æ­£å¸¸å®‰è£…\nå®‰è£… Gitï¼š\nGit æ˜¯ Hexo éƒ¨ç½²å’Œå¤‡ä»½çš„å¸¸ç”¨å·¥å…·ã€‚Ubuntu é€šå¸¸é¢„è£… Gitï¼Œè‹¥æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œï¼š\nsudo apt-get install -y git\nå®‰è£…åï¼Œé…ç½® Git çš„å…¨å±€ç”¨æˆ·åå’Œé‚®ç®±ï¼š\ngit config --global user.name &quot;Your Name&quot;git config --global user.email &quot;youremail@example.com&quot;\nå®‰è£… Hexo CLIï¼š\né€šè¿‡ npm å…¨å±€å®‰è£… Hexo CLIï¼š\nsudo npm install -g hexo-cli\nå®‰è£…æˆåŠŸåï¼Œé€šè¿‡ hexo -v æ£€æŸ¥ç‰ˆæœ¬ï¼Œç¡®ä¿ Hexo CLI å¯ç”¨ã€‚\nåˆå§‹åŒ– Hexo åšå®¢ï¼š\né€‰æ‹©åšå®¢æ–‡ä»¶å¤¹ï¼ˆä¾‹å¦‚ /var/www/hexo æˆ–å½“å‰ç”¨æˆ·ä¸»ç›®å½•ä¸‹çš„ my-blog æ–‡ä»¶å¤¹ï¼‰ï¼Œå¹¶åœ¨è¯¥ç›®å½•ä¸‹åˆå§‹åŒ– Hexo åšå®¢ï¼š\nsudo mkdir -p /var/www/hexo &amp;&amp; sudo chown $USER:$USER /var/www/hexocd /var/www/hexohexo initnpm install\nåˆå§‹åŒ–å®Œæˆåï¼ŒHexo ä¼šç”Ÿæˆé»˜è®¤çš„åšå®¢ç»“æ„ï¼ŒåŒ…æ‹¬ _config.yml é…ç½®æ–‡ä»¶ã€scaffolds/ æ¨¡æ¿ç›®å½•ã€source/ å†…å®¹ç›®å½•å’Œ themes/ ä¸»é¢˜ç›®å½•ç­‰ã€‚å¯ä»¥é€šè¿‡è¿è¡Œ hexo server é¢„è§ˆæœ¬åœ°åšå®¢ã€‚\nå¼€å¯é˜²ç«å¢™ï¼š\nä¸ºäº†ç¡®ä¿æœåŠ¡å™¨å®‰å…¨ï¼Œå»ºè®®å¼€å¯é˜²ç«å¢™ã€‚Ubuntu è‡ªå¸¦ UFW é˜²ç«å¢™ï¼Œå¯ä»¥å¼€å¯ SSHã€HTTP(S) ä»¥åŠ Hexo é»˜è®¤é¢„è§ˆç«¯å£ 4000ï¼š\nsudo apt-get install ufw  sudo ufw allow &quot;OpenSSH&quot;  sudo ufw allow 4000  sudo ufw allow http  sudo ufw allow https  sudo ufw enable\n2. é€‰æ‹©ä¸é…ç½® Hexo ä¸»é¢˜\nHexo é»˜è®¤ä¸»é¢˜ä¸º Landscapeï¼Œä½†ä¸ºäº†æ‰“é€ ä¸€ä¸ªç®€æ´ç¾è§‚çš„æŠ€æœ¯åšå®¢ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ NexT ä¸»é¢˜ï¼Œå®ƒåŠŸèƒ½å¼ºå¤§ä¸”å¤–è§‚ä¼˜é›…ã€‚ä»¥ä¸‹æ˜¯ä¸»é¢˜çš„å®‰è£…å’Œé…ç½®æ­¥éª¤ï¼š\nè·å– NexT ä¸»é¢˜ï¼š\nåœ¨ Hexo åšå®¢æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥å…‹éš† NexT ä¸»é¢˜ï¼š\ncd /var/www/hexogit clone https://github.com/theme-next/hexo-theme-next themes/next\nä¿®æ”¹ä¸»é¢˜é…ç½®ï¼š\nå…‹éš†å®Œæˆåï¼Œæ‰“å¼€ _config.yml é…ç½®æ–‡ä»¶ï¼Œå°† theme é…ç½®ä»é»˜è®¤çš„ landscape æ”¹ä¸º nextï¼š\n# _config.ymltheme: next\nå®‰è£…ä¸»é¢˜ä¾èµ–ï¼š\næ ¹æ®éœ€è¦å®‰è£… NexT ä¸»é¢˜çš„ä¾èµ–ï¼Œå¹¶å¯ç”¨ä½ æ‰€éœ€çš„åŠŸèƒ½ã€‚\nç”Ÿæˆå¸¸ç”¨é¡µé¢ï¼š\nä¸ºäº†å®Œå–„ç½‘ç«™ç»“æ„ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆæ ‡ç­¾ã€åˆ†ç±»ã€å½’æ¡£ç­‰é¡µé¢ï¼š\nhexo new page &quot;tags&quot;hexo new page &quot;categories&quot;hexo new page &quot;archives&quot;hexo new page &quot;about&quot;\nç¼–è¾‘æ¯ä¸ªé¡µé¢çš„ index.mdï¼Œåœ¨ Front-matter ä¸­æŒ‡å®šé¡µé¢ç±»å‹ï¼š\ntitle: æ ‡ç­¾date: 2025-03-06 15:00:00type: &quot;tags&quot;\nå¯¼èˆªæ èœå•å®šåˆ¶ï¼š\nåœ¨ themes/next/_config.yml ä¸­æ‰¾åˆ° menu è®¾ç½®ï¼Œå¹¶æ·»åŠ æ–°åˆ›å»ºçš„é¡µé¢ï¼š\nmenu:  home: / || home  categories: /categories/ || th  tags: /tags/ || tags  archives: /archives/ || archive  about: /about/ || user\nä¿å­˜ä¿®æ”¹åï¼Œé‡æ–°ç”Ÿæˆç«™ç‚¹ï¼Œæ–°çš„å¯¼èˆªæ èœå•å³ä¼šæ˜¾ç¤ºã€‚\n3. æ’°å†™ä¸ç®¡ç†åšå®¢å†…å®¹\nHexo ä½¿ç”¨ Markdown æ ¼å¼æ¥æ’°å†™æ–‡ç« ï¼Œéå¸¸é€‚åˆæŠ€æœ¯åšå®¢ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ç®¡ç†å’Œç¼–å†™æ–‡ç« çš„æ­¥éª¤ï¼š\næ–°å»ºåšæ–‡ï¼š\nä½¿ç”¨ Hexo CLI åˆ›å»ºæ–°çš„æ–‡ç« ï¼š\nhexo new &quot;æ–‡ç« æ ‡é¢˜&quot;\nè¿™å°†åœ¨ source/_posts/ ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª Markdown æ–‡ä»¶ï¼Œæ–‡ä»¶çš„å¼€å¤´æ˜¯ Front-matterï¼Œç”¨äºé…ç½®æ–‡ç« çš„å…ƒæ•°æ®ï¼ˆå¦‚æ ‡é¢˜ã€æ—¥æœŸã€åˆ†ç±»å’Œæ ‡ç­¾ç­‰ï¼‰ï¼š\ntitle: æ·±åº¦å­¦ä¹ å…¥é—¨æŒ‡å—  date: 2025-03-06 15:00:00  categories:    - äººå·¥æ™ºèƒ½    - æ·±åº¦å­¦ä¹   tags:    - ç¥ç»ç½‘ç»œ    - å…¥é—¨æ•™ç¨‹ \nä½¿ç”¨ Markdown æ’°å†™å†…å®¹ï¼š\nåœ¨ Front-matter ä¸‹æ–¹ï¼Œç”¨ Markdown è¯­æ³•æ’°å†™æ­£æ–‡ã€‚Hexo é»˜è®¤æ”¯æŒ GFMï¼ˆGitHub Flavored Markdownï¼‰ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ä¹¦å†™æ ¼å¼ï¼Œä¾‹å¦‚ï¼š\n# ä¸€çº§æ ‡é¢˜## äºŒçº§æ ‡é¢˜**ç²—ä½“**ã€*æ–œä½“*å¼ºè°ƒ\næ’å…¥å›¾ç‰‡å’Œèµ„æºï¼š\nå¯ç”¨ post_asset_folder: true åï¼Œæ¯ç¯‡æ–‡ç« ä¼šæœ‰ç‹¬ç«‹çš„èµ„æºç›®å½•ã€‚å¯ä»¥å°†å›¾ç‰‡æ–‡ä»¶æ”¾å…¥è¯¥æ–‡ä»¶å¤¹ï¼Œå¹¶åœ¨æ–‡ç« ä¸­å¼•ç”¨ï¼š\n![](my-post/images/example.png)\nè‰ç¨¿ç®¡ç†ä¸å‘å¸ƒï¼š\nå¯ç”¨è‰ç¨¿åŠŸèƒ½åï¼Œæ–°åˆ›å»ºçš„æ–‡ç« ä¼šå…ˆæ”¾åœ¨ _drafts/ ä¸‹ã€‚å®Œæˆåï¼Œä½¿ç”¨ hexo publish \"æ–‡ç« æ ‡é¢˜\" å°†å…¶å‘å¸ƒã€‚\næ–‡ç« ç»“æ„å’Œåˆ†é¡µï¼š\nHexo æ”¯æŒæ–‡ç« åˆ†ç±»å’Œæ ‡ç­¾è‡ªåŠ¨æ•´ç†ã€‚ä½ ä¹Ÿå¯ä»¥é€šè¿‡ &lt;!-- more --&gt; æ¥æ‰‹åŠ¨æˆªæ–­æ‘˜è¦ï¼Œæé«˜é¦–é¡µåŠ è½½é€Ÿåº¦ã€‚\n4. SEO ä¼˜åŒ–\nä¸ºäº†è®©æ›´å¤šäººçœ‹åˆ°ä½ çš„æŠ€æœ¯åšå®¢ï¼Œè¿›è¡Œ SEOï¼ˆæœç´¢å¼•æ“ä¼˜åŒ–ï¼‰éå¸¸é‡è¦ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¼˜åŒ–æªæ–½ï¼š\nç«™ç‚¹æ ‡é¢˜ä¸å…ƒä¿¡æ¯ï¼š\nåœ¨ _config.yml ä¸­å¡«å†™æœ‰åŠ©äº SEO çš„ç«™ç‚¹åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬ titleï¼ˆæ ‡é¢˜ï¼‰ã€descriptionï¼ˆæè¿°ï¼‰å’Œ keywordsï¼ˆå…³é”®è¯ï¼‰ã€‚\né“¾æ¥ä¼˜åŒ–ï¼š\nä¿®æ”¹æ°¸ä¹…é“¾æ¥æ ¼å¼ï¼Œç®€åŒ– URL ç»“æ„ï¼š\npermalink: :category/:title/\nç«™ç‚¹åœ°å›¾ï¼š\nç”Ÿæˆç«™ç‚¹åœ°å›¾å¸®åŠ©æœç´¢å¼•æ“æŠ“å–æ‰€æœ‰é¡µé¢ï¼š\nnpm install hexo-generator-sitemap hexo-generator-baidu-sitemap --save\nå¹¶åœ¨ _config.yml ä¸­æ·»åŠ é…ç½®ï¼š\nsitemap:  path: sitemap.xmlbaidusitemap:  path: baidusitemap.xml\næœºå™¨äººåè®®ï¼š\nåœ¨ source/ ç›®å½•ä¸‹åˆ›å»º robots.txt æ–‡ä»¶ï¼Œå¹¶å†™å…¥è§„åˆ™ï¼š\nUser-agent: *Allow: /Disallow: /admin/Sitemap: https://ä½ çš„åŸŸå/sitemap.xml\nå¥½çš„ï¼Œä»¥ä¸‹æ˜¯æˆ‘é‡æ–°ç”Ÿæˆå¹¶ä¿æŒå®Œæ•´çš„ Hexo Deploy è‡ªåŠ¨éƒ¨ç½²éƒ¨åˆ†ï¼Œç¡®ä¿æ²¡æœ‰çœç•¥ä»»ä½•ç»†èŠ‚ï¼š\n\n5. åšå®¢éƒ¨ç½²\nå®Œæˆå†…å®¹åˆ›ä½œå’Œä¼˜åŒ–åï¼Œå°±éœ€è¦å°†åšå®¢éƒ¨ç½²ä¸Šçº¿ã€‚Hexo ç”Ÿæˆçš„æ˜¯çº¯é™æ€ç½‘é¡µï¼Œå¯ä»¥éƒ¨ç½²åœ¨ä»»æ„é™æ€æœåŠ¡å™¨æˆ–æ‰˜ç®¡å¹³å°ä¸Šã€‚è¿™é‡Œä»‹ç»åœ¨ Ubuntu æœåŠ¡å™¨ä¸Šä½¿ç”¨ Nginx éƒ¨ç½²çš„æ–¹æ¡ˆï¼Œå¹¶è®¨è®º Nginx é…ç½®å’Œ Git è‡ªåŠ¨éƒ¨ç½²æ–¹æ³•ã€‚\næœ¬åœ°ç”Ÿæˆé™æ€æ–‡ä»¶ï¼š\nHexo æä¾›å‘½ä»¤å°† Markdown å†…å®¹ç”Ÿæˆé™æ€ç½‘é¡µã€‚ä¸€èˆ¬åœ¨æœ¬åœ°æˆ–æœåŠ¡å™¨ä¸Šè¿è¡Œï¼š\nhexo clean        # æ¸…ç†ä¸Šæ¬¡ç”Ÿæˆçš„æ–‡ä»¶hexo generate (hexo g)   # ç”Ÿæˆæœ€æ–°é™æ€ç½‘é¡µ\nç”Ÿæˆçš„æ–‡ä»¶ä½äºåšå®¢ç›®å½•ä¸‹çš„ public/ æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«åšå®¢çš„æ‰€æœ‰ HTMLã€CSSã€JSã€å›¾ç‰‡ç­‰é™æ€èµ„æºã€‚è¿™ä¸ª public æ–‡ä»¶å¤¹å³æ˜¯æœ€ç»ˆéƒ¨ç½²çš„ç½‘ç«™å†…å®¹ã€‚\nNginx éƒ¨ç½²é™æ€ç«™ç‚¹ï¼š\nåœ¨æœåŠ¡å™¨ä¸Šå®‰è£… Nginx å¹¶é…ç½®ç«™ç‚¹ï¼Œä»¥æä¾› Web æœåŠ¡ï¼š\nå®‰è£… Nginxï¼š\nsudo apt-get install -y nginx\nå®‰è£…åå¯åŠ¨ Nginx æœåŠ¡ï¼š\nsudo systemctl start nginx  # å¯è®¾ç½®å¼€æœºè‡ªå¯\né…ç½®ç«™ç‚¹ï¼šåœ¨ /etc/nginx/sites-available/ ç›®å½•ä¸‹åˆ›å»ºé…ç½®æ–‡ä»¶ï¼Œå¦‚ hexo.confï¼Œå†…å®¹å¦‚ä¸‹ï¼š\nserver &#123;    listen 80;    server_name example.com;  # å°†æ­¤æ›¿æ¢ä¸ºä½ çš„åŸŸåæˆ–æœåŠ¡å™¨IP    root /var/www/hexo/public;    index index.html index.htm;    location / &#123;        try_files $uri $uri/ =404;    &#125;&#125;\nä¸Šè¿°é…ç½®æŒ‡å®šæœåŠ¡å™¨ç›‘å¬ 80 ç«¯å£ï¼Œserver_name ä¸ºä½ çš„åŸŸåï¼ˆéœ€è¦å°†åŸŸåè§£ææŒ‡å‘è¯¥æœåŠ¡å™¨ï¼‰ã€‚root æŒ‡å‘ Hexo ç”Ÿæˆçš„ public ç›®å½•ï¼Œindex å£°æ˜é»˜è®¤é¦–é¡µæ–‡ä»¶ã€‚\nå¯ç”¨ç«™ç‚¹é…ç½®ï¼šå°†é…ç½®æ–‡ä»¶é“¾æ¥åˆ° sites-enabledï¼š\nln -s /etc/nginx/sites-available/hexo.conf /etc/nginx/sites-enabled/nginx -t  # æµ‹è¯•é…ç½®è¯­æ³•æ­£ç¡®æ€§systemctl reload nginx  # é‡æ–°åŠ è½½ Nginx é…ç½®\næ‰§è¡Œä»¥ä¸Šå‘½ä»¤åï¼Œåšå®¢ç«™ç‚¹å³å¯é€šè¿‡åŸŸåè®¿é—®ã€‚å¦‚æœæš‚æ—¶æ²¡æœ‰åŸŸåï¼Œä½¿ç”¨æœåŠ¡å™¨ IP ä¹Ÿèƒ½è®¿é—®ï¼ˆæ­¤æ—¶å¯å°† server_name æ”¹ä¸º _ é€šé…ç¬¦ï¼‰ã€‚\né…ç½® HTTPSï¼ˆå¯é€‰ï¼‰ï¼š\nå»ºè®®ä¸ºåšå®¢é…ç½® SSL è¯ä¹¦ã€‚å¯ä»¥ä½¿ç”¨ Certbot è·å– Letâ€™s Encrypt å…è´¹è¯ä¹¦ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š\napt-get install -y certbot python3-certbot-nginx  certbot --nginx -d example.com -d www.example.com\næŒ‰æç¤ºå®ŒæˆåŸŸåæ‰€æœ‰æƒéªŒè¯åï¼ŒCertbot ä¼šè‡ªåŠ¨ç”Ÿæˆè¯ä¹¦å¹¶é…ç½® Nginx å°†ç«™ç‚¹å‡çº§ä¸º HTTPSã€‚\nHexo Deploy è‡ªåŠ¨éƒ¨ç½²ï¼š\næ¯æ¬¡æ›´æ–°å†…å®¹åéƒ½è¦é‡æ–°ç”Ÿæˆå¹¶ä¸Šä¼ æ–‡ä»¶ï¼Œä½¿ç”¨ Hexo çš„éƒ¨ç½²åŠŸèƒ½å¯ä»¥ç®€åŒ–æµç¨‹ã€‚Hexo æ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ï¼Œå…¶ä¸­ Git éƒ¨ç½²æ˜¯å¸¸ç”¨æ–¹æ¡ˆä¹‹ä¸€ã€‚åŸºæœ¬æ€è·¯æ˜¯åˆ©ç”¨ Git æŠŠç”Ÿæˆçš„é™æ€æ–‡ä»¶æ¨é€åˆ°æœåŠ¡å™¨æˆ–æ‰˜ç®¡æœåŠ¡ã€‚æ¦‚æ‹¬äº†è¿™ç§æ€è·¯ï¼šåœ¨æœåŠ¡å™¨ä¸Šå®‰è£… Nginx æä¾›ç½‘é¡µæœåŠ¡ï¼Œç”¨ Git å®ç°ä»£ç ä¸Šä¼ è‡ªåŠ¨åŒ–ï¼Œè¿™æ ·æœ¬åœ°æ‰§è¡Œä¸€æ¬¡ hexo dï¼ˆdeployï¼‰å°±èƒ½è®©ç½‘ç«™æ›´æ–°ã€‚\næ¨é€åˆ°è¿œç¨‹æ‰˜ç®¡ï¼š\nå°†åšå®¢é™æ€æ–‡ä»¶éƒ¨ç½²åˆ°åƒ GitHub Pagesã€Coding Pages è¿™ç±»å¹³å°ã€‚è¿™éœ€è¦åœ¨ _config.yml ä¸­é…ç½®ï¼š\ndeploy:  type: git  repo: https://github.com/yourname/yourrepo.git  branch: main  # æˆ– gh-pages åˆ†æ”¯ç­‰\nç„¶åè¿è¡Œ hexo generate &amp;&amp; hexo deployï¼ŒHexo ä¼šæŠŠ public æ–‡ä»¶å¤¹å†…å®¹æ¨é€åˆ°æŒ‡å®šä»“åº“çš„åˆ†æ”¯ã€‚å¯¹äº GitHub Pagesï¼Œå¦‚æœ repo æ˜¯ yourname.github.io åˆ™ç›´æ¥ç”¨ä¸»åˆ†æ”¯ï¼›è‹¥æ˜¯é¡¹ç›®ä»“åº“ï¼Œå¯ä»¥ç”¨ gh-pages åˆ†æ”¯æ‰˜ç®¡ã€‚\néƒ¨ç½²åï¼ŒGitHub Pages æœåŠ¡å°†æ‰˜ç®¡ä½ çš„é™æ€åšå®¢ï¼Œä½ å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰åŸŸåç»‘å®šå®ƒã€‚ä½†æ³¨æ„ï¼šå¦‚æœä½ å¸Œæœ›åšå®¢è¿è¡Œåœ¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Šï¼ˆè€Œéç¬¬ä¸‰æ–¹å¹³å°ï¼‰ï¼Œåˆ™è¿™ç§æ–¹æ¡ˆä¸æ¶‰åŠä½ çš„æœåŠ¡å™¨ Nginxã€‚å¦å¤–ï¼Œå›½å†…è®¿é—® GitHub Pages å¯èƒ½ä¸ç¨³å®šï¼Œéœ€ç»“åˆå®é™…æƒ…å†µè€ƒè™‘ã€‚\næ¨é€åˆ°è‡ªå·±æœåŠ¡å™¨ï¼š\næ­å»ºå±äºè‡ªå·±çš„ Git è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹ï¼Œå®ç°å°†æœ¬åœ°æ›´æ–°ä¸€é”®éƒ¨ç½²åˆ°æœåŠ¡å™¨ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š\n\nåœ¨æœåŠ¡å™¨ä¸Šåˆ›å»ºä¸€ä¸ªè£¸ä»“åº“ï¼ˆbare repositoryï¼‰ï¼Œç”¨äºæ¥æ”¶æ¨é€ã€‚ä¾‹å¦‚åˆ›å»º /home/git/hexo.git è£¸ä»“åº“ã€‚\nç¼–å†™ Git é’©å­ï¼ˆpost-receiveï¼‰ï¼šè£¸ä»“åº“çš„ hooks/post-receive è„šæœ¬ä¼šåœ¨æ”¶åˆ°æ–°æ¨é€æ—¶æ‰§è¡Œã€‚è„šæœ¬å†…å®¹å¯ä»¥æ˜¯å°†æ›´æ–°çš„å†…å®¹æ£€å‡ºåˆ° Nginx ç›®å½•ã€‚ä¾‹å¦‚ï¼š\nGIT_WORK_TREE=/var/www/hexo git checkout -f  # å°†ä»“åº“å†…å®¹å¼ºåˆ¶æ£€å‡ºåˆ° /var/www/hexocd /var/www/hexo &amp;&amp; hexo generate            # ï¼ˆè‹¥æ¨é€çš„æ˜¯æºç è€Œéç”Ÿæˆæ–‡ä»¶ï¼Œåˆ™éœ€è¦åœ¨æœåŠ¡å™¨æ‰§è¡Œç”Ÿæˆï¼‰\nç»™è„šæœ¬å¯æ‰§è¡Œæƒé™ï¼š\nchmod +x post-receive\nè¿™æ ·ï¼Œæ¯å½“æ¨é€åˆ°è¯¥ä»“åº“æ—¶ï¼Œå®ƒå°±ä¼šæŠŠæ›´æ–°éƒ¨ç½²åˆ°åšå®¢ç›®å½•å¹¶ç”Ÿæˆæœ€æ–°é¡µé¢ã€‚\næœ¬åœ° Hexo é…ç½®éƒ¨ç½²ï¼šå°† _config.yml ä¸­çš„ deploy.repo è®¾ç½®ä¸ºä¸Šè¿°è£¸ä»“åº“çš„åœ°å€ï¼ˆé€šè¿‡ SSHï¼‰ã€‚ä¾‹å¦‚ï¼š\ndeploy:  type: git  repo: ssh://[emailÂ protected]/home/git/hexo.git  branch: master\nç„¶åæ‰§è¡Œ hexo clean &amp;&amp; hexo deployã€‚Hexo ä¼šé€šè¿‡ Git æ¨é€åˆ°æœåŠ¡å™¨ä»“åº“ï¼Œè§¦å‘ post-receive é’©å­ï¼Œå®ç°è‡ªåŠ¨éƒ¨ç½²ã€‚å®Œæˆåï¼ŒNginx ä¼šç«‹åˆ»æä¾›æ–°å†…å®¹æœåŠ¡ï¼Œæ— éœ€æ‰‹åŠ¨ç™»å½•æœåŠ¡å™¨æ“ä½œã€‚\n\né€šè¿‡è¿™ç§æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨æœ¬åœ°å†™å¥½æ–‡ç« åä¸€æ¡å‘½ä»¤å®Œæˆéƒ¨ç½²ï¼Œéå¸¸é«˜æ•ˆã€‚è®¸å¤šå¼€æºåšå®¢éƒ¨ç½²è„šæœ¬å’Œå·¥å…·ä¹Ÿæ˜¯åŸºäºç±»ä¼¼åŸç†å®ç°çš„ã€‚åˆæ¬¡è®¾ç½®å¯èƒ½ç¨æ˜¾ç¹çï¼Œä½†ä¸€æ—¦é…ç½®æˆåŠŸï¼Œæ—¥å¸¸æ›´æ–°å°†éå¸¸ä¾¿æ·ã€‚\næç¤ºï¼š ä½¿ç”¨ Git è‡ªåŠ¨éƒ¨ç½²éœ€ç¡®ä¿æœåŠ¡å™¨å¼€æ”¾ Git æ‰€ç”¨çš„ SSH ç«¯å£ï¼ˆé»˜è®¤ä¸º 22ï¼‰ï¼Œå¹¶é…ç½®å¥½å…¬é’¥å…å¯†ç™»å½•ï¼Œä»¥ä¾¿ Hexo åœ¨æœ¬åœ°èƒ½é¡ºåˆ©æ¨é€åˆ°æœåŠ¡å™¨ã€‚å¦‚æœä½ çš„æœåŠ¡å™¨ SSH ç«¯å£ä¸æ˜¯ 22ï¼Œå¯åœ¨éƒ¨ç½²é…ç½®ä¸­åŠ å…¥ç«¯å£å·æˆ–åœ¨ .ssh/config ä¸­é…ç½®åˆ«åã€‚å¯¹äºä¸ç†Ÿæ‚‰ Git é’©å­çš„æ–°æ‰‹ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘ä½¿ç”¨ç®€å•çš„ rsync è„šæœ¬åŒæ­¥æ–‡ä»¶æˆ–å€ŸåŠ© CI å¹³å°å®ç°éƒ¨ç½²ï¼Œä½†åŸç†ç±»ä¼¼ã€‚\n\n6. ç»´æŠ¤ä¸ä¼˜åŒ–\nåšå®¢æ­å»ºå®Œæˆå¹¶ä¸æ„å‘³ç€ä¸€åŠ³æ°¸é€¸ï¼Œå®šæœŸçš„ç»´æŠ¤å’Œä¼˜åŒ–èƒ½ä¿è¯åšå®¢ç¨³å®šã€å®‰å…¨ï¼Œå¹¶æŒç»­æå‡ç”¨æˆ·ä½“éªŒã€‚\n\næ’ä»¶æ‰©å±•ï¼š Hexo æ‹¥æœ‰ä¸°å¯Œçš„æ’ä»¶ç”Ÿæ€ï¼Œå¯æ ¹æ®éœ€è¦å®‰è£…æ’ä»¶ä»¥å¢å¼ºåŠŸèƒ½ã€‚\nå¤‡ä»½ä¸ç‰ˆæœ¬æ§åˆ¶ï¼š ä½¿ç”¨ Git ç®¡ç†åšå®¢æºç ï¼Œå®šæœŸå¤‡ä»½ã€‚\næ›´æ–°ä¸å‡çº§ï¼š å…³æ³¨ Hexo çš„ç‰ˆæœ¬æ›´æ–°ã€æ’ä»¶æ›´æ–°ç­‰ã€‚\n\n","categories":["å…¶å®ƒ"]}]