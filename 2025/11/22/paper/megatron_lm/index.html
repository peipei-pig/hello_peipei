<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yaopepe.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"post","display":"post","padding":18,"offset":12,"onmobile":false,"width_dual_column":240},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="Megatron-LM">
<meta property="og:type" content="article">
<meta property="og:title" content="Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism">
<meta property="og:url" content="https://yaopepe.com/2025/11/22/paper/megatron_lm/index.html">
<meta property="og:site_name" content="果冻甜甜的">
<meta property="og:description" content="Megatron-LM">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-11-22T03:43:29.078Z">
<meta property="article:modified_time" content="2025-11-22T03:43:29.078Z">
<meta property="article:author" content="PePe">
<meta property="article:tag" content="paper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yaopepe.com/2025/11/22/paper/megatron_lm/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | 果冻甜甜的</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">果冻甜甜的</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-首页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

<div class="site-meta-counts" style="font-size:.9em;opacity:.85;margin-top:.25rem;display:flex;gap:.75rem;flex-wrap:wrap">
  <span class="post-meta-item">
    <i class="fa fa-eye"></i>
    <span class="post-meta-item-text">总访问量</span>
    <span id="vercount_value_site_pv">0</span>
  </span>

  <span class="post-meta-item">
    <i class="fa fa-file"></i>
    <span class="post-meta-item-text">总文章数</span>
    <span id="total_posts_count">11</span>
  </span>
</div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://yaopepe.com/2025/11/22/paper/megatron_lm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="PePe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="果冻甜甜的">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-11-22 11:43:29" itemprop="dateCreated datePublished" datetime="2025-11-22T11:43:29+08:00">2025-11-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

<span class="post-meta-item">
  <span class="post-meta-item-icon"><i class="fa fa-eye"></i></span>
  <span class="post-meta-item-text">Views:</span>
  <span id="vercount_value_page_pv">0</span>  
</span>


            <div class="post-description">Megatron-LM</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>
<hr />
<h2 id="一论文速览">一、论文速览</h2>
<p>《Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism》聚焦的问题是：<strong>在单卡显存远远不够的情况下，如何在 GPU 集群上高效训练多亿到几十亿参数规模的 Transformer 语言模型</strong>。作者提出了一种专门为 Transformer 设计的 <strong>层内模型并行（intra-layer model parallelism / tensor parallelism）</strong>：将自注意力和前馈网络中的大矩阵按行/列拆分到多块 GPU 上，并在关键位置插入少量 <code>all-reduce</code> / <code>all-gather</code> 通信以拼合结果，从而突破单卡显存限制。该方案完全基于原生 PyTorch，不需要新的编译器或 DSL，并且可以与数据并行、流水并行组合使用；在 512 张 V100 上，作者训练了 8.3B GPT-2 类模型和 3.9B BERT 类模型，达到了 15.1 PFLOPs、约 76% 强扩展效率，并在 WikiText-103、LAMBADA、RACE 等任务上取得当时的 SOTA。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p>
<h2 id="二论文结构概览">二、论文结构概览</h2>
<ol type="1">
<li><p><strong>Introduction</strong> 说明大规模 Transformer 语言模型在效果上的优势，以及显存资源对训练的制约，指出仅靠数据并行已经难以继续扩展模型规模。引出 Megatron-LM 的核心思路：通过简单、高效的层内模型并行，把每一层的大矩阵拆到多卡上计算，并给出 8.3B / 3.9B 模型的主要实验结果。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p></li>
<li><p><strong>Related Work</strong> 回顾 GPT-2、BERT 等典型大模型，以及常见的分布式训练方式（数据并行、传统按层模型并行、GPipe/PipeDream 这类流水并行），指出这些方法在显存利用、通信开销、多机扩展方面的不足。通过这部分，读者可以把 Megatron-LM 放在“大模型并行训练方法谱系”中来理解。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p></li>
<li><p><strong>Model Parallel Transformer</strong> 全文技术核心：详细介绍如何在 Transformer 自注意力和 MLP 两个子层中对权重矩阵进行按行/按列切分，如何安排前向和反向中的 <code>all-reduce</code> / <code>all-gather</code>，以及如何处理 embedding / 输出层等特殊模块。这一部分实际上定义了后来广泛使用的 “Megatron 式 Tensor Parallelism” 的具体语义。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
<li><p><strong>Implementation Details</strong> 介绍在多机多卡集群上的部署细节，包括：硬件拓扑（32 台 DGX-2H，512 张 V100，NVSwitch+InfiniBand）、通信后端（NCCL）、混合精度训练（FP16+loss scaling）、优化器配置等。这里给出了一些影响实际吞吐的关键工程选择，对希望在自己系统里复现实验或移植思想的读者非常有参考价值。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
<li><p><strong>Experiments</strong> 系统评估 Megatron-LM 的扩展性和任务性能：在 GPT-2 类（1.2B–8.3B）和 BERT 类（0.3B–3.9B）模型上，报告 FLOPs 利用率、强/弱扩展效率，以及 WikiText-103、LAMBADA、RACE 等下游任务上的 SOTA 结果。同时分析 BERT 架构中 LayerNorm 放置位置对大模型训练稳定性和效果的影响。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p></li>
<li><p><strong>Conclusion</strong> 总结提出的 intra-layer 模型并行方案及其实验结果，强调该方法简单、高效、易与数据并行和流水并行结合，并指出这为后续更大规模（甚至 trillion 参数级）的模型训练铺平了道路。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p></li>
</ol>
<blockquote>
<p><strong>核心思想：</strong> Megatron-LM 提出了一套专为 Transformer 设计的层内张量并行方案，通过对每层中的大矩阵按行/列规则切分并在固定位置插入少量集体通信，在不改变模型结构和框架的前提下，将 GPT/BERT 类语言模型扩展到多亿至几十亿参数规模，并在多机多卡集群上实现了高 FLOPs 利用率与实测任务性能的同步提升。</p>
</blockquote>
<hr />
<h2 id="三方法与系统设计拆解">三、方法与系统设计拆解</h2>
<p>Megatron-LM 的方法部分针对的是一个很直接的系统问题：<strong>如何在不重写框架、不引入新编译器的前提下，把单层内部的大矩阵拆给多张 GPU 做，又把通信次数压到很少？</strong> 作者围绕 Transformer 的 MLP、自注意力和 Embedding 三个“算力与显存大头”，设计了一整套张量并行（tensor parallel）策略，并通过有限次 <code>all-reduce</code> / <code>all-gather</code> 完成必要的同步。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p>
<p><strong>主要在解决的子问题包括：</strong></p>
<ul>
<li>如何在 <strong>MLP 两个大线性层</strong> 上进行张量切分，既降低单卡显存，又控制同步次数在可接受范围？(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li>如何在 <strong>自注意力（Q/K/V 投影 + 输出线性层）</strong> 上并行，让每张 GPU 负责一部分 head，同时保证最终 attention 输出正确？(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li>如何对 <strong>输入/输出 embedding</strong> 做并行，避免 <code>batch × seq × vocab</code> 级别的巨大通信，同时兼顾权重共享和 cross-entropy 计算？(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li>如何将上述层内并行与 <strong>数据并行</strong> 组合，把模型规模与吞吐量一起扩展到数百 GPU？(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul>
<h3 id="核心模块一览">3.1 核心模块一览</h3>
<ul>
<li><p><strong>MLP：列切分的第一层线性（Column Parallel Linear）</strong> 第一层线性层 (X W_1) 的权重 (W_1) 按列切分为 (W_{1,1}, W_{1,2}, …)，不同 GPU 负责输出 hidden 的不同切片；GeLU 在各 GPU 本地计算，后续根据需要再做 <code>all-reduce</code> / <code>all-gather</code>。这种设计避免了在激活前就同步，从而减少通信。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
<li><p><strong>MLP：行切分的第二层线性（Row Parallel Linear）</strong> 第二层线性层 (Y W_2) 中，权重 (W_2) 按行切分，各 GPU 仅持有一部分行。输入 (Y) 已经是按列切好的局部张量，每个 GPU 独立计算局部输出，最后通过一次 <code>all-reduce</code> 累加求和得到完整输出。这样 MLP 两层整体只需少量同步。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
<li><p><strong>自注意力：按 head/列切分的并行多头注意力</strong> Q/K/V 投影矩阵按列切分，各 GPU 负责一部分 heads 的计算；attention 完成后，输出线性层按行切分，每个 GPU 得到局部输出，再通过 <code>all-reduce</code> 得到完整 hidden 表示。整体上，自注意力模块的前向+反向只引入固定几次集体通信。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
<li><p><strong>I/O Embedding：词表维度上的并行 + 并行 cross-entropy</strong> embedding 矩阵在 vocab 维度上切分：</p>
<ul>
<li>输入端：每 GPU 存一部分 vocab，对应 tokens 查表后，用 <code>all-reduce</code> 合成完整 embedding；</li>
<li>输出端：将 <code>all-gather</code> 和 cross-entropy loss 计算融合成“并行 cross-entropy”，把原本 <code>batch × seq × vocab</code> 的通信，缩减为 <code>batch × seq</code> 规模的数据交换。(<a target="_blank" rel="noopener" href="https://minjiazhang.github.io/courses/sp24-resource/Megatron-LM.pdf">Minjia Zhang</a>)</li>
</ul></li>
<li><p><strong>张量并行 × 数据并行的组合</strong> 将上述张量并行视为“模型并行组”内部的事，再在其外层套一圈数据并行。每个数据并行组内是一组做 TP 的 GPU，组间通过数据并行同步参数/梯度，实现模型规模和 batch 尺度的双向扩展。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
</ul>
<h3 id="数据流与控制流示意文字版">3.2 数据流与控制流示意（文字版）</h3>
<p>可以用以下“从输入到输出”的流程来理解 Megatron-LM 的张量并行数据流与控制流：</p>
<ol type="1">
<li><p><strong>输入阶段：并行 Embedding</strong></p>
<ul>
<li>输入 tokens 分发到每张 GPU；</li>
<li>每张 GPU 持有 embedding 的一部分列，局部查表得到局部 embedding；</li>
<li>一次 <code>all-reduce</code> 把局部 embedding 聚合为完整的 hidden 表示 ()。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul></li>
<li><p><strong>Transformer 层 – 自注意力块</strong></p>
<ol type="1">
<li><p>Q/K/V 投影：</p>
<ul>
<li>权重按列切分，各 GPU 计算自己那部分 heads 对应的 Q/K/V；</li>
</ul></li>
<li>在每张 GPU 上独立计算本地 heads 的自注意力（Q·Kᵀ / √d → softmax → ×V）；</li>
<li><p>输出线性层按行切分，各 GPU 得到局部输出，最后通过一次 <code>all-reduce</code> 得到完整 hidden 表示。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
</ol></li>
<li><p><strong>MLP 块</strong></p>
<ol type="1">
<li><p>第一层线性：列切分</p>
<ul>
<li>每张 GPU 计算 (Y_i = X W_{1,i})，并本地执行 GeLU；</li>
</ul></li>
<li><p>第二层线性：行切分</p>
<ul>
<li>每张 GPU 计算 (Z_i = Y_i W_{2,i})，通过 <code>all-reduce</code> 将各 GPU 局部输出相加，得到整体输出。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul></li>
</ol></li>
<li><p><strong>反向传播中的通信</strong></p>
<ul>
<li>对列切分层：前向可不 <code>all-gather</code>，反向需要合并梯度；</li>
<li>对行切分层：前向需要一次 <code>all-reduce</code>，反向梯度可以局部更新；</li>
<li>结合注意力与 MLP，一个完整的前向+反向中，每层总共约 4 次集体通信。(<a target="_blank" rel="noopener" href="https://minjiazhang.github.io/courses/sp24-resource/Megatron-LM.pdf">Minjia Zhang</a>)</li>
</ul></li>
<li><p><strong>输出层与 loss</strong></p>
<ul>
<li>输出 embedding 与输入 embedding 共享权重，并在 vocab 维度切分；</li>
<li>通过并行 cross-entropy，将 logits 的合并与 loss 计算融合，通信规模大幅降低。(<a target="_blank" rel="noopener" href="https://minjiazhang.github.io/courses/sp24-resource/Megatron-LM.pdf">Minjia Zhang</a>)</li>
</ul></li>
<li><p><strong>外层数据并行</strong></p>
<ul>
<li>TP 组内部完成上述所有操作后，按常规数据并行方式在组间做梯度 <code>all-reduce</code>；</li>
<li>从全局看，是“组内模型并行 + 组间数据并行”的双层结构。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul></li>
</ol>
<h3 id="关键假设与适用范围">3.3 关键假设与适用范围</h3>
<ul>
<li><p><strong>假设一：Transformer 结构足够规则、层间同构</strong></p>
<ul>
<li>所有层都由“多头注意力 + 两层 MLP”堆叠，hidden size 基本一致；</li>
<li>这使得简单的“行切/列切 + 固定通信模式”在所有层上都能复用。</li>
<li>对高度异构或带大量动态分支的结构，直接套用会变得复杂。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul></li>
<li><p><strong>假设二：组内网络带宽和延迟足够好</strong></p>
<ul>
<li>默认 TP 组内 GPU 之间通过 NVLink/NVSwitch 高速互联，跨节点通过多条 InfiniBand 连接；</li>
<li>在这样的拓扑下，每层 4 次左右的集体通信是可以接受的。</li>
<li>在带宽较低或拓扑复杂的环境下，通信可能会成为主要瓶颈。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul></li>
<li><p><strong>假设三：切分后负载在不同 GPU 之间大致均衡</strong></p>
<ul>
<li>按行/列均匀切分矩阵时，假定各 shard 的 FLOPs 和显存占用相近；</li>
<li>若硬件异构或有额外任务打扰，简单均匀切分可能出现严重的 straggler 问题。</li>
</ul></li>
<li><p><strong>假设四：通信原语实现高效稳定</strong></p>
<ul>
<li>强依赖 NCCL 等库高效实现 <code>all-reduce</code> / <code>all-gather</code> 等操作；</li>
<li>若通信库调优不足或底层网络栈存在问题，即便算法结构相同，实际性能也可能大打折扣。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul></li>
<li><p><strong>假设五：模型规模和 batch 规模足够大，可以摊薄通信成本</strong></p>
<ul>
<li>TP 的优势更明显地体现在 multi-billion 参数、大 batch 的 regime；</li>
<li>对中小模型或极小 batch，通信开销可能会压过计算，导致整体效率不佳。</li>
</ul></li>
</ul>
<p><strong>与常见训练栈的对应关系</strong></p>
<ul>
<li><strong>DataLoader / 数据通路</strong>：基本不改数据格式，主要需要在 embedding/输出层适配切分后的张量布局。</li>
<li><strong>并行调度</strong>：在数据并行之外增加“模型并行组”的概念，形成组内 TP + 组间 DP 的双层调度。</li>
<li><strong>模型并行（TP）</strong>：行切/列切线性层、按 head 的自注意力并行、embedding 并行，构成现代 TP 的原型。</li>
<li><strong>kernel 与算子</strong>：大多数算子仍是标准 GEMM/attention，只是张量形状和 layout 按 TP 方向变化；并行 cross-entropy 是通信与算子融合的代表。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li><strong>通信 backend</strong>：高度依赖 NCCL 等库高效实现集体操作，是 TP 性能的基础。</li>
<li><strong>配置与自动调参</strong>：TP size、DP size 等直接决定单卡显存占用和通信/计算比例，是系统配置的关键维度。</li>
</ul>
<hr />
<h2 id="四建模方式与评估指标">四、建模方式与评估指标</h2>
<h3 id="问题是如何形式化的">4.1 问题是如何形式化的？</h3>
<p>Megatron-LM 并没有给出严格的数学规划形式，而是从“系统 + 性能”的角度去形式化问题：在固定 GPU 数量、单卡显存、网络带宽条件下，如何设计一种层内模型并行策略，使得：</p>
<ul>
<li>模型参数总量 (P) 尽可能大（扩展到 multi-billion 参数）；</li>
<li>单步训练时间 (T) 尽可能小，FLOPs 利用率尽可能高；</li>
<li>通信开销相对可控，可以在实际集群上高效运行。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</li>
</ul>
<p>可以理解为隐含的目标：</p>
<ul>
<li><p><strong>目标 1：显存约束下最大化可训练参数规模</strong></p>
<ul>
<li>单卡显存上限为 M，模型并行度为 N，希望达到 (P O(M N)) 量级的模型。</li>
</ul></li>
<li><p><strong>目标 2：在给定网络条件下最小化通信开销相对占比</strong></p>
<ul>
<li>每层只做有限次、模式固定的 <code>all-reduce</code> / <code>all-gather</code>，并尽量让通信的数据量与参数切片大小相关，而不是与 <code>batch × seq × vocab</code> 这种大张量直接成正比。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul></li>
<li><p><strong>目标 3：放大模型规模带来的性能收益，而不是“只堆参数”</strong></p>
<ul>
<li>通过在 WikiText-103、LAMBADA、RACE 等任务上报告 SOTA，证明模型变大有实质任务收益。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</li>
</ul></li>
</ul>
<h3 id="核心评估指标">4.2 核心评估指标</h3>
<p>论文主要使用了以下几类指标来评估方法效果：(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p>
<ul>
<li><p><strong>模型参数规模（#Params）</strong></p>
<ul>
<li>GPT-2 类模型：从 1.2B 扩展到 8.3B；</li>
<li>BERT 类模型：从 0.3B 扩展到 3.9B。</li>
<li>这是最直接体现“模型到底有多大”的指标，也是层内模型并行存在的前提。</li>
</ul></li>
<li><p><strong>FLOPs 利用率 / Sustained FLOPs</strong></p>
<ul>
<li>单卡基线：1.2B 模型在单 V100 上能 sustain 39 TFLOPs，约为理论峰值的 30%。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li>多卡训练：8.3B 模型在 512 张 V100 上能达到 15.1 PFLOPs，约 76% 强扩展效率。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li>该指标体现“GPU 被用得有多满”，是系统性能的硬指标。</li>
</ul></li>
<li><p><strong>扩展效率（Scaling Efficiency）</strong></p>
<ul>
<li>强扩展：保持模型和 batch 不变，增加 GPU 数，观察吞吐变化和效率；</li>
<li>弱扩展：随 GPU 数增加成比例地增大模型规模或 batch，看 per-GPU 利用率是否稳定。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li>Megatron-LM 报告，在 1–8 卡模型并行 + 64-way 数据并行的 512 GPU 场景下，强扩展效率约为 74–76%。</li>
</ul></li>
<li><p><strong>下游任务性能（NLP Benchmark）</strong></p>
<ul>
<li><p>GPT-2 类模型：</p>
<ul>
<li>WikiText-103 perplexity：10.8（相对 15.8 的 SOTA 有明显提升）；</li>
<li>LAMBADA 准确率：66.5%（优于 63.2% 的 SOTA）。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</li>
</ul></li>
<li><p>BERT 类模型：</p>
<ul>
<li>RACE 准确率：90.9%（优于 89.4% 的 SOTA）。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</li>
</ul></li>
<li><p>这些指标证明大模型不仅“能训出来”，而且“能带来更好的任务效果”。</p></li>
</ul></li>
<li><p><strong>训练稳定性相关指标（如收敛曲线）</strong></p>
<ul>
<li>尤其在 BERT 大模型中，对比不同 LayerNorm 布局下的训练 loss 收敛情况，展示了部分架构选择在大规模下会导致训练不稳定。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
</ul></li>
</ul>
<hr />
<h2 id="五主要实验发现">五、主要实验发现</h2>
<p>综合来看，Megatron-LM 在实验部分传递的结论可以用几条工程化的说法概括：(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p>
<ul>
<li>采用层内张量并行，可以在 512 张 V100 GPU 上训练 <strong>8.3B GPT-2 类模型</strong> 和 <strong>3.9B BERT 类模型</strong>，突破了当时通用 GPU 上的模型规模极限。</li>
<li>在强基线（1.2B 单卡 39 TFLOPs ≈ 30% 峰值）的基础上，8.3B 模型在 512 卡上依然能 sustain 15.1 PFLOPs，对应约 <strong>76% 的强扩展效率</strong>。</li>
<li>模型变大后，语言建模和阅读理解任务的性能都有显著提升：WikiText-103 perplexity 从 15.8 降到 10.8；LAMBADA 准确率从 63.2% 升到 66.5%；RACE 准确率从 89.4% 升到 90.9%。</li>
<li>对 BERT 架构做了系统的 LayerNorm 位置对比，发现原始 Post-LN 设计在 3.9B 规模下不稳定，而改成 Pre-LN 后可以显著改善训练稳定性和最终性能。</li>
</ul>
<h3 id="关键图表解读">5.1 关键图表解读</h3>
<ul>
<li><p><strong>扩展性图：吞吐/效率 vs GPU 数</strong></p>
<ul>
<li>展示了从单 GPU 到 512 GPU 的强、弱扩展曲线：随着 GPU 数增加，吞吐接近线性提升，强扩展效率仍保持在 ~74–76%。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li>这个图支撑了“TP 在大规模集群上是可扩展的”这一主张。</li>
</ul></li>
<li><p><strong>任务性能 vs 模型规模曲线</strong></p>
<ul>
<li>例如 GPT-2 模型在 WikiText-103、LAMBADA 上的性能随参数规模的变化；</li>
<li>趋势是：模型越大，效果越好，8.3B 模型明显优于之前的 SOTA。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</li>
<li>这说明“扩展模型不是纯炫技，而是能换来真实的任务收益”。</li>
</ul></li>
<li><p><strong>BERT 架构变体的收敛曲线（不同 LN 布局对比）</strong></p>
<ul>
<li>图中对比了不同 LayerNorm 放置方式下的训练 loss 随 step 变化；</li>
<li>可以看出某些布局在 3.9B 模型上难以收敛，而 Pre-LN 更稳定、效果更好。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</li>
<li>这强调了在大规模 regime 下架构细节的重要性。</li>
</ul></li>
</ul>
<p><strong>结果解读与边界</strong></p>
<p>从结果来看，Megatron-LM 在它关注的核心问题上给出了比较完整的证据链：(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p>
<ul>
<li>系统层面：有单卡强基线，有 512 卡扩展结果，有 FLOPs 利用率和扩展效率指标；</li>
<li>模型效果层面：在多个公开基准上达到或超越 SOTA，且清晰展示了“模型规模 → 性能”的收益曲线；</li>
<li>稳定性层面：对 BERT 架构的 LN 布局做了可视化对比，强调了结构设计对大模型训练的影响。</li>
</ul>
<p>但也有一些边界和未覆盖的点需要读者自己脑补：</p>
<ul>
<li>实验仅在 NVIDIA 自家硬件和栈上完成，对不同硬件/通信库的泛化需要自己评估；</li>
<li>没有与当时的其他并行方案（如 GPipe/PipeDream 等）进行更细致的系统 head-to-head 对比；(<a target="_blank" rel="noopener" href="https://minjiazhang.github.io/courses/sp24-resource/Megatron-LM.pdf">Minjia Zhang</a>)</li>
<li>对通信开销的理论建模和复杂拓扑下的表现，只有经验性说明，没有给出更 formal 的分析。</li>
</ul>
<hr />
<h2 id="六优点与局限">六、优点与局限</h2>
<h3 id="亮点strengths">亮点（Strengths）</h3>
<ul>
<li><p><strong>问题选择到位：直接瞄准“多亿参数 Transformer 怎么训”这个核心痛点</strong> 把显存瓶颈和数据并行的局限讲清楚，以 GPT-2/BERT 为例说明“大模型确实有用”，问题动机非常自然且重要。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p></li>
<li><p><strong>方法规则、简单、工程友好</strong> 行切/列切 + 少量固定位置的集体通信，几乎可以被抽象为一套“模板”：</p>
<blockquote>
<p>线性层要么按列切，要么按行切；注意力按 head 切；每层前后各几次 all-reduce/all-gather。 不依赖新框架或特殊编译器，完全可以在原生 PyTorch 上实现，对于工程团队来说成本低且可维护。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p>
</blockquote></li>
<li><p><strong>系统性能与任务性能兼顾</strong> 同时给出 FLOPs 利用率、扩展效率和标准 benchmark 上的 SOTA 指标，说明它既是系统优化，又有实质的模型效果提升。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p></li>
<li><p><strong>强调与其他并行方式的“正交可组合”特性</strong> 论文清楚指出层内模型并行可与数据并行、流水并行叠加使用，这一点后来在 3D/4D 并行中被广泛继承。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
<li><p><strong>对 BERT 架构的 LayerNorm 位置给出大规模下的经验结论</strong> 展示了 Pre-LN 对大规模模型稳定性的好处，对后续 Transformer 结构设计有深远影响。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
</ul>
<h3 id="局限limitations">局限（Limitations）</h3>
<ul>
<li><p><strong>对网络拓扑、异构集群等复杂环境的考虑较少</strong> 假定的是高速 NVSwitch + InfiniBand 的“好环境”，简单 TP 切法在弱网络或异构环境下可能表现不佳。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
<li><p><strong>实验主要基于自家硬件栈，泛化到其他平台需要额外工作</strong> 未给出在其他硬件（如 TPU 或不同厂商 GPU）上的验证，对跨生态迁移的读者不够友好。(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p></li>
<li><p><strong>理论分析偏薄，更多依赖经验和实测</strong> 例如行切/列切组合为什么“好用”，在什么条件下接近最优，缺乏更 formal 的模型和保证； 通信成本和性能边界也主要是通过实测说明，而不是建立统一的分析框架。(<a target="_blank" rel="noopener" href="https://minjiazhang.github.io/courses/sp24-resource/Megatron-LM.pdf">Minjia Zhang</a>)</p></li>
<li><p><strong>对未来复杂模型结构（MoE、稀疏注意力等）的适配未讨论</strong> 可以理解为时代限制，但从今天回看，它主要覆盖 dense Transformer，对后续更复杂架构提供的是启发而非直接方案。</p></li>
</ul>
<hr />
<h2 id="七业内相关工作对比">七、业内相关工作对比</h2>
<p>这里选几个与你经常接触、且和 Megatron-LM 强相关的工作做简要对比：(<a target="_blank" rel="noopener" href="https://minjiazhang.github.io/courses/sp24-resource/Megatron-LM.pdf">Minjia Zhang</a>)</p>
<ol type="1">
<li><p><strong>GPT-2：Language Models are Unsupervised Multitask Learners</strong></p>
<ul>
<li>重点是“scale → zero-shot 能力”，主要用单机 + 数据并行。</li>
<li>没有提出系统层面的新并行方法。</li>
</ul></li>
<li><p><strong>GPipe / PipeDream：基于流水并行的巨型网络训练</strong></p>
<ul>
<li>将网络按层切成不同 stage，通过 micro-batch 流水传递实现流水并行；</li>
<li>关注的是“深度方向”的并行与显存扩展。</li>
</ul></li>
<li><p><strong>后续多维并行工作（如更后来的 Megatron-DeepSpeed、3D/4D 并行框架）</strong>(<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3458817.3476209">ACM Digital Library</a>)</p>
<ul>
<li>在 Megatron TP 的基础上叠加流水并行、ZeRO 等技术，实现 DP+TP+PP 的联合搜索与调度；</li>
<li>进一步解决 trillion 级模型训练中的通信、内存和调度问题。</li>
</ul></li>
</ol>
<p><strong>问题定义维度：</strong></p>
<ul>
<li>GPT-2：能否通过单机/少机训练更大的语言模型，从而获得更强的 zero-shot 能力？</li>
<li>GPipe / PipeDream：如何通过流水并行解决“网络很深”带来的显存与吞吐问题？</li>
<li>Megatron-LM：如何通过层内张量并行解决“单层矩阵太大”带来的显存与吞吐问题？</li>
</ul>
<p><strong>方法路线关系：</strong></p>
<ul>
<li>Megatron-LM 与 GPT-2：前者在系统和并行策略上为 GPT-2 类架构提供扩展路径，两者是<strong>可叠加</strong>的；</li>
<li>Megatron-LM 与 GPipe/PipeDream：一个在层内、一个在层间，本质上是<strong>正交维度</strong>，可以组合成 3D 并行；</li>
<li>后续 3D/4D 并行工作：更多是把 Megatron 模式化 TP 变成一个组件，同时引入自动搜索与更复杂调度。</li>
</ul>
<h3 id="个人观点">7.1 个人观点</h3>
<p>从整体论证方式看，Megatron-LM 是一篇很“工程味”的系统论文：基线强、实验扎实、结果清晰。但站在今天的角度，会觉得有几个可以加强的点：</p>
<ul>
<li>baseline 主要集中在“无 TP 的数据并行”和简单的模型并行上，如果能有更多与当时其它并行方法（GPipe/PipeDream）的正面对比，会让“为什么选 TP”这一点更加有说服力。</li>
<li>对通信成本的分析偏经验：依赖实测 FLOPs 利用率和扩展效率曲线，对“网络条件变化时 TP 是否仍然有优势”缺少更系统的解释。</li>
<li>关于架构稳定性（LayerNorm 等）的部分，如果能配合一些梯度传播或谱分析，抽象出更一般的原则，会更容易被后续工作继承和推广。</li>
</ul>
<p>如果让我基于这篇论文再写一版“Megatron-LM 202X”，在保留核心 TP 设计不变的前提下，我会倾向于：</p>
<ul>
<li>增加一个简洁的“通信 vs 计算”性能模型，分析不同网络条件下 TP 的适用边界；</li>
<li>和代表性的多维并行方案做 head-to-head 对比（包括吞吐、显存、复杂度）；</li>
<li>把关于 LN 位置和大模型稳定性的经验，上升为更通用的指导原则，以便迁移到其他架构上。</li>
</ul>
<hr />
<h2 id="八在实际训练栈中如何落地">八、在实际训练栈中如何落地？</h2>
<p>从工程实践的角度，如果要把 Megatron-LM 的思想引入到自己的大规模训练栈（如 Megatron 系列、DeepSpeed、vLLM 或自研框架），大致需要考虑以下几个方面：(<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.08053">arXiv</a>)</p>
<h3 id="数据打包与前向通路">1. 数据打包与前向通路</h3>
<ul>
<li>保持原有 DataLoader 基本不变，主要要求 embedding 和输出层支持“分片权重 + 全局向量”模式；</li>
<li>在输入 embedding 阶段，引入“分片查表 + all-reduce 聚合”的逻辑。</li>
</ul>
<h3 id="并行调度与进程组">2. 并行调度与进程组</h3>
<ul>
<li>在数据并行之外增加“模型并行组”概念；</li>
<li>明确划分 tp_group / dp_group，对每个并行原语绑定到正确的 group 上；</li>
<li>调度器需要保证：组内完成 TP 通信，再进行组间 DP 同步。</li>
</ul>
<h3 id="张量并行策略落地">3. 张量并行策略落地</h3>
<ul>
<li>把 Transformer 中所有线性层改写为“行切/列切”版本，注意自注意力和 MLP 的特定组合；</li>
<li>确保激活形状和切分方向一致，避免 subtle 的 shape 对不上问题；</li>
<li>对 fused kernel 做兼容性检查（输入输出是否支持分片）。</li>
</ul>
<h3 id="kernel-与算子集成">4. kernel 与算子集成</h3>
<ul>
<li>对已有高性能 GEMM/attention kernel，确认在新的张量形状上依然能高效运行；</li>
<li>对 embedding + cross-entropy 这类组合算子设计并行版本，减少通信量；</li>
<li>考虑将部分通信与算子融合，减少中间写回。</li>
</ul>
<h3 id="通信-backend-调优">5. 通信 backend 调优</h3>
<ul>
<li>在拓扑层面尽量将 TP 组限制在高速互联范围（如单机 NVLink/NVSwitch）；</li>
<li>调试和监控 TP 组内 <code>all-reduce</code> / <code>all-gather</code> 的带宽利用、延迟，必要时限定算法（ring/tree）或设置分组策略；</li>
<li>结合 profiler 确认通信与计算的 overlap 是否有效。</li>
</ul>
<h3 id="配置与调参策略">6. 配置与调参策略</h3>
<ul>
<li>针对不同模型规模和硬件资源，设计一套 TP size × DP size 的推荐组合（比如 TP=2/4/8，DP 根据 GPU 总数自动推导）；</li>
<li>配合混合精度、gradient checkpointing、ZeRO 等方法，整体调节显存峰值和吞吐；(<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3458817.3476209">ACM Digital Library</a>)</li>
<li>对不同模型与数据集，积累经验性的“平衡点”（如“TP=8 + DP=16 比 TP=4 + DP=32 更划算还是相反”）。</li>
</ul>
<hr />
<h2 id="九值得进一步探索的研究方向">九、值得进一步探索的研究方向</h2>
<p>结合 Megatron-LM 的思路，以及当前大模型训练的发展，可以衍生出多条继续深入的方向：(<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3458817.3476209">ACM Digital Library</a>)</p>
<h3 id="方向一多维并行的自动规划与调度">方向一：多维并行的自动规划与调度</h3>
<p>在 TP 之外引入 DP、PP、序列并行、专家并行后，人肉调参成本极高。如何在给定模型结构和集群拓扑的前提下，自动搜索 TP/PP/DP 等多维并行组合，并生成可执行的并行计划，是一个高度工程化又非常有价值的问题。后续的一些工作（如自动混合并行框架）就在尝试解决这类问题。</p>
<h3 id="方向二拓扑感知的张量并行">方向二：拓扑感知的张量并行</h3>
<p>Megatron-LM 默认的是规整、带宽充足的拓扑。现实中经常遇到异构 GPU、网络不对称、多集群环境。为此可以考虑：</p>
<ul>
<li>根据物理拓扑划分 TP 组，尽量把高频通信限制在带宽更好的区域；</li>
<li>对于跨节点通信较慢的场景，探索新的通信规约方式或部分重新布局模型分片；</li>
<li>将拓扑信息纳入并行规划中，使 TP 不再“盲目均分”。</li>
</ul>
<h3 id="方向三张量并行与稀疏结构协同">方向三：张量并行与稀疏结构协同</h3>
<p>面对 MoE、稀疏注意力、大规模检索增强模型等结构，传统的“dense 行/列切分”不再直接适用。研究如何在这些结构中设计 TP：</p>
<ul>
<li>对 MoE，可以在专家维度和 hidden 维度上同时考虑切分与负载均衡；</li>
<li>对稀疏注意力，可以探索“块稀疏 + TP”联合设计，使分片后的 attention 仍然高效。</li>
</ul>
<h3 id="方向四通信与计算的深度融合">方向四：通信与计算的深度融合</h3>
<p>Megatron-LM 已经通过并行 cross-entropy 展示了“算子 + 通信”融合的威力。进一步可以研究：</p>
<ul>
<li>如何在 MLP/attention 内部引入算子级通信融合，减少显式 <code>all-reduce</code> 调用；</li>
<li>在编译器或图优化层次自动识别和替换“通信 + 算子”模式；</li>
<li>在硬件支持更强通信 offload 的情况下，重新设计 TP 的通信路径。</li>
</ul>
<h3 id="方向五从经验现象到理论理解">方向五：从经验现象到理论理解</h3>
<p>LayerNorm 位置在大模型训练中的影响，目前在实践上已经有很多共识，但理论分析仍比较缺乏。可以考虑：</p>
<ul>
<li>基于梯度传播和残差网络理论，分析不同 LN 布局对深层网络稳定性的影响；</li>
<li>研究在大模型 regime 下，哪些架构修改能系统性减少梯度爆炸/消失与训练不稳定问题；</li>
<li>将这些理论结果反馈到新架构设计中，形成一套“为大规模训练友好”的设计规范。</li>
</ul>
<hr />
<h2 id="十知识图谱思维链">十、知识图谱思维链</h2>
<p>从一个偏“大模型系统工程 + 并行训练”的知识图谱视角来看，Megatron-LM 这篇论文主要对以下几个板块有直接补链或补强作用：(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</p>
<ul>
<li><p><strong>并行与调度</strong></p>
<ul>
<li>明确提出并验证了层内张量并行（Tensor Parallelism）的具体拆分方式；</li>
<li>将“模型并行组 + 数据并行组”的组织模式固化成一个可复用范式。</li>
</ul></li>
<li><p><strong>通信与集体操作</strong></p>
<ul>
<li>将层间通信“规整化”为固定模式的 <code>all-reduce</code> / <code>all-gather</code>，减少了 point-to-point 通信；</li>
<li>为后续通信与计算 overlap、通信算法优化提供了稳定的模式基础。</li>
</ul></li>
<li><p><strong>kernel 与算子优化</strong></p>
<ul>
<li>通过约束张量切分方式，间接为写高性能 GEMM/attention/fused kernel 创造了统一的形状假设；</li>
<li>并行 cross-entropy 是“将损失计算与通信融合”的经典例子。</li>
</ul></li>
<li><p><strong>模型结构与架构设计</strong></p>
<ul>
<li>BERT 实验里对 LayerNorm 位置的探索，直接推动了 Pre-LN Transformer 的普及；</li>
<li>说明在 multi-billion 参数 regime 下，结构细节不再是“小 tweak”，而是训练成败的关键变量。</li>
</ul></li>
<li><p><strong>内存管理与显存优化</strong></p>
<ul>
<li>通过 TP 把权重与激活分摊到多卡，本质上是一种“用并行换显存”的策略；</li>
<li>与后续 ZeRO、activation checkpointing 等技术组合后，构成完整的大模型显存解决方案。</li>
</ul></li>
<li><p><strong>数据与预处理</strong></p>
<ul>
<li>论文虽未过多展开数据管线细节，但通过混合多任务、多语料的大规模训练，展示了“统一大模型 + 多任务训练”的基本范式。</li>
</ul></li>
</ul>
<h3 id="个人收获与反思">10.1 个人收获与反思</h3>
<p>对我个人的“知识图谱思维链”来说，Megatron-LM 带来的最大收获有两点：</p>
<p>第一，它把“层内张量并行”从一个模糊概念，变成了可以直接编码的设计模式：</p>
<blockquote>
<p><strong>线性层只做两种拆分：按列切或按行切；注意力按 head 切；在少数几处做 all-reduce/all-gather。</strong></p>
</blockquote>
<p>这让大模型并行训练从“一堆 ad-hoc trick”变成了一套相对规范、可复用的工程模板。</p>
<p>第二，它让我更清楚地看到：<strong>系统设计和模型设计在大模型 regime 下是高度耦合的</strong>。 张量并行的结构决定了通信模式和 kernel 形状，LayerNorm 的位置决定了在这个 regime 下能不能训稳；任何一端的设计，都不能脱离另一端孤立地看待。</p>
<blockquote>
<p><strong>整体评价：</strong> 从大模型训练的历史线上看，《Megatron-LM》是把“多亿到几十亿参数的 Transformer 训练”从概念变成工程现实的一块关键拼图。它用极其简单、规则的张量并行设计，给出了一个可以被广泛继承的基础范式，也为后续 3D/4D 并行和 trillion 级模型训练提供了出发点。如果你在搭建或理解现代大模型训练框架，Megatron-LM 仍然是一篇值得完整阅读和细品的基础论文。</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/paper/" rel="tag"># paper</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/09/07/other/token/" rel="prev" title="token 简介">
      <i class="fa fa-chevron-left"></i> token 简介
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88"><span class="nav-number">1.</span> <span class="nav-text">一、论文速览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84%E6%A6%82%E8%A7%88"><span class="nav-number">2.</span> <span class="nav-text">二、论文结构概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E6%96%B9%E6%B3%95%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%8B%86%E8%A7%A3"><span class="nav-number">3.</span> <span class="nav-text">三、方法与系统设计拆解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E4%B8%80%E8%A7%88"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 核心模块一览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%8E%E6%8E%A7%E5%88%B6%E6%B5%81%E7%A4%BA%E6%84%8F%E6%96%87%E5%AD%97%E7%89%88"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 数据流与控制流示意（文字版）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%81%87%E8%AE%BE%E4%B8%8E%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 关键假设与适用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F%E4%B8%8E%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.</span> <span class="nav-text">四、建模方式与评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%A2%E5%BC%8F%E5%8C%96%E7%9A%84"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 问题是如何形式化的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 核心评估指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E4%B8%BB%E8%A6%81%E5%AE%9E%E9%AA%8C%E5%8F%91%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">五、主要实验发现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%9B%BE%E8%A1%A8%E8%A7%A3%E8%AF%BB"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 关键图表解读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E4%BC%98%E7%82%B9%E4%B8%8E%E5%B1%80%E9%99%90"><span class="nav-number">6.</span> <span class="nav-text">六、优点与局限</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%AE%E7%82%B9strengths"><span class="nav-number">6.1.</span> <span class="nav-text">亮点（Strengths）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%99%90limitations"><span class="nav-number">6.2.</span> <span class="nav-text">局限（Limitations）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E4%B8%9A%E5%86%85%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E5%AF%B9%E6%AF%94"><span class="nav-number">7.</span> <span class="nav-text">七、业内相关工作对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E8%A7%82%E7%82%B9"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 个人观点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E5%9C%A8%E5%AE%9E%E9%99%85%E8%AE%AD%E7%BB%83%E6%A0%88%E4%B8%AD%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0"><span class="nav-number">8.</span> <span class="nav-text">八、在实际训练栈中如何落地？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%89%93%E5%8C%85%E4%B8%8E%E5%89%8D%E5%90%91%E9%80%9A%E8%B7%AF"><span class="nav-number">8.1.</span> <span class="nav-text">1. 数据打包与前向通路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E8%B0%83%E5%BA%A6%E4%B8%8E%E8%BF%9B%E7%A8%8B%E7%BB%84"><span class="nav-number">8.2.</span> <span class="nav-text">2. 并行调度与进程组</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5%E8%90%BD%E5%9C%B0"><span class="nav-number">8.3.</span> <span class="nav-text">3. 张量并行策略落地</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kernel-%E4%B8%8E%E7%AE%97%E5%AD%90%E9%9B%86%E6%88%90"><span class="nav-number">8.4.</span> <span class="nav-text">4. kernel 与算子集成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E4%BF%A1-backend-%E8%B0%83%E4%BC%98"><span class="nav-number">8.5.</span> <span class="nav-text">5. 通信 backend 调优</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E4%B8%8E%E8%B0%83%E5%8F%82%E7%AD%96%E7%95%A5"><span class="nav-number">8.6.</span> <span class="nav-text">6. 配置与调参策略</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E5%80%BC%E5%BE%97%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8E%A2%E7%B4%A2%E7%9A%84%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91"><span class="nav-number">9.</span> <span class="nav-text">九、值得进一步探索的研究方向</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%90%91%E4%B8%80%E5%A4%9A%E7%BB%B4%E5%B9%B6%E8%A1%8C%E7%9A%84%E8%87%AA%E5%8A%A8%E8%A7%84%E5%88%92%E4%B8%8E%E8%B0%83%E5%BA%A6"><span class="nav-number">9.1.</span> <span class="nav-text">方向一：多维并行的自动规划与调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%90%91%E4%BA%8C%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E7%9A%84%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C"><span class="nav-number">9.2.</span> <span class="nav-text">方向二：拓扑感知的张量并行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%90%91%E4%B8%89%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E4%B8%8E%E7%A8%80%E7%96%8F%E7%BB%93%E6%9E%84%E5%8D%8F%E5%90%8C"><span class="nav-number">9.3.</span> <span class="nav-text">方向三：张量并行与稀疏结构协同</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%90%91%E5%9B%9B%E9%80%9A%E4%BF%A1%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%9A%84%E6%B7%B1%E5%BA%A6%E8%9E%8D%E5%90%88"><span class="nav-number">9.4.</span> <span class="nav-text">方向四：通信与计算的深度融合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E5%90%91%E4%BA%94%E4%BB%8E%E7%BB%8F%E9%AA%8C%E7%8E%B0%E8%B1%A1%E5%88%B0%E7%90%86%E8%AE%BA%E7%90%86%E8%A7%A3"><span class="nav-number">9.5.</span> <span class="nav-text">方向五：从经验现象到理论理解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%80%9D%E7%BB%B4%E9%93%BE"><span class="nav-number">10.</span> <span class="nav-text">十、知识图谱思维链</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E6%94%B6%E8%8E%B7%E4%B8%8E%E5%8F%8D%E6%80%9D"><span class="nav-number">10.1.</span> <span class="nav-text">10.1 个人收获与反思</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">PePe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备2024078386号 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PePe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
<script defer src="https://events.vercount.one/js"></script>

</body>
</html>
