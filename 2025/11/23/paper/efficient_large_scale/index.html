<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yaopepe.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"post","display":"post","padding":18,"offset":12,"onmobile":false,"width_dual_column":240},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="Megatron-LM 三维并行实践解析">
<meta property="og:type" content="article">
<meta property="og:title" content="Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM">
<meta property="og:url" content="https://yaopepe.com/2025/11/23/paper/efficient_large_scale/index.html">
<meta property="og:site_name" content="果冻甜甜的">
<meta property="og:description" content="Megatron-LM 三维并行实践解析">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-11-23T03:46:23.981Z">
<meta property="article:modified_time" content="2025-11-23T06:28:21.874Z">
<meta property="article:author" content="PePe">
<meta property="article:tag" content="paper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yaopepe.com/2025/11/23/paper/efficient_large_scale/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM | 果冻甜甜的</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">果冻甜甜的</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-首页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

<div class="site-meta-counts" style="font-size:.9em;opacity:.85;margin-top:.25rem;display:flex;gap:.75rem;flex-wrap:wrap">
  <span class="post-meta-item">
    <i class="fa fa-eye"></i>
    <span class="post-meta-item-text">总访问量</span>
    <span id="vercount_value_site_pv">0</span>
  </span>

  <span class="post-meta-item">
    <i class="fa fa-file"></i>
    <span class="post-meta-item-text">总文章数</span>
    <span id="total_posts_count">13</span>
  </span>
</div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://yaopepe.com/2025/11/23/paper/efficient_large_scale/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="PePe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="果冻甜甜的">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-11-23 11:46:23 / Modified: 14:28:21" itemprop="dateCreated datePublished" datetime="2025-11-23T11:46:23+08:00">2025-11-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

<span class="post-meta-item">
  <span class="post-meta-item-icon"><i class="fa fa-eye"></i></span>
  <span class="post-meta-item-text">Views:</span>
  <span id="vercount_value_page_pv">0</span>  
</span>


            <div class="post-description">Megatron-LM 三维并行实践解析</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>
<hr />
<!-- toc -->
<hr />
<h2 id="一论文速览">一、论文速览</h2>
<p>这篇 SC’21 论文聚焦的核心问题是：在上千块 GPU 的集群上，如何高效训练 100B～1T 级别的 Transformer 语言模型，同时既不被显存限制卡死，又不过度浪费算力在通信和流水空泡上。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
<p>作者提出了一套组合式并行方案 PTD-P：在单机内做张量并行（Tensor MP），跨机做流水线并行（Pipeline MP），最外层叠加数据并行（DP），并配套新的 interleaved 1F1B 流水调度以压缩 pipeline bubble。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
<p>在一台台 DGX A100 组成的集群上，这套方案把 1T 参数 GPT 模型的训练迭代做到了 3072 块 GPU 上总计 502 PFLOP/s 的吞吐，单卡约 163 TFLOP/s，相当于 A100 理论峰值的约 52%。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
<p>论文最后给出了一些非常工程向的“选型指南”：TP/PP/DP 比例如何搭配、micro-batch 如何选、通信拓扑和并行策略如何适配，为之后的大规模 LLM 训练实践基本定了“教科书级”的基准。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
<h2 id="二论文结构">二、论文结构</h2>
<ol type="1">
<li><p><strong>引言与问题背景</strong> 说明大模型训练在显存容量与算力需求上的矛盾，回顾已有的 TP / PP / DP 工作（Megatron、GPipe、PipeDream、ZeRO 等），并点出这些方法在“上千 GPU 规模”时的根本瓶颈，适合想快速知道“为什么要搞 PTD-P” 的读者先读。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>并行模式综述与 PTD-P 总体设计</strong> 系统性地讲解数据并行、流水并行、张量并行三种模式的优缺点，并给出三者组合（PTD-P）的高层结构示意与实践经验，是理解整体系统架构与进程组布局的关键部分。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>流水并行调度：GPipe、PipeDream-Flush 与 Interleaved 1F1B</strong> 详细分析不同 pipeline 调度的 bubble 大小、激活显存占用与通信量，并给出 interleaved 1F1B 的新调度及它在吞吐上的收益，是本文理论分析的核心。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>张量并行与通信优化</strong> 回顾 Megatron-LM 的张量并行拆分方式，说明在多机多卡环境下如何把 TP 局限在单机内部，配合 InfiniBand 等跨节点通信优化，是实际把代码改对的工程指南。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>实验评估：从 1B 到 1T 的缩放实证</strong> 展示不同 TP/PP/DP 配置下的吞吐与扩展效率，对比 ZeRO-3 等方案，以及在 175B / 530B / 1T 模型上的性能数据，是最值得工程人员细读对标自己集群的一节。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>相关工作与小结</strong> 将本工作与 GPipe、PipeDream、ZeRO 等方法对比，强调自身的定位（严格同步语义 + 三维并行 + 工程落地），适合作为写自己方案时的“Related Work 模板”。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
</ol>
<blockquote>
<p>核心思想：通过在单机内做张量并行、跨机做流水并行并叠加数据并行的 PTD-P 三维并行架构，再配合 interleaved 1F1B 流水调度和通信优化，可以在保持严格同步语义和有限显存占用的前提下，把 GPT 类语言模型高效扩展到 1T 参数和上千 GPU 规模。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
</blockquote>
<hr />
<h2 id="三方法与系统设计">三、方法与系统设计</h2>
<p>整体思路可以概括为：<strong>在给定集群拓扑（DGX + NVLink + InfiniBand）的前提下，用最适合拓扑的方式组合 TP / PP / DP，并通过新流水调度最大化算力利用率，同时把跨节点通信压力压到最低</strong>。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
<p>作者重点解决了几个子问题：</p>
<ul>
<li><strong>子问题 1：</strong> 如何组合 TP / PP / DP，在有限显存下支撑 1T 级模型，又避免在上千 GPU 时被通信拖垮？</li>
<li><strong>子问题 2：</strong> 传统 GPipe/1F1B 调度的 pipeline bubble 过大，如何在不放弃严格同步语义的前提下进一步压缩 bubble？</li>
<li><strong>子问题 3：</strong> 在现实集群拓扑中（多机多卡、NVLink + InfiniBand），如何聪明地分配 TP/PP 维度，减少“跨节点 all-reduce”这种昂贵通信？</li>
<li><strong>子问题 4：</strong> 在实际训练中，如何通过 micro-batch / global batch / activation recompute 等超参调节，获取更高的吞吐？(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul>
<h3 id="核心模块一览">3.1 核心模块一览</h3>
<p>以下模块名是结合论文内容与 Megatron 实现的工程拆解，不是原文的 section 名称：</p>
<ul>
<li><p><strong>PTD-P 三维并行布局模块</strong>：负责把全集群划分为数据并行组、流水并行组和张量并行组，并在 Megatron 中映射为一系列进程组（data / model / pipeline groups），解决“算力和显存如何在维度之间分配”的问题。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>张量并行 Transformer 层模块</strong>：沿用 Megatron-LM 的列并行 / 行并行线性层设计，在多 GPU 上分片 QKV / FFN 权重，并插入必要的 all-reduce / all-gather 通信，解决“单层权重过大，单卡放不下”的问题。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>流水并行切分与调度模块</strong>：把 N 层 Transformer 均匀切分为多个 pipeline stage，并实现 GPipe、PipeDream-Flush（1F1B）和 interleaved 1F1B 三套调度逻辑，解决“多机跨层串行导致闲置”的问题。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>数据并行与梯度聚合模块</strong>：在每个 PT 配置下再复制若干数据并行副本，通过高效的 data-parallel all-reduce（典型就是 NCCL AllReduce）同步梯度，解决“大 batch 训练稳定性与吞吐”的问题。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>显存与通信优化模块</strong>：利用混合精度、激活重计算、通信 overlap 和拓扑感知映射，保证：1）绝大部分 kernel 处于 compute-bound 状态；2）数据并行 / 流水并行通信尽量在计算之下“埋掉”。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>并行配置与经验准则模块</strong>：论文最后总结的“经验公式”和 heuristics，用来指导如何选择 TP/PP/DP 因子、micro-batch 大小等，实际就是一套“人肉 auto-parallel tuner”。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
</ul>
<h3 id="数据流与控制流">3.2 数据流与控制流</h3>
<p>从工程视角看，一次训练迭代可以分解为如下步骤（可以直接据此画时序图或 Mermaid 流程图）：</p>
<ol type="1">
<li><p><strong>数据预处理与分片</strong></p>
<ul>
<li>文本数据离线分词、chunk 化为固定长度序列（例如 GPT 风格的 packed dataset）。</li>
<li>训练前通过 index mapping 把全局样本索引按数据并行 rank 均匀切分，形成每个 DP rank 的本地 shard。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
<li><p><strong>DataLoader + DistributedSampler</strong></p>
<ul>
<li>各 DP rank 使用分布式 Sampler 迭代自己的 shard，得到一个 global batch。</li>
<li>global batch 被进一步拆分为 <span class="math inline">\(m\)</span> 个 micro-batch，用于流水并行的管线填充。</li>
</ul></li>
<li><p><strong>三维并行输入映射</strong></p>
<ul>
<li><p>对于某个 DP rank 内的一个 micro-batch：</p>
<ul>
<li>沿流水线维度（PP）把 micro-batch 交给第一个 stage。</li>
<li>沿张量并行维度（TP），每个张量分片只接收自己那一份输入张量（例如列并行线性层每卡拿到输入的全部，但权重只是列分片）。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
</ul></li>
<li><p><strong>前向传播（interleaved 1F1B 调度）</strong></p>
<ul>
<li>进入 warmup 区段：不同 stage 执行不同数目的 forward，以填满整条 pipeline。</li>
<li>进入 steady 区段：每个 stage 按“1 个 forward + 1 个 backward”的 1F1B pattern 工作，但这里的“1 个 stage”已经被拆成多个 model chunk，形成 interleaved 时间表。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>在 TP 维度内部，前向中的线性 / attention 层会插入 all-reduce / all-gather，通常限制在单机 NVLink 内。</li>
</ul></li>
<li><p><strong>反向传播与梯度同步</strong></p>
<ul>
<li>每个 micro-batch 在管线尾部完成 loss 计算，把梯度向前一站一站传回去。</li>
<li>每个 TP 分片在本机内完成张量并行相关的 all-reduce 后，得到局部 shard 梯度。</li>
<li>DP 维度对所有 replica 的参数梯度做一次 data-parallel all-reduce，保证所有副本权重一致。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
<li><p><strong>参数更新与管线 flush</strong></p>
<ul>
<li>当本 batch 的所有 micro-batch 都完成 forward+backward 后，在 pipeline flush 位置统一做一次 optimizer step（例如 AdamW），以保持严格的同步语义。</li>
<li>由于 interleaved 1F1B 减少了 bubble，flush 发生得更早，整体 idle 时间下降。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
<li><p><strong>统计与监控</strong></p>
<ul>
<li>在训练循环中持续统计 per-GPU FLOPs、通信带宽使用、激活显存、stage 利用率等指标，用于后续调参与故障排查。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
</ol>
<h3 id="关键假设与适用范围">3.3 关键假设与适用范围</h3>
<ol type="1">
<li><p><strong>假设：集群具备高带宽、低延迟的 GPU 间互连</strong></p>
<ul>
<li>论文实验基于 NVLink/NVSwitch（单机）+ 高速 InfiniBand（跨机），数据并行和流水线通信使用了接近 TB/s 级别的有效带宽。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>若换成普通以太网或老旧互连，TP/PP 之间的最佳分配点会显著改变，甚至可能需要更重的计算-通信 overlap 或压缩，否则吞吐可能大幅跌落。</li>
</ul></li>
<li><p><strong>假设：模型结构主要是均匀堆叠的 Transformer block</strong></p>
<ul>
<li>PTD-P 和 interleaved 切分均假设各个 block 计算量接近，可以简单“均分层数”实现负载均衡。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>对于含有大量异构模块（如超大 embedding、MoE、decoder-only + 复杂头部）的模型，如果不做额外的层级重分配与 profile，容易在流水线某些 stage 出现明显瓶颈。</li>
</ul></li>
<li><p><strong>假设：采用混合精度、激活重计算等显存优化手段</strong></p>
<ul>
<li>论文的 1T 模型训练默认使用 mixed precision 和 activations recompute，否则显存很难支撑多 micro-batch + 多 stage 的组合。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>在只用 FP32 且不开重计算的环境下，pipeline 深度和 micro-batch 数量必须显著收缩，bubble 理论分析仍成立，但可选的工作点会大幅受限。</li>
</ul></li>
<li><p><strong>假设：采用严格同步的优化器语义</strong></p>
<ul>
<li>PTD-P 始终在 pipeline flush 处才做一次权重更新，不使用延迟或异步更新。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>如果改用 PipeDream-2BW 等允许 stale weights 的方案，虽然可以进一步缩短 bubble，但会引入训练稳定性和收敛行为的不确定性，需要额外实验支撑。(<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/" title="Scaling Language Model Training to a Trillion Parameters Using Megatron | NVIDIA Technical Blog">NVIDIA Developer</a>)</li>
</ul></li>
</ol>
<h3 id="数学公式与算法解读">3.4 数学公式与算法解读</h3>
<p>论文的方法部分包含了一些关于 <strong>pipeline bubble</strong> 与 <strong>interleaved 调度</strong> 的定量分析，这里选两组关键公式做解读。公式的形式忠实于原文，但讲解部分是等价重写。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
<h4 id="gpipe-调度的-pipeline-bubble-分析">3.4.1 GPipe 调度的 pipeline bubble 分析</h4>
<p><strong>原文中的公式：管线空泡占比</strong></p>
<p>在 GPipe 风格的 “all-forward-then-all-backward” 调度下，设：</p>
<ul>
<li><span class="math inline">\(m\)</span>：一个 batch 内的 micro-batch 数量。</li>
<li><span class="math inline">\(p\)</span>：pipeline stage 数（使用多少设备做流水并行）。</li>
<li><span class="math inline">\(t_f\)</span>：单个 micro-batch 的前向时间。</li>
<li><span class="math inline">\(t_b\)</span>：单个 micro-batch 的反向时间。</li>
</ul>
<p>则：</p>
<ul>
<li>批处理的理想计算时间为 $ t_{} = m (t_f + t_b) $</li>
<li>pipeline bubble 的时间为 $ t_{} = (p - 1)(t_f + t_b) $</li>
<li>bubble 占理想时间的比例为 <span class="math display">\[
\text{BubbleFrac} = \frac{t_{\text{pb}}}{t_{\text{id}}}
= \frac{p-1}{m}.
\]</span>(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul>
<p><strong>含义与直观理解</strong></p>
<ul>
<li>这组公式解决的问题：在给定 stage 数 <span class="math inline">\(p\)</span> 和 micro-batch 数 <span class="math inline">\(m\)</span> 时，pipeline 起停阶段“白白空转”的时间占比是多少。</li>
<li>关键结论：想让 bubble 小，就要让 <span class="math inline">\(m \gg p\)</span>，即“micro-batch 数远大于 pipeline 深度”。</li>
</ul>
<p><strong>直观版操作描述</strong></p>
<ul>
<li>先把一个大 batch 拆成很多 micro-batch。</li>
<li>pipeline 的最前几个时间步里，下游 device 一直在等上游的第一批数据 —— 这就是前半段 bubble。</li>
<li>等所有 micro-batch 都流完，最后几个时间步里，上游 device 已经没活干，下游还在处理尾巴 —— 这是后半段 bubble。</li>
<li>总体来说，bubble 的长度就是“两端各空转 <span class="math inline">\((p-1)\)</span> 步”的时间之和，平均到整个 batch 上就是 <span class="math inline">\(\frac{p-1}{m}\)</span>。</li>
</ul>
<h4 id="interleaved-1f1b-调度的-bubble-改进">3.4.2 Interleaved 1F1B 调度的 bubble 改进</h4>
<p><strong>原文中的公式：interleaved 之后的 bubble 占比</strong></p>
<p>在 interleaved 1F1B 调度中，每块 GPU 不只负责一段连续层，而是被切成 <span class="math inline">\(v\)</span> 个包含更少层的“model chunks”，换句话说 <strong>每个 device 上有 <span class="math inline">\(v\)</span> 个 pipeline stage</strong>。</p>
<p>在这样的情况下，论文给出的结果是（这里形式上等价于原文的推导）：(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
<ul>
<li>每个 chunk 的前向 / 反向时间近似变为 <span class="math inline">\(t_f / v\)</span>、<span class="math inline">\(t_b / v\)</span>。</li>
<li>bubble 时间变为： <span class="math display">\[
t^{\text{int}}_{\text{pb}} = \frac{(p-1)(t_f + t_b)}{v}
\]</span></li>
<li>对应的 bubble 占比为： <span class="math display">\[
\text{BubbleFrac}^{\text{int}}
= \frac{t^{\text{int}}*{\text{pb}}}{t*{\text{id}}}
= \frac{1}{v} \cdot \frac{p-1}{m}.
\]</span></li>
</ul>
<p><strong>含义与直观理解</strong></p>
<ul>
<li>相当于把原来的“<span class="math inline">\(p\)</span> 个 big-stage pipeline”细分成“<span class="math inline">\(p \cdot v\)</span> 个小 stage”，但这些小 stage 被“打包分配”到同一块 GPU 上顺序执行。</li>
<li>时间轴上，pipe flush 会更早地发生，相当于“用更密集的计算块填补了原来两端的空洞”，bubble 被缩短了约 <span class="math inline">\(v\)</span> 倍。</li>
</ul>
<p><strong>代价与权衡</strong></p>
<ul>
<li>这并不是免费的：由于一个 micro-batch 要经过更多 stage，stage 之间的激活通信次数也会增加 <span class="math inline">\(v\)</span> 倍。论文指出，对应的通信量也线性放大，需要依靠多网卡 / 拓扑感知通信把代价压下去。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul>
<h4 id="训练时间估算公式">3.4.3 训练时间估算公式</h4>
<p>论文与官方博客进一步给出一个“估算总训练时间”的简单公式（对大模型常见）：(<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/" title="Scaling Language Model Training to a Trillion Parameters Using Megatron | NVIDIA Technical Blog">NVIDIA Developer</a>)</p>
<p>设：</p>
<ul>
<li><span class="math inline">\(P\)</span>：模型参数量；</li>
<li><span class="math inline">\(T\)</span>：训练 token 总数；</li>
<li><span class="math inline">\(N\)</span>：GPU 数量；</li>
<li><span class="math inline">\(X\)</span>：单卡实际吞吐（TFLOP/s）；</li>
</ul>
<p>则训练时间约为： <span class="math display">\[
\text{TrainTime(sec)} \approx 8 \cdot \frac{T \cdot P}{N \cdot X}.
\]</span></p>
<p>这个 <span class="math inline">\(8\)</span> 是把一次前向 + 反向的 FLOPs 系数折合后的近似因子（对 GPT 类模型常见估计）。在工程实践里，这个公式可以用来做“<strong>预算级</strong>”估算：给定模型规模、token 数和集群配置，大致判断要训几周。</p>
<hr />
<p><strong>与常见训练栈的对应关系</strong></p>
<p>如果把上面的模块放进“我的训练栈（如 Megatron / DeepSpeed / vLLM 等）”里，大致可以对应到：</p>
<ul>
<li><strong>DataLoader / 数据预处理层</strong>：负责 global batch 拆分、分布式采样、packed dataset 构建，对应论文里的数据分片与 micro-batch 拆分逻辑。</li>
<li><strong>并行调度层（launcher + parallel engine）</strong>：负责构建 PTD-P 的进程组、决定 TP/PP/DP 因子和 rank 映射，实现在集群上的 3D 并行布局。</li>
<li><strong>模型定义层（nn.Module + sharded layers）</strong>：将 Transformer 层改写为张量并行版本（列并行/行并行线性、分片 attention 等）。</li>
<li><strong>通信 backend 层（NCCL / RCCL / 自研）</strong>：实现数据并行 all-reduce、张量并行 all-reduce / all-gather 以及流水线 stage 之间的 point-to-point 传输。</li>
<li><strong>kernel / 算子优化层</strong>：为大矩阵乘、softmax、layernorm 等提供高效 kernel，并配合 activation recompute，让大部分 step 处于 compute-bound。</li>
<li><strong>监控与自动调参层</strong>：收集 per-stage 吞吐、bubble 占比、通信带宽等指标，根据论文 heuristics 自动搜索合适的 TP/PP/DP 与 micro-batch。</li>
</ul>
<hr />
<h2 id="四建模方式与评估指标">四、建模方式与评估指标</h2>
<h3 id="问题是如何形式化的">4.1 问题是如何形式化的？</h3>
<p>从系统角度看，作者关心的核心优化目标是：</p>
<blockquote>
<p>在给定的 GPU 数量、互连拓扑和模型参数规模下，<strong>最小化训练时间</strong> / <strong>最大化实际 FLOPs 利用率</strong>，同时满足显存约束和严格同步语义。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p>
</blockquote>
<p>可以用两个层次来理解建模方式：</p>
<ol type="1">
<li><p><strong>算力层面</strong>：</p>
<ul>
<li>对于 GPT 类模型，一次前向+反向的 FLOPs 大约和 “参数量 × 序列长度 × batch 大小” 成正比。</li>
<li>若单卡吞吐为 <span class="math inline">\(X\)</span> TFLOP/s，总 FLOPs 为 <span class="math inline">\(8TP\)</span>（前面公式中的近似），目标就是让实际流水线调度 + 通信开销下的有效 <span class="math inline">\(X\)</span> 尽可能接近硬件峰值。(<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/" title="Scaling Language Model Training to a Trillion Parameters Using Megatron | NVIDIA Technical Blog">NVIDIA Developer</a>)</li>
</ul></li>
<li><p><strong>并行策略层面</strong>：</p>
<ul>
<li>给定 TP/PP/DP 三个并行度 <span class="math inline">\((t, p, d)\)</span>，以及 micro-batch 数 <span class="math inline">\(m\)</span>，可以分析对应的 bubble 比例、激活显存占用和通信量，并通过实验测量实际吞吐。</li>
<li>论文没有构造一个完整的形式化最优化模型，而是提供一系列经验规则来选取 “近似最优” 的 <span class="math inline">\((t, p, d, m)\)</span> 组合。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
</ol>
<p>主要简化包括：</p>
<ul>
<li>把大部分 kernel 看成 compute-bound，忽略细粒度 cache 行为等复杂因素；</li>
<li>把 pipeline 调度的代价抽象为 bubble + 通信，两者以简单参数（如 <span class="math inline">\(p, v, m\)</span>）来刻画；</li>
<li>假设相同 stage 内的层计算量基本均匀，可忽略 load imbalance。</li>
</ul>
<h3 id="核心评估指标">4.2 核心评估指标</h3>
<p>论文在系统评估中使用了以下几个关键指标（我用工程视角做了重新组织）：</p>
<ol type="1">
<li><p><strong>单卡实际吞吐（TFLOP/s）与峰值占比</strong></p>
<ul>
<li>统计包含计算和通信在内的 end-to-end FLOPs 利用率，例如 1T 模型在 3072 A100 上达到了 163 TFLOP/s / GPU ≈ 52% 峰值。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>这是直接衡量“这套并行+调度把硬件压榨得怎么样”的核心指标。</li>
</ul></li>
<li><p><strong>聚合吞吐（PetaFLOP/s）与弱扩展效率</strong></p>
<ul>
<li>随着 GPU 数从几十扩展到几千，测量总 petaFLOP/s 与理想线性扩展的偏差。(<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/" title="Scaling Language Model Training to a Trillion Parameters Using Megatron | NVIDIA Technical Blog">NVIDIA Developer</a>)</li>
<li>用于判断这套方案在大规模集群上的可扩展性，直接对应“能不能训 1T 甚至更大模型”。</li>
</ul></li>
<li><p><strong>pipeline bubble 占比</strong></p>
<ul>
<li>使用前面推导的 <span class="math inline">\(\frac{p-1}{m}\)</span> 与 <span class="math inline">\(\frac{1}{v}\frac{p-1}{m}\)</span> 等公式来估算不同调度下的理论 bubble，并通过时序图（时间轴）验证。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>与流水深度、micro-batch 数和 interleaved 度数直接对应，是理解为什么 interleaved 1F1B 有收益的关键。</li>
</ul></li>
<li><p><strong>显存占用（参数、激活、优化器状态）</strong></p>
<ul>
<li>对比 GPipe vs 1F1B vs interleaved 等不同流水调度下的激活显存峰值；同时与 ZeRO-3 等“切参数+优化器”的方案相比。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>帮助读者理解“显存是被参数吃掉了还是被激活吃掉了”，对实际工程里调 activation recompute、checkpoint 非常有指导意义。</li>
</ul></li>
<li><p><strong>通信带宽消耗（pipeline / data parallel 两类）</strong></p>
<ul>
<li>论文给出了训练 1T 模型时 pipeline 通信和 data-parallel 通信的有效 bisection 带宽（如数百 GB/s vs 数十 TB/s 级别），以展示通信已经是第一等公民。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>这一指标与集群网络配置（网卡数量、拓扑、拥塞控制）强相关，是迁移到自己机房时必须核对的数字。</li>
</ul></li>
</ol>
<hr />
<h2 id="五主要实验发现">五、主要实验发现</h2>
<ul>
<li><p><strong>三维并行（PTD-P）在大规模集群上实现了接近线性的扩展</strong>：在 3072 块 A100 上，1T 参数 GPT 模型的总吞吐达到 502 PFLOP/s，单卡约 163 TFLOP/s，显示在高带宽互连下 TP+PP+DP 的组合可以充分吃满硬件。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>interleaved 1F1B 调度在多种配置下带来了 10% 以上的吞吐提升</strong>：在保持显存占用接近不变的前提下，通过把每块 GPU 切成多个 model chunk，缩短了 pipeline flush 的时间，从而减少了 idle。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>TP 与 PP 的组合方式对性能影响巨大</strong>：论文显示，一些“看起来合理”的 TP/PP 因子在上千 GPU 时会导致最多 2× 的吞吐损失，主要原因是跨节点的张量并行 all-reduce 成本过高。将 TP 限制在单机内、把跨机维度留给 PP 是实践中非常关键的经验。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>合适的 micro-batch 大小可以再挖出 10%～15% 的收益</strong>：micro-batch 太小，kernel 无法被充分填满；太大又会放大 pipeline bubble 或击穿显存。论文的实证表明，“最佳 micro-batch” 是一个强烈依赖模型规模和并行配置的超参。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
<li><p><strong>与 ZeRO-3 等纯 DP+参数切分方案相比，PTD-P 在百亿～千亿规模上有明显优势</strong>：在 175B 和 530B 模型上，与 ZeRO-3 对比，PTD-P 方案在相同设备数下吞吐高约 70%，关键差异在于减少了跨节点大规模参数同步。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
</ul>
<h3 id="关键图表解读">5.1 关键图表解读</h3>
<blockquote>
<p>下列图表描述基于论文和官方博客中的内容，具体数值以原文为准。</p>
</blockquote>
<ol type="1">
<li><p><strong>图：聚合吞吐 vs GPU 数量与模型规模</strong></p>
<ul>
<li>现象：从约 1.7B 参数模型在 32 GPU，上升到 1T 模型在 3072 GPU，总吞吐从数 PFLOP/s 提升到 502 PFLOP/s，整体扩展效率超过 100×。(<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/" title="Scaling Language Model Training to a Trillion Parameters Using Megatron | NVIDIA Technical Blog">NVIDIA Developer</a>)</li>
<li>支撑的观点：说明 PTD-P 架构在现实硬件与网络条件下可以稳当扩展到万亿级模型，为后来各种 500B / 1T 模型提供了可行性证明。</li>
</ul></li>
<li><p><strong>图：GPipe vs 1F1B vs Interleaved 1F1B 调度时间线</strong></p>
<ul>
<li><p>现象：</p>
<ul>
<li>GPipe：先执行所有 micro-batch 的 forward，再执行所有 backward，bubble 大且激活显存占用高。</li>
<li>1F1B（PipeDream-Flush）：warmup + steady 交替 F/B，bubble 与 GPipe 相同，但激活显存峰值显著降低。</li>
<li>interleaved 1F1B：把每个 device 上的层切成多个 chunk，时间轴上 flush 点明显提前，bubble 长度缩短。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
<li><p>支撑的观点：解释了为什么在相同显存预算下，interleaved 调度可以额外再吃掉一部分 bubble，从而多拿一截吞吐。</p></li>
</ul></li>
<li><p><strong>表：不同 TP/PP/DP 配置下的吞吐对比</strong></p>
<ul>
<li>现象：例如在 175B / 530B 模型上，使用更高的 TP（跨节点）会显著恶化吞吐，而增加 PP 深度并限制 TP 在单机内则能持续靠近线性扩展。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>支撑的观点：定量展示了“TP 尽量局限在单机、PP 负责跨机扩展”的实践准则，反驳了“TP 越大越好”的直觉。</li>
</ul></li>
</ol>
<p><strong>结果解读与边界</strong></p>
<p>总体来看，这些实验非常有力地支撑了论文的两个核心结论： 1）三维并行 + interleaved 调度在现实大集群上是可落地且高效的； 2）TP/PP/DP 和 micro-batch 的组合有一套可复用的经验规则。</p>
<p>但也有一些明显的边界与潜在混淆因素：</p>
<ul>
<li>实验主要基于 A100 + NVLink + 高速 InfiniBand 的“豪华配置”，在普通以太网环境下的可迁移性需要额外实验。</li>
<li>目标任务偏向 GPT 类自回归 LLM，尚未系统覆盖 MoE、encoder-decoder、多模态等架构。</li>
<li>对收敛质量与稳定性的分析相对简略（尤其是对于极大 batch、激进 pipeline 深度的设置），在“只看 throughput 不看 loss”的场景里可能会被误用。</li>
</ul>
<hr />
<h2 id="六优点与局限">六、优点与局限</h2>
<p><strong>亮点（Strengths）</strong></p>
<ul>
<li><strong>问题定义清晰且贴近工业实践</strong>：直接瞄准“如何高效训练 1T 模型”的系统问题，而不是抽象的理论模型，非常契合当下大模型训练需求。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.04473">arXiv</a>)</li>
<li><strong>方法设计系统且组合性强</strong>：通过 PTD-P 把 TP / PP / DP 有机地拼在一起，并给出 interleaved 1F1B 这样可直接在现有框架中实现的调度改进。</li>
<li><strong>分析与工程细节兼顾</strong>：既有 bubble 公式、通信量等理论分析，又给出了 network bandwidth 使用、kernel bound/ memory bound 判定等非常“工程味”的指标。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li><strong>实验规模与说服力</strong>：在 3072 A100 上训练 1T 模型的结果本身就具有很强的“示范效应”，也为后续工作提供了对标基线。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li><strong>开源实现可直接参考</strong>：基于 Megatron-LM 的公开代码让读者可以直接对照实现细节、复现实验甚至扩展自己的并行策略。(<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/Megatron-LM">GitHub</a>)</li>
</ul>
<p><strong>局限（Limitations）</strong></p>
<ul>
<li><strong>依赖高端硬件与网络环境</strong>：几乎所有关键结论都是在 NVLink + 高速 InfiniBand 的前提下给出的，对“普通机房配置”的适用性需要谨慎解读。</li>
<li><strong>模型类型相对单一</strong>：主要聚焦 GPT 类 dense Transformer，对 MoE、sparse attention、encoder-decoder 等结构缺乏系统实验。</li>
<li><strong>缺少自动并行搜索机制</strong>：虽然给出了 heuristics，但并没有类似 FlexFlow / Alpa 那样的自动探索机制，实际使用仍需要大量经验和人工调参。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li><strong>训练质量分析不够深入</strong>：更偏重系统指标（throughput、利用率等），对不同并行策略 / batch 配置下收敛速度与最终精度的影响讨论有限。</li>
<li><strong>与 ZeRO / FSDP 等参数切分技术的组合空间未完全展开</strong>：只给出了一些对比结果，但没有深入探讨“PTD-P + ZeRO-like”的可能组合。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul>
<hr />
<h2 id="七业内相关工作对比">七、业内相关工作对比</h2>
<p>这里选三类典型工作做横向对比：Megatron-LM（原始张量并行）、GPipe（流水并行）和 ZeRO 系列（数据并行 + 参数切分）。</p>
<table>
<colgroup>
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 24%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>工作</th>
<th>问题聚焦</th>
<th>方法路线</th>
<th>与本文关系与评价</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Megatron-LM (2019)</strong> (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">arXiv</a>)</td>
<td>单机多卡、显存不足时如何通过 intra-layer 张量并行训练 10B 级 Transformer</td>
<td>主要通过列并行 / 行并行线性层 + all-reduce/all-gather，在 8 GPU 内实现数十亿参数模型</td>
<td>本文在此基础上扩展到“多机+更多 GPU”，并首次系统性探索 TP 与 PP、DP 的组合，是从“单机张量并行”到“三维并行”的自然演进</td>
</tr>
<tr class="even">
<td><strong>GPipe (2019)</strong> (<a target="_blank" rel="noopener" href="https://fid3024.github.io/papers/2019%20-%20GPipe%3A%20Efficient%20Training%20of%20Giant%20Neural%20Networks%20using%20Pipeline%20Parallelism.pdf" title="GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism">fid3024.github.io</a>)</td>
<td>如何通过流水并行训练超大模型并保持同步语义</td>
<td>将模型切为多个 stage，通过 micro-batch 流水 + activation recompute 实现高效 pipeline</td>
<td>本文继承 GPipe 的同步语义与 batch splitting 思路，但在调度上改用 PipeDream-Flush / interleaved 1F1B，以降低激活显存和 bubble，是更工程化的“第二代流水方案”</td>
</tr>
<tr class="odd">
<td><strong>ZeRO / ZeRO-Offload / ZeRO-3</strong> (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.06840">arXiv</a>)</td>
<td>通过参数 / 梯度 / 优化器状态切分 + offload 在 DP 框架下支撑超大模型</td>
<td>在数据并行维度上对参数与优化器进行细粒度分片，并可将部分状态 offload 到 CPU/NVMe</td>
<td>ZeRO 系列强调“DP+参数切分”路线，本工作展示了在 175B/530B 规模上 PTD-P 对 ZeRO-3 的性能优势；两者在理念上是互补的，后续也可以探索 PTD-P 与 ZeRO/FSDP 的组合</td>
</tr>
</tbody>
</table>
<p>总体来说，这篇 SC’21 论文更像是“张量并行 + 流水并行 + 数据并行”这条路线的阶段性集大成者，与 ZeRO/FSDP 等“参数切分”路线属于可互补、可对比的两条主线。</p>
<h3 id="个人观点">7.1 个人观点</h3>
<p>从 reviewer 的视角看，这篇工作在 baseline 选择与实验设置上还是比较谨慎的：对比了 ZeRO-3 等当时主流方案，也给出了较完整的缩放曲线。但如果进一步抠细节，我会希望看到：</p>
<ul>
<li>更多关于“同等显存预算”的对比，例如在相同显存峰值而非相同设备数量下 PTD-P vs ZeRO/FSDP 的吞吐差异；</li>
<li>对训练稳定性和 sample efficiency 的更细粒度分析，尤其是极大 batch、极深 pipeline 时是否需要额外技巧（LR schedule、optimizer scaling 等）。</li>
</ul>
<p>如果由我来设计一版“升级版”实验，我可能会：</p>
<ul>
<li>加入不同网络拓扑（例如只用 100GbE、RoCE）的实验，对 PTD-P 的可迁移性做更全面的评估；</li>
<li>系统探索 “PTD-P + 参数切分（ZeRO/FSDP）” 的组合空间，看是否存在更优的 Pareto 前沿点；</li>
<li>在同一套代码框架下公开一组“标准配置”（YAML/JSON），方便社区直接对标和复现。</li>
</ul>
<hr />
<h2 id="八在实际训练栈中如何落地">八、在实际训练栈中如何落地？</h2>
<p>如果你已经有一套自己的大规模训练栈（例如基于 Megatron / DeepSpeed / vLLM 等），要引入本文方法，大致可以从以下几个方面改造：</p>
<ol type="1">
<li><p><strong>DataLoader / 数据打包与预处理</strong></p>
<ul>
<li>确保数据可以被稳定地划分为大的 global batch 和足够多的 micro-batch，以满足 <span class="math inline">\(m \gg p\)</span> 的条件。</li>
<li>对 packed dataset 做好“样本到 micro-batch”的映射和重复度控制，避免 pipeline 深度引入隐式的 data skew。</li>
</ul></li>
<li><p><strong>并行调度（TP/PP/DP 组合）</strong></p>
<ul>
<li>在 launcher 端显式引入三维并行配置：<code>tensor_parallel_size</code>, <code>pipeline_model_parallel_size</code>, <code>data_parallel_size</code>。</li>
<li>rank 映射上，优先保证：<strong>TP 维度完全落在单机内</strong>，PP 维度跨机，DP 再跨更大范围；必要时根据物理拓扑编写自定义 <code>rank -&gt; (dp,tp,pp)</code> 映射函数。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>工程风险：映射错误会直接导致“跨节点大 all-reduce”，性能大跳水。</li>
</ul></li>
<li><p><strong>张量并行策略与算子实现</strong></p>
<ul>
<li>把核心模块（QKV projection、FFN、embedding、LM head 等）改写为张量并行版本，在 TP 维度上插入必要的 all-reduce / all-gather。</li>
<li>对于“非对称模块”（例如超大词表 embedding、MoE experts），需要单独策略（如 vocabulary parallel embedding、expert parallel 等）。</li>
<li>风险：参数初始化、checkpoint load/save 都必须遵循相同分片规则，否则极易在恢复训练时踩雷。</li>
</ul></li>
<li><p><strong>流水并行调度与通信</strong></p>
<ul>
<li>在 pipeline 维度引入 stage 划分逻辑，把模型分为 <code>num_layers / pipeline_size</code> 左右的均匀块；再基于 interleaved 方案进一步把每块拆成多个 chunk。</li>
<li><p>实现 1F1B 和 interleaved 1F1B 调度器，确保：</p>
<ul>
<li>flush 点一致；</li>
<li>不同 stage 的 weight 版本在一个 batch 内保持严格同步。</li>
</ul></li>
<li><p>风险：一旦调度器实现有 bug（例如某些 micro-batch 的 F/B 顺序错位），非常难以排查，且表象往往只是“loss 不稳定”。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</p></li>
</ul></li>
<li><p><strong>通信 backend 与 overlap</strong></p>
<ul>
<li>在 NCCL 后端显式区分几类通信：TP all-reduce、DP all-reduce、PP P2P（send/recv），并给每类分配独立的 stream 与优先级。</li>
<li>尝试把 DP all-reduce 放在 backward tail 部分与部分计算重叠，把 PP P2P 与下一个 micro-batch 的 F/B 重叠。</li>
<li>风险：stream 依赖与事件（event）同步关系复杂，容易埋 race condition 或死锁。</li>
</ul></li>
<li><p><strong>显存管理与 activation recompute</strong></p>
<ul>
<li>根据论文建议，在较深 pipeline 设置下优先开启 activation recompute，把激活显存峰值从 <span class="math inline">\(O(mL)\)</span> 压缩到 <span class="math inline">\(O(p)\)</span> 级别。(<a target="_blank" rel="noopener" href="https://fid3024.github.io/papers/2019%20-%20GPipe%3A%20Efficient%20Training%20of%20Giant%20Neural%20Networks%20using%20Pipeline%20Parallelism.pdf" title="GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism">fid3024.github.io</a>)</li>
<li>对不同 module（attention / FFN / embedding）设置不同的 recompute 策略，避免把所有层都重算到导致算力浪费。</li>
<li>风险：显存碎片和 allocator 行为在大规模并行下会放大，需要仔细观测 <code>allocated / reserved / active</code> 等指标。</li>
</ul></li>
<li><p><strong>配置搜索 / 自动调参</strong></p>
<ul>
<li>把论文中的 heuristics 封装为一个“并行配置建议器”：给定模型规模、目标序列长度、设备数量，输出候选 <code>(tp, pp, dp, microbatch)</code> 组合。</li>
<li>在上线前对若干候选配置跑短程 benchmark（几十到几百 step），根据实际吞吐、通信占比、显存峰值选择最终配置。</li>
</ul></li>
</ol>
<hr />
<h2 id="九值得进一步探索的研究方向">九、值得进一步探索的研究方向</h2>
<ol type="1">
<li><p><strong>自动化三维并行搜索与代价模型</strong></p>
<ul>
<li>问题：目前 PTD-P 的配置主要基于经验和少量试验，缺少系统化的自动搜索。</li>
<li>价值：构建一个针对 TP/PP/DP + micro-batch 的代价模型，再结合图搜索或强化学习，在给定集群拓扑和模型结构下自动给出近似最优配置，可以显著降低工程人员的试错成本。(<a target="_blank" rel="noopener" href="https://deepakn94.github.io/assets/papers/thesis.pdf">Deepak Narayanan</a>)</li>
</ul></li>
<li><p><strong>与参数切分 / FSDP 的深度融合</strong></p>
<ul>
<li>问题：当前 PTD-P 和 ZeRO/FSDP 多以“谁更快”来对比，缺乏对两者互补性的系统探索。</li>
<li>价值：探索在 PTD-P 外又叠一层参数切分（例如对嵌入层或优化器状态做 FSDP/ZeRO）的混合方案，有望在保持高吞吐的同时进一步降低显存峰值，使得更大模型在更小集群上可行。(<a target="_blank" rel="noopener" href="https://www.deepspeed.ai/tutorials/zero/">DeepSpeed</a>)</li>
</ul></li>
<li><p><strong>面向非均匀模型结构的负载均衡流水并行</strong></p>
<ul>
<li>问题：现实大模型越来越“非均匀”，例如 embedding 特别大、部分 block 带 MoE、decoder head 特别重，简单的“均分层数”不再合理。</li>
<li>价值：在 PTD-P 框架下引入自动 partition（如基于 profile 的图划分），对 pipeline stage 做负载均衡，可以显著降低单 stage 成为瓶颈的概率。(<a target="_blank" rel="noopener" href="https://pacman.cs.tsinghua.edu.cn/~cwg/publication/10-1145-3620666-3651359/10-1145-3620666-3651359.pdf">pacman.cs.tsinghua.edu.cn</a>)</li>
</ul></li>
<li><p><strong>针对弱互连集群的鲁棒并行策略</strong></p>
<ul>
<li>问题：很多实际集群并没有 NVLink + 多路 InfiniBand 这种配置，如何在 100GbE 或单路 IB 上获得有意义的扩展仍不清楚。</li>
<li>价值：研究在弱互连场景下，如何调整 TP/PP/DP 的分配、加入通信压缩/稀疏 all-reduce、延迟更新等手段，使 PTD-P 能在“平价集群”上依然实用。</li>
</ul></li>
<li><p><strong>端到端训练稳定性与大 batch 收敛性研究</strong></p>
<ul>
<li>问题：pipeline 深度、interleaved 度数、micro-batch 大小都会影响有效 batch 和梯度噪声，但目前分析有限。</li>
<li>价值：系统地研究不同并行配置对 loss 曲线、泛化性能的影响，可以指导在不牺牲收敛质量的前提下更激进地推大 batch 和推高吞吐。</li>
</ul></li>
</ol>
<hr />
<h2 id="十知识图谱思维链">十、知识图谱思维链</h2>
<p>从“脑内知识图谱”的角度，这篇论文在多个方向上都起到了“连接节点”的作用：</p>
<ul>
<li><p><strong>并行与调度</strong></p>
<ul>
<li>提供了一个经典的三维并行 PTD-P 模式，把 TP/PP/DP 三种思路统一在一个框架下。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>通过 GPipe → PipeDream-Flush → interleaved 1F1B 的演进，给出了如何在保持同步语义的前提下极限压缩 pipeline bubble 的结构化方法。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
<li><p><strong>内存管理与显存优化</strong></p>
<ul>
<li>用 activation recompute + 深 pipeline 控制激活显存，把大部分显存预算留给参数和 optimizer。(<a target="_blank" rel="noopener" href="https://fid3024.github.io/papers/2019%20-%20GPipe%3A%20Efficient%20Training%20of%20Giant%20Neural%20Networks%20using%20Pipeline%20Parallelism.pdf" title="GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism">fid3024.github.io</a>)</li>
<li>与 ZeRO/FSDP 系列形成了“激活 vs 参数优化”的两条互补路线。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.06840">arXiv</a>)</li>
</ul></li>
<li><p><strong>通信与集体操作</strong></p>
<ul>
<li>明确区分了 TP all-reduce / DP all-reduce / PP P2P 三类通信，并强调拓扑感知映射对性能的重要性。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
<li>通过对 bisection bandwidth 使用的分析，把“网络”从辅助因素提升为一等公民。</li>
</ul></li>
<li><p><strong>kernel 与算子优化</strong></p>
<ul>
<li>虽然不是本文重点，但作者强调为了让训练 compute-bound，需要高效实现 GEMM、Attention、LayerNorm 等核心算子，这与后续各种 FlashAttention、fused-kernel 工作有天然连接。(<a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">people.eecs.berkeley.edu</a>)</li>
</ul></li>
<li><p><strong>模型结构与架构设计</strong></p>
<ul>
<li>默认场景是多层均匀的 GPT Transformer，这对后来的人在设计“大模型结构”时提供了一个“对 pipeline 友好”的参考范式。</li>
<li>也为后续 MoE / encoder-decoder 等非均匀架构如何嵌入 PTD-P 提供了出发点。</li>
</ul></li>
<li><p><strong>数据、预处理与打包策略</strong></p>
<ul>
<li>强调 large batch + 多 micro-batch 对流水并行的必要性，间接推动了大家在数据管线中更早地做 packed dataset、分布式 sampler 等工程优化。</li>
</ul></li>
</ul>
<h3 id="个人收获与反思">10.1 个人收获与反思</h3>
<p>对我个人而言，这篇论文最大的启发有两点：</p>
<ol type="1">
<li><p><strong>把“并行策略”和“集群拓扑”视作一个整体来优化</strong> 很多时候我们在讨论 TP/PP/DP 时会“先设定逻辑并行度，再去适配硬件”，而这篇工作反过来：它先看清楚 A100 + NVSwitch + InfiniBand 的物理结构，再设计 PTD-P 的 rank 映射和通信调度。这种“硬件驱动的软件设计”思路，对任何做大规模系统的人都很值得借鉴。</p></li>
<li><p><strong>系统工作也可以做得非常“工程可复用”</strong> 论文不仅仅给出结果，还给了清晰的经验准则和公开实现（Megatron-LM）。这使得它不仅是一个研究成果，也是一个可以直接照搬到自己训练栈的“操作手册”。对我后续设计自己训练系统（无论是基于 Megatron、还是更轻量的栈）都提供了一个非常好的模板：任何设计，都尽量沉淀为可复用的代码与 heuristics。</p></li>
</ol>
<p>在实践层面，我会考虑：</p>
<ul>
<li>在自己的训练栈中，把 pipeline 调度抽象成一个可插拔模块，尝试从最基础的 1F1B 升级到 interleaved 1F1B，观察对显存和吞吐的具体影响；</li>
<li>系统整理一套针对自己集群的 “TP/PP/DP + micro-batch 推荐表”，并加入简单的 profile 驱动机制，逐步向“自动配置”演进。</li>
</ul>
<blockquote>
<p>总体评价：这篇 SC’21 论文在“如何把 GPT 类大模型可靠地训到 1T 参数”这个问题上给出了非常系统且可落地的答案，是理解当今主流三维并行训练栈（尤其是 Megatron 系）的必读文献，更偏工程与系统优化，对做大规模训练基础设施的读者尤其有长期参考价值。</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/paper/" rel="tag"># paper</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/11/22/paper/InstructCoder/" rel="prev" title="InstructCoder: Instruction Tuning Large Language Models for Code Editing">
      <i class="fa fa-chevron-left"></i> InstructCoder: Instruction Tuning Large Language Models for Code Editing
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88"><span class="nav-number">1.</span> <span class="nav-text">一、论文速览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">二、论文结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E6%96%B9%E6%B3%95%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.</span> <span class="nav-text">三、方法与系统设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E4%B8%80%E8%A7%88"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 核心模块一览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%8E%E6%8E%A7%E5%88%B6%E6%B5%81"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 数据流与控制流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%81%87%E8%AE%BE%E4%B8%8E%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 关键假设与适用范围</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E4%B8%8E%E7%AE%97%E6%B3%95%E8%A7%A3%E8%AF%BB"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 数学公式与算法解读</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gpipe-%E8%B0%83%E5%BA%A6%E7%9A%84-pipeline-bubble-%E5%88%86%E6%9E%90"><span class="nav-number">3.4.1.</span> <span class="nav-text">3.4.1 GPipe 调度的 pipeline bubble 分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#interleaved-1f1b-%E8%B0%83%E5%BA%A6%E7%9A%84-bubble-%E6%94%B9%E8%BF%9B"><span class="nav-number">3.4.2.</span> <span class="nav-text">3.4.2 Interleaved 1F1B 调度的 bubble 改进</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%97%B6%E9%97%B4%E4%BC%B0%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="nav-number">3.4.3.</span> <span class="nav-text">3.4.3 训练时间估算公式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F%E4%B8%8E%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.</span> <span class="nav-text">四、建模方式与评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%A2%E5%BC%8F%E5%8C%96%E7%9A%84"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 问题是如何形式化的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 核心评估指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E4%B8%BB%E8%A6%81%E5%AE%9E%E9%AA%8C%E5%8F%91%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">五、主要实验发现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%9B%BE%E8%A1%A8%E8%A7%A3%E8%AF%BB"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 关键图表解读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E4%BC%98%E7%82%B9%E4%B8%8E%E5%B1%80%E9%99%90"><span class="nav-number">6.</span> <span class="nav-text">六、优点与局限</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E4%B8%9A%E5%86%85%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E5%AF%B9%E6%AF%94"><span class="nav-number">7.</span> <span class="nav-text">七、业内相关工作对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E8%A7%82%E7%82%B9"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 个人观点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E5%9C%A8%E5%AE%9E%E9%99%85%E8%AE%AD%E7%BB%83%E6%A0%88%E4%B8%AD%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0"><span class="nav-number">8.</span> <span class="nav-text">八、在实际训练栈中如何落地？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E5%80%BC%E5%BE%97%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8E%A2%E7%B4%A2%E7%9A%84%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91"><span class="nav-number">9.</span> <span class="nav-text">九、值得进一步探索的研究方向</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%80%9D%E7%BB%B4%E9%93%BE"><span class="nav-number">10.</span> <span class="nav-text">十、知识图谱思维链</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E6%94%B6%E8%8E%B7%E4%B8%8E%E5%8F%8D%E6%80%9D"><span class="nav-number">10.1.</span> <span class="nav-text">10.1 个人收获与反思</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">PePe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备2024078386号 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PePe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
<script defer src="https://events.vercount.one/js"></script>

</body>
</html>
