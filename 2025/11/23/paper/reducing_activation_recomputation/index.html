<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yaopepe.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"post","display":"post","padding":18,"offset":12,"onmobile":false,"width_dual_column":240},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"gitalk":{"enable":true,"client_id":"your-client-id","client_secret":"your-client-secret","repo":"your-repo","owner":"your-username","admin":["your-username"],"distraction_free_mode":true}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="大规模 Transformer 激活重计算的系统级优化">
<meta property="og:type" content="article">
<meta property="og:title" content="Reducing Activation Recomputation in Large Transformer Models">
<meta property="og:url" content="https://yaopepe.com/2025/11/23/paper/reducing_activation_recomputation/index.html">
<meta property="og:site_name" content="果冻甜甜的">
<meta property="og:description" content="大规模 Transformer 激活重计算的系统级优化">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-11-23T06:53:36.260Z">
<meta property="article:modified_time" content="2025-11-23T07:12:37.772Z">
<meta property="article:author" content="PePe">
<meta property="article:tag" content="paper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yaopepe.com/2025/11/23/paper/reducing_activation_recomputation/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Reducing Activation Recomputation in Large Transformer Models | 果冻甜甜的</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">果冻甜甜的</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-首页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        
      
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

<div class="site-meta-counts" style="font-size:.9em;opacity:.85;margin-top:.25rem;display:flex;gap:.75rem;flex-wrap:wrap">
  <span class="post-meta-item">
    <i class="fa fa-eye"></i>
    <span class="post-meta-item-text">总访问量</span>
    <span id="vercount_value_site_pv">0</span>
  </span>

  <span class="post-meta-item">
    <i class="fa fa-file"></i>
    <span class="post-meta-item-text">总文章数</span>
    <span id="total_posts_count">14</span>
  </span>
</div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yaopepe.com/2025/11/23/paper/reducing_activation_recomputation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="PePe">
      <meta itemprop="description" content="技术博客-分享自然语言处理、人工智能等相关知识">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="果冻甜甜的">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reducing Activation Recomputation in Large Transformer Models
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-11-23 14:53:36 / 修改时间：15:12:37" itemprop="dateCreated datePublished" datetime="2025-11-23T14:53:36+08:00">2025-11-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

<span class="post-meta-item">
  <span class="post-meta-item-icon"><i class="fa fa-eye"></i></span>
  <span class="post-meta-item-text">Views:</span>
  <span id="vercount_value_page_pv">0</span>  
</span>


            <div class="post-description">大规模 Transformer 激活重计算的系统级优化</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>
<hr />
<h2 id="一论文速览">一、论文速览</h2>
<p>这篇论文关注的大问题是：在大规模 Transformer 模型训练中，激活（activations）占用的显存越来越夸张，为了省显存普遍使用“全层激活重计算（全 checkpoint）”，但这会带来 30%–40% 的额外算力开销。 作者从 Transformer 结构出发，建立了一套近似但非常实用的“激活内存模型”，系统分析了张量并行（TP）、序列并行（SP）、流水并行（PP）对激活内存的影响，并提出两大技术：<strong>将序列并行与张量并行融合</strong>，以及<strong>对激活进行选择性重计算</strong>。 综合起来，这些方法在不增加通信量的前提下，实现了激活显存约 <strong>5×</strong> 的压缩，相比“全层重算”只保留了 <strong>~2%–7%</strong> 的计算开销；在 22B–1T 规模模型上，迭代 throughput 提升大约 <strong>30%</strong>，GPU FLOPs 利用率能稳定在 50%+。</p>
<h2 id="二论文结构">二、论文结构</h2>
<ol type="1">
<li><p><strong>引言与相关工作</strong> 介绍大模型训练中的内存瓶颈、现有并行/内存优化技术（TP/PP、ZeRO、offload、已有 SP 等），并说明本文聚焦在“模型并行 + 激活内存”这个维度。适合快速了解问题背景和与其它方案关系时阅读。</p></li>
<li><p><strong>Transformer 结构与符号约定（Section 3）</strong> 统一定义 <span class="math inline">\(s,b,h,a,L,t,p,v\)</span> 等符号，并拆开分析 self-attention 与 MLP 内部的激活结构。适合在自己做推导、对接代码实现时重点看。</p></li>
<li><p><strong>激活内存建模与并行策略（Section 4）</strong> 先给出“单层激活内存近似公式”，再依次叠加：张量并行（TP）、TP+SP、再到流水并行（PP），推导出总激活内存的闭式表达，是全文最核心的理论部分。</p></li>
<li><p><strong>选择性激活重计算（Section 5）</strong> 对比“全层重算”和“只重算 attention 中一部分算子”的差异，给出在 GPT-3 / MT-NLG 规模下的内存与 FLOPs 量级，对工程上“该 checkpoint 哪些 op”给出明确指引。</p></li>
<li><p><strong>实验评估（Section 6）</strong> 通过单层 micro benchmark + 端到端训练（22B/175B/530B/1T）验证模型与实验的一致性，报告显存占用、每层时延、迭代时间、MFU/HFU 等指标，是判断“值不值的上工程实现”的关键。</p></li>
<li><p><strong>总结与未来工作（Section 7 + Appendix）</strong> 小结两大技术（TP+SP + selective recompute）的贡献，并讨论 pipeline 首段显存碎片、自动化搜索 checkpoint 策略等未来方向。</p></li>
</ol>
<blockquote>
<p>核心思想：针对大规模 Transformer，先用解析模型精确刻画激活内存，再通过“张量并行 + 序列并行”的组合将激活均匀分摊到各设备，并只对 FLOPs 便宜但内存巨大的子算子做选择性重计算，在几乎不增加通信、极小算力开销的前提下，实现约 5× 的激活显存压缩与 30% 左右的吞吐提升。</p>
</blockquote>
<hr />
<h2 id="三方法与系统设计">三、方法与系统设计</h2>
<p>从工程视角看，本文要解决的是：</p>
<blockquote>
<p><strong>“如何在不崩掉训练吞吐的前提下，把激活显存压到能跑 trillion-scale 模型的水平？”</strong></p>
</blockquote>
<p>整体思路是“两步走”：</p>
<ol type="1">
<li><strong>建模</strong>：把 Transformer 每一层、每一块（attention / MLP / LayerNorm / Dropout）的激活内存用公式数清楚，顺带把 TP / SP / PP 的影响都代入进去。</li>
<li><p><strong>优化</strong>：</p>
<ul>
<li>在结构层面：设计一种 <strong>TP + SP 组合的并行方式</strong>，通过 <span class="math inline">\(g / \bar g\)</span> 操作把非 TP 区域按序列切片，避免 LayerNorm/Dropout 这类激活在 TP 组内重复存储。</li>
<li>在算子层面：只对 attention 中“大激活、低 FLOPs”那一部分做 <strong>选择性重计算</strong>，其它地方照常缓存，从而在“显存”和“重算开销”之间取得更优折中。</li>
</ul></li>
</ol>
<p>可以拆成几个具体子问题：</p>
<ul>
<li><em>子问题 1：</em> 如何用一个简洁的公式刻画“单层 Transformer 的激活内存”，并能平滑代入 TP/SP/PP 等并行参数？</li>
<li><em>子问题 2：</em> 如何把序列并行和张量并行揉在一起，在不增加通信带宽的前提下，把之前 TP 里“没法切”的那一部分激活按序列分片？</li>
<li><em>子问题 3：</em> 在一层内部，哪些激活适合 checkpoint（重算），哪些应该直接存？怎样在 QKV/softmax/attention over V 这些子算子之间切分？</li>
<li><em>子问题 4：</em> 当再叠加流水并行时，第一 stage 需要存多少 micro-batch 的激活，以及如何在实践中控制 recompute 的开销不失控？</li>
</ul>
<h3 id="核心模块一览">3.1 核心模块一览</h3>
<p>按论文思路，把方法拆成几个“工程模块”会更清晰：</p>
<ul>
<li><strong>激活内存近似模型</strong>：给出无并行时的“单层激活内存公式”，把 attention / MLP / LayerNorm / Dropout 各自的贡献拆开，并明确哪些可以忽略（小 buffer）。</li>
<li><strong>张量并行（Tensor Parallel, TP）基线</strong>：假设 TP 只切 attention / MLP 内部的大 GEMM，把激活在这些 op 内部均匀分摊到 <span class="math inline">\(t\)</span> 个设备，但 LayerNorm / Dropout 等非 TP 区域仍是每卡一份。</li>
<li><strong>序列并行（Sequence Parallel, SP）+ 转换算子 <span class="math inline">\(g/\bar g\)</span></strong>：在非 TP 区域沿序列维 <span class="math inline">\(s\)</span> 切分，设计 <span class="math inline">\(g\)</span>（all-gather）和 <span class="math inline">\(\bar g\)</span>（reduce-scatter）来在“序列切分域”和“张量切分域”之间无缝转换。</li>
<li><strong>选择性激活重计算（Selective Activation Recomputation）</strong>：只重算 attention 中在 <span class="math inline">\(QK^\top\)</span>、softmax、softmax dropout、attention over V 区域的激活，它们内存巨大但每元素 FLOPs 不多；其余部分照常缓存。</li>
<li><strong>与流水并行的结合（1F1B / interleaved 1F1B）</strong>：分析在经典 1F1B 调度下，首个流水 stage 永远需要同时 hold <span class="math inline">\(L\)</span> 层激活；在此基础上给出总激活内存公式，并讨论 interleaved pipeline 时的修正因子。</li>
</ul>
<h3 id="数据流与控制流">3.2 数据流与控制流</h3>
<p>用“从输入到 loss，再到反向”的视角，可以把数据流/控制流串成如下步骤（只关注单个 stack）：</p>
<ol type="1">
<li><p><strong>输入嵌入层</strong></p>
<ol type="1">
<li>词表 embedding：查表得到形状为 <span class="math inline">\((s, b, h)\)</span> 的 token 表示。</li>
<li>加上可学习的位置编码（同形状），得到 <span class="math inline">\(X^{(0)}\)</span> 作为第 1 层输入。</li>
<li>在启用 SP 时，这一层的 Dropout mask 也可以按序列切分存储。</li>
</ol></li>
<li><p><strong>第 <span class="math inline">\(\ell\)</span> 个 Transformer 层的前向（无并行视角）</strong></p>
<ol type="1">
<li>LayerNorm：<span class="math inline">\(Y = \text{LN}(X)\)</span>，输出仍为 <span class="math inline">\((s,b,h)\)</span>，需要缓存输入 <span class="math inline">\(X\)</span> 作为激活。</li>
<li><p>Self-Attention：</p>
<ul>
<li>QKV 投影：从 <span class="math inline">\(Y\)</span> 经过三次线性层得到 <span class="math inline">\(Q,K,V\)</span>，尺寸 $ (s,b,h)$ 或 <span class="math inline">\((s,b,h/a)\)</span>。</li>
<li><span class="math inline">\(QK^\top\)</span>：计算注意力 logits，尺寸大约为 <span class="math inline">\((a,s,s,b)\)</span>。</li>
<li>softmax + dropout：得到注意力权重，再施加 dropout。</li>
<li>attention over V：用注意力权重加权 V，得到 <span class="math inline">\((s,b,h)\)</span> 的输出。</li>
<li>输出线性：再投影回 <span class="math inline">\((s,b,h)\)</span>。这些步骤产生大量中间激活。</li>
</ul></li>
<li>残差 + LayerNorm：把 attention 输出加回输入，做第二次 LN。</li>
<li><p>MLP：</p>
<ul>
<li>线性 <span class="math inline">\(h \to 4h\)</span>，产生 <span class="math inline">\((s,b,4h)\)</span>。</li>
<li>GeLU 非线性，需要缓存输入。</li>
<li>线性 <span class="math inline">\(4h \to h\)</span>，再加 Dropout。</li>
<li>残差加回。</li>
</ul></li>
</ol></li>
<li><p><strong>TP + SP 下的前向控制流（以 MLP 为例）</strong></p>
<ol type="1">
<li>在 LayerNorm 前，输入 <span class="math inline">\(X\)</span> 已按序列维切分：<span class="math inline">\([X^{s}_1, X^{s}_2, \dots, X^{s}_t]\)</span>。</li>
<li>LayerNorm 在各 rank 本地做，输出 <span class="math inline">\([Y^{s}_1,\dots,Y^{s}_t]\)</span>，此时仍按序列切分。</li>
<li>为送入 MLP 中的 GEMM，需要完整序列：调用 <span class="math inline">\(g\)</span> 做 <strong>all-gather</strong> 把 <span class="math inline">\(Y\)</span> 在每个 TP rank 上拼成完整的 <span class="math inline">\((s,b,h)\)</span>。</li>
<li>线性 + GeLU + 线性内部沿隐藏维切分（标准 TP），每卡只处理 <span class="math inline">\(h/t\)</span> 或 <span class="math inline">\(4h/t\)</span> 的 slice。</li>
<li>MLP 输出 <span class="math inline">\(W_1, W_2, \dots, W_t\)</span> 需要先求和再按序列切分给下游 Dropout，于是用 <span class="math inline">\(\bar g\)</span> 实现“求和 + 按序列 RS”的 <strong>reduce-scatter</strong>。</li>
<li>Dropout、残差在序列切分域中本地完成。</li>
</ol></li>
<li><p><strong>选择性重计算的控制流（以 attention 为主）</strong></p>
<ol type="1">
<li><p>正常前向时，只保留：</p>
<ul>
<li>输入 LN 前后的张量；</li>
<li>MLP 输入/输出；</li>
<li>以及 attention 中“宽度尚未放大”的部分。</li>
</ul></li>
<li><p>对于 <span class="math inline">\(QK^\top\)</span>、softmax、softmax dropout、attention over V 等区域：</p>
<ul>
<li>不缓存中间激活，只在反向需要时重跑一次前向子图。</li>
</ul></li>
<li><p>反向时，框架的 checkpoint 驱动：</p>
<ul>
<li>先重算被标记的子图，再基于重算激活做反向。</li>
<li>其它未 checkpoint 的部分直接用缓存激活反向。</li>
</ul></li>
</ol></li>
<li><p><strong>流水并行下的时序关系</strong></p>
<ol type="1">
<li>采用 1F1B 调度，首个 stage 必须同时 hold 多个 micro-batch 的激活，以填满流水。</li>
<li>对首个 stage 来说，有效“层数”是 <span class="math inline">\(L\)</span>，即使它实际只包含 <span class="math inline">\(L/p\)</span> 个物理层。</li>
<li>selective recompute 允许优先对最占内存的部分重算，rest full-cache，从而在显存和重算开销间按实际卡容量做折中。</li>
</ol></li>
</ol>
<h3 id="关键假设与适用范围">3.3 关键假设与适用范围</h3>
<p>论文中的推导和结论基于若干重要假设，在实践中需要意识到它们的边界：</p>
<ol type="1">
<li><p><strong>只考虑主干 Transformer 块，忽略“小 buffer”</strong></p>
<ul>
<li>假设：LayerNorm 的均值/方差（<span class="math inline">\(2sb\)</span>）和 bias 等 <span class="math inline">\(O(h)\)</span> 级别 buffer 可以忽略，仅关注 <span class="math inline">\(O(sbh)\)</span> 的激活。</li>
<li>可能失效的场景：极短序列、小 hidden size 或大量额外辅助分支（例如多任务头）时，这些“小 buffer”占比上升，理论模型与实际显存可能有数个百分点偏差。</li>
</ul></li>
<li><p><strong>统一使用 16-bit 激活（每元素 2 bytes），dropout mask 1 byte</strong></p>
<ul>
<li>假设：所有激活都以 FP16/BF16 存储，只有 logits 等少量张量使用 FP32。</li>
<li>风险：如果你的栈中仍大量保留 FP32 激活（比如稳定性原因）、或者有自定义 kernel 使用更宽的中间格式，实际显存会高于模型预测。</li>
</ul></li>
<li><p><strong>层结构高度同质，忽略 embedding / output 层贡献</strong></p>
<ul>
<li>假设：所有 Transformer 块的结构相同，embedding 和最后一层 FC / loss 的额外激活可以近似忽略。</li>
<li>例外：在非常浅的网络（小 <span class="math inline">\(L\)</span>）或 embedding/output 极大（超大 vocab）时，这一近似会变差，需要手工加上额外项。</li>
</ul></li>
<li><p><strong>采用 1F1B 或 interleaved 1F1B 流水调度，首 stage 为瓶颈</strong></p>
<ul>
<li>假设：流水调度为 1F1B 或文中的 interleaved 变体，并通过增大 micro-batch 数量把流水“压满”，使首个 stage 的激活显存成为系统瓶颈。</li>
<li>在非典型调度（大量 pipeline bubble、异构 stage、动态分配）或强 offload 场景，这个假设可能不成立，需要重新计算每个 stage 的峰值。</li>
</ul></li>
<li><p><strong>attention 头数 <span class="math inline">\(a\)</span>、序列长度 <span class="math inline">\(s\)</span> 足够大，使 <span class="math inline">\(5as/h \gg 34\)</span></strong></p>
<ul>
<li>假设：在 GPT-3、MT-NLG 这种规模下，attention 后半段激活（<span class="math inline">\(QK^\top\)</span>、softmax 等）占了绝大多数显存。</li>
<li>当 <span class="math inline">\(s\)</span> 很短、<span class="math inline">\(a\)</span> 很少、<span class="math inline">\(h\)</span> 很大时，<span class="math inline">\(5as/h\)</span> 不再显著大于 34，这时 selective recompute 的收益会下降。</li>
</ul></li>
</ol>
<h3 id="数学公式与算法解读">3.4 数学公式与算法解读</h3>
<p>这一小节挑出论文中几个关键公式，分别从“原文形式 → 含义 → 直观操作”三个层次来理解。</p>
<h4 id="单层激活内存无模型并行">3.4.1 单层激活内存（无模型并行）</h4>
<p><strong>原文中的公式（式 (1)）：</strong></p>
<p><span class="math display">\[
M_{\text{act, layer}} = sbh \left( 34 + 5a \frac{s}{h} \right)
\]</span></p>
<ul>
<li><p><strong>在解决什么问题？</strong> 这是“一个 Transformer 层在前向中需要缓存多少激活”的近似公式，用来估算在不使用任何 TP/SP/PP 时，每层激活占用的显存。</p></li>
<li><p><strong>符号含义：</strong></p>
<ul>
<li><span class="math inline">\(s\)</span>：序列长度（sequence length）</li>
<li><span class="math inline">\(b\)</span>：micro-batch 大小</li>
<li><span class="math inline">\(h\)</span>：hidden 维度</li>
<li><span class="math inline">\(a\)</span>：attention 头数</li>
<li><span class="math inline">\(M_{\text{act, layer}}\)</span>：这一层的总激活内存（单位是 bytes，因为每元素已经乘上了 2 bytes）</li>
</ul></li>
<li><p><strong>如何得到 34 和 <span class="math inline">\(5a s/h\)</span>？（直观版）</strong></p>
<ol type="1">
<li>把一个层拆成：两次 LayerNorm、一个 attention 块、一个 MLP 块。</li>
<li><p>粗略统计每部分需要缓存的张量数量和大小：</p>
<ul>
<li>attention 块约贡献 <span class="math inline">\(11sbh + 5as^2b\)</span>；</li>
<li>MLP 约贡献 <span class="math inline">\(19sbh\)</span>；</li>
<li>两个 LayerNorm 合计贡献 <span class="math inline">\(4sbh\)</span>。</li>
</ul></li>
<li><p>合起来就是 <span class="math inline">\((11 + 19 + 4) s b h = 34sbh\)</span>，再把 <span class="math inline">\(5as^2b\)</span> 写成 <span class="math inline">\(sbh \cdot 5a s/h\)</span>，得到上式。</p></li>
</ol></li>
<li><p><strong>等价重写（仅为直观）：</strong></p></li>
</ul>
<p><span class="math display">\[
M_{\text{act, layer}}
= s b h \cdot 34 ;+; 5 a s^2 b
\]</span></p>
<p>可以直接看成“<strong>与序列长度线性相关的主干部分</strong> + <strong>与 <span class="math inline">\(s^2\)</span> 相关的 attention 复杂部分</strong>”。</p>
<ul>
<li><p><strong>直观操作描述：</strong> 如果你给定 <span class="math inline">\((s, b, h, a)\)</span>，那么：</p>
<ol type="1">
<li>先算出“每层主干激活”的大小：<span class="math inline">\(34sbh\)</span>；</li>
<li>再算出“attention 正方形矩阵相关”的大小：<span class="math inline">\(5as^2b\)</span>；</li>
<li>二者相加就是这一层需要缓存的激活字节数。</li>
</ol></li>
</ul>
<h4 id="张量并行下的单层激活tp">3.4.2 张量并行下的单层激活（TP）</h4>
<p><strong>原文中的公式（式 (2)）：</strong></p>
<p><span class="math display">\[
M_{\text{act, layer}}^{\text{TP}}
= sbh \left(
10 + \frac{24}{t} + 5a \frac{s}{ht}
\right)
\]</span></p>
<ul>
<li><p><strong>含义：</strong> 在 <span class="math inline">\(t\)</span> 路张量并行（TP）下，只有 attention 和 MLP 内部“切得动”的那部分激活按 <span class="math inline">\(1/t\)</span> 分摊到了各卡，而 LayerNorm 和若干 Dropout 区域仍然在每卡完整保留，导致常数从 34 变成了 <span class="math inline">\(10 + 24/t\)</span>，而 attention 中的 <span class="math inline">\(5as^2b\)</span> 项变成了 <span class="math inline">\(5as^2b/t\)</span>。</p></li>
<li><p><strong>直观理解：</strong></p>
<ul>
<li><strong>“10”</strong>：未切分、在每张卡上重复存在的 LayerNorm + Dropout 等激活。</li>
<li><strong><span class="math inline">\(24/t\)</span></strong>：TP 后真正被均分的部分（大 GEMM 相关）。</li>
<li><strong><span class="math inline">\(5a s/(ht)\)</span></strong>：attention 中 <span class="math inline">\(s^2\)</span> 级别的激活在 <span class="math inline">\(t\)</span> 卡上平均分摊。</li>
</ul></li>
<li><p><strong>直观操作描述：</strong></p>
<ol type="1">
<li>先像式 (1) 那样算一遍“总的主干激活”与 “attention 激活”；</li>
<li>再把能切的部分除以 <span class="math inline">\(t\)</span>，不能切的部分保持不变；</li>
<li>把它们合起来，就得到了上式。</li>
</ol></li>
</ul>
<h4 id="张量-序列并行tpsp">3.4.3 张量 + 序列并行（TP+SP）</h4>
<p><strong>原文中的公式（式 (4)）：</strong></p>
<p><span class="math display">\[
\begin{aligned}
M_{\text{act, layer}}^{\text{TP+SP}}
&amp;= sbh \left(
\frac{10}{t} + \frac{24}{t} + 5a \frac{s}{ht}
\right) \
&amp;= \frac{sbh}{t}\left(
34 + 5a \frac{s}{h}
\right)
\end{aligned}
\]</span></p>
<ul>
<li><p><strong>含义：</strong> 把之前 TP 下仍然重复的 10<span class="math inline">\(sbh\)</span> 这块，通过沿序列维的 SP 再切一刀，最终整层激活（包括 attention 的那一块）都被均匀地分摊到了 <span class="math inline">\(t\)</span> 个 TP rank 上——直观就是“<strong>激活内存整体除以 <span class="math inline">\(t\)</span></strong>”。</p></li>
<li><p><strong>关键点：</strong></p>
<ul>
<li>依靠 <span class="math inline">\(g\)</span>（all-gather）和 <span class="math inline">\(\bar g\)</span>（reduce-scatter）这对“转换算子”把 LayerNorm / Dropout 区域从序列切分域切回张量切分域再切回去。</li>
<li>通信带宽不变：因为原来的 ring all-reduce 本身就是 reduce-scatter + all-gather 的组合。</li>
</ul></li>
<li><p><strong>直观操作描述：</strong></p>
<ol type="1">
<li>先按式 (1) 算出无并行时 <span class="math inline">\(M_{\text{act, layer}}\)</span>；</li>
<li>再简单除以 <span class="math inline">\(t\)</span>，就得到 TP+SP 下每卡需要的激活内存。</li>
</ol></li>
</ul>
<h4 id="加上流水并行后的总激活内存">3.4.4 加上流水并行后的总激活内存</h4>
<p><strong>原文中的公式（式 (5)）：</strong></p>
<p><span class="math display">\[
M_{\text{total}}^{\text{acts}} =
\frac{s b h L}{t}\left(
34 + 5 a \frac{s}{h}
\right)
\]</span></p>
<ul>
<li><p><strong>含义：</strong> 在 1F1B 流水调度下，首个 pipeline stage 尽管只负责 <span class="math inline">\(L/p\)</span> 个物理层，但因为要同时“在飞”<span class="math inline">\(p\)</span> 个 micro-batch，最终 peak 激活量等价于“<strong><span class="math inline">\(L\)</span> 层都压在这一卡上</strong>”。因此总激活内存等于“单层激活 × <span class="math inline">\(L\)</span> 层 / <span class="math inline">\(t\)</span>”。</p></li>
<li><p><strong>直观操作：</strong></p>
<ol type="1">
<li>用式 (4) 算出单层 TP+SP 下的激活：<span class="math inline">\(\frac{s b h}{t}(34 + 5 a s/h)\)</span>；</li>
<li>乘上需要同时驻留的“等效层数”——在经典 1F1B 中就是 <span class="math inline">\(L\)</span>；</li>
<li>得到上式。</li>
</ol></li>
</ul>
<blockquote>
<p>如果采用 interleaved pipeline，论文指出需要再乘上一个 <span class="math inline">\((1 + \frac{p-1}{pm})\)</span> 的修正因子，这里不展开。</p>
</blockquote>
<h4 id="全层重算-vs-选择性重算">3.4.5 全层重算 vs 选择性重算</h4>
<ol type="1">
<li><p><strong>全层激活重算的内存（简单情形）</strong></p>
<p><strong>原文中的结论：</strong></p>
<p><span class="math display">\[M_{\text{full-recompute}} \approx 2 s b h L\]</span></p>
<ul>
<li>含义：如果你只 checkpoint 每层输入/输出（假设每层只一组），忽略其它激活，那么每层只需要存两份 <span class="math inline">\((s,b,h)\)</span>，总共就是 <span class="math inline">\(2sbhL\)</span>。</li>
<li>问题：显存是下来了，但每次反向要多跑一个完整前向，FLOPs 增加约 33%–40%，在大模型上非常肉疼。</li>
</ul></li>
<li><p><strong>选择性激活重算（重点）</strong></p>
<p><strong>原文中的公式（式 (6)）：</strong></p>
<p><span class="math display">\[
M_{\text{selective}} =
\frac{34 s b h L}{t}
\]</span></p>
<ul>
<li><p>含义：在 TP+SP 的基础上，只对 attention 中“大激活、低 FLOPs”的那几块做重算，把 <span class="math inline">\(5 a s^2 b\)</span> 那一坨激活完全从显存中移除，只剩下主干的 <span class="math inline">\(34sbh\)</span>，然后再除以 <span class="math inline">\(t\)</span>。</p></li>
<li><p>直观：</p>
<ul>
<li>无并行 + 无重算：<span class="math inline">\(L \times sbh(34 + 5as/h)\)</span>；</li>
<li>TP+SP + 无重算：再除以 <span class="math inline">\(t\)</span>；</li>
<li>TP+SP + 选择性重算：再把 <span class="math inline">\(5as/h\)</span> 那块整个砍掉，对应就变成式 (6)。</li>
</ul></li>
<li><p>以 GPT-3 / MT-NLG 为例： 对于 GPT-3 (<span class="math inline">\(a=96,s=2048,h=12288\)</span>)，有 <span class="math inline">\(5 a s/h \approx 80\)</span>； 对于 MT-NLG (<span class="math inline">\(a=128,s=2048,h=20480\)</span>)，有 <span class="math inline">\(5 a s/h \approx 64\)</span>。 相比主干常数 34，这说明<strong>绝大多数激活其实来自那一小撮 attention 子算子</strong>，砍掉它们能省掉 60%–70% 的激活，而相应重算 FLOPs 仅增加 1.6%–2.7%。</p></li>
</ul></li>
</ol>
<hr />
<p><strong>与常见训练栈的对应关系</strong></p>
<p>从“我的大规模训练栈（如 Megatron / DeepSpeed / vLLM 等）”视角，可以这么理解这些模块对应到哪几层：</p>
<ul>
<li><p><strong>激活内存模型 → 配置搜索/自动调参层</strong></p>
<ul>
<li>用上面的公式快速预估在给定 <span class="math inline">\((s,b,h,a,L,t,p)\)</span> 下的激活峰值，帮助选择 TP/SP/PP 组合和 micro-batch 大小。</li>
</ul></li>
<li><p><strong>TP+SP 组合并行 → 模型并行策略层</strong></p>
<ul>
<li>对应框架里“张量并行 + 序列并行”的维度配置，例如 <code>tensor_model_parallel_size</code>、<code>sequence_parallel_size</code>，以及相关 shard 规则。</li>
</ul></li>
<li><p><strong><span class="math inline">\(g/\bar g\)</span> 算子 → 通信 backend + kernel 层</strong></p>
<ul>
<li>实际落地就是把原来的 <code>all_reduce</code> 替换成配对的 <code>all_gather</code> + <code>reduce_scatter</code>，常常与 GEMM kernel 融合在一起以减少中间缓冲拷贝。</li>
</ul></li>
<li><p><strong>选择性激活重算 → Checkpoint 策略/自动重算层</strong></p>
<ul>
<li>对应框架里的 <code>activation_checkpoint_method</code>、<code>checkpoint_attention</code> 之类的开关，以及在 Python 图里包一层 <code>checkpoint(function, *args)</code>。</li>
</ul></li>
<li><p><strong>Pipeline 分析 → 并行调度与作业编排层</strong></p>
<ul>
<li>决定每个 stage 放多少层、micro-batch 数量、是否使用 interleaved pipeline，并确保首 stage 的显存峰值满足卡容量。</li>
</ul></li>
</ul>
<hr />
<h2 id="四建模方式与评估指标">四、建模方式与评估指标</h2>
<h3 id="问题是如何形式化的">4.1 问题是如何形式化的？</h3>
<p><strong>核心优化目标</strong>可以简单概括为：</p>
<blockquote>
<p>在给定设备显存约束与并行配置（TP/SP/PP）的条件下， 最小化激活内存峰值与重算带来的额外 FLOPs 开销之和。</p>
</blockquote>
<p>论文没有写成严格的优化问题，但通过公式基本隐式完成了建模：</p>
<ol type="1">
<li><p><strong>激活内存模型：</strong></p>
<ul>
<li>无并行时单层激活： <span class="math inline">\(M_{\text{act, layer}} = sbh(34 + 5 a s/h)\)</span>。</li>
<li>TP+SP+PP 后首 stage 总激活： <span class="math inline">\(M_{\text{total}} = \dfrac{s b h L}{t} (34 + 5 a s/h)\)</span>。</li>
<li>selective recompute 后： <span class="math inline">\(M_{\text{selective}} = \dfrac{34 s b h L}{t}\)</span>。</li>
</ul></li>
<li><p><strong>FLOPs 模型：</strong></p>
<p>表 2 中给出了不同配置下每层 FLOPs，例如无并行时：</p>
<p>$$ _{} =================================</p>
<p>72 s b h^2 (1 + ) $$</p>
<p>其它配置则在此基础上除以 <span class="math inline">\(t\)</span>，或增加部分重算相关项（比如 selective recompute 下的 <span class="math inline">\(1 + \frac{2s}{9h}\)</span> 等）。</p></li>
<li><p><strong>简化与约束：</strong></p>
<ul>
<li>假设所有层同构，忽略 embedding/output 等小头；</li>
<li>只考虑单 precision（16-bit）激活；</li>
<li>只分析 FP 算力，暂不引入通信时延模型（通信通过“bytes communicated”单独报告）。</li>
</ul></li>
</ol>
<p>整个建模的思路是：<strong>先用解析式锁定“理论上最优的内存分摊方式”，再在这个空间内讨论不同 checkpoint 策略的代价</strong>。</p>
<h3 id="核心评估指标">4.2 核心评估指标</h3>
<p>论文里的指标非常工程向，基本可以直接映射到你的监控面板上：</p>
<ol type="1">
<li><p><strong>激活内存（Activations Memory）</strong></p>
<ul>
<li>含义：单层或整个模型在前向/反向时为激活分配的显存峰值（通常以 GB 或占卡总显存的百分比表示）。</li>
<li>对应关系：直接决定“能不能在一张卡上跑下这个配置”，也是是否需要再打开重算的第一判断依据。</li>
</ul></li>
<li><p><strong>每层前向/反向时延（Forward / Backward Time per Layer）</strong></p>
<ul>
<li>含义：固定模型 &amp; batch 设置下，单层 forward + backward 的 wall-clock 时间（ms），文中用单层 22B 模型来做 micro benchmark。</li>
<li>对应关系：用来拆分“重算多耗了多少时间”、“SP/TP 对 LayerNorm/Dropout 加速了多少”。</li>
</ul></li>
<li><p><strong>重算开销（Recompute Overhead）</strong></p>
<ul>
<li>含义：在“无重算 baseline”的前提下，重算之后 forward+backward 总时延的相对提升，比如 full recompute 的 +39% vs selective+SP 的 +4%。</li>
<li>对应关系：帮助判断“多省的显存是否值这点时间”，对于整机训练吞吐尤为关键。</li>
</ul></li>
<li><p><strong>端到端迭代时间（Iteration Time / Throughput）</strong></p>
<ul>
<li>含义：一次完整迭代（forward+backward+优化器更新）所需时间，论文中报告的是 22B/175B/530B/1T 模型的迭代时间与对应 throughput 提升（约 29%–32%）。</li>
<li>对应关系：这是最贴近“训练总时长”的指标，也最容易映射到预算上。</li>
</ul></li>
<li><p><strong>模型 FLOPs 利用率（MFU）与硬件 FLOPs 利用率（HFU）</strong></p>
<ul>
<li><p>含义：</p>
<ul>
<li>MFU：模型理论 FLOPs / 峰值算力；</li>
<li>HFU：实际执行 FLOPs（包括重算）/ 峰值算力。</li>
</ul></li>
<li><p>对应关系：说明在应用 selective recompute 后，虽然实际 FLOPs 稍有增加，但总体算力利用率仍然可以维持甚至略升，比如 1T 模型的 MFU/HFU 在 56% 左右。</p></li>
</ul></li>
<li><p><strong>通信量（Bytes Communicated）</strong></p>
<ul>
<li>含义：每层在不同配置下的总通信字节数，表 2 中以 bytes 形式给出。</li>
<li>对应关系：对比“TP vs TP+SP vs full/partial recompute”是否引入额外 all-gather / reduce-scatter，帮助判断在不同网络拓扑（单机 NVLink、多机 IB）下是否会被通信瓶颈卡住。</li>
</ul></li>
</ol>
<hr />
<h2 id="五主要实验发现">五、主要实验发现</h2>
<p>用几条结论把整篇实验的要点串一下：</p>
<ul>
<li><p><strong>TP+SP / selective recompute 单独使用时，各自都能将激活显存压到 TP 基线的约一半：</strong> 仅加 sequence parallelism，就能把激活降到原来的 ~50%；仅加 selective recompute，同样约半；两者叠加可达到约 <strong>5×</strong> 的压缩，使得 175B / 530B / 1T 配置在 80GB 卡上变得可行。</p></li>
<li><p><strong>选择性重算极大降低了重算开销：</strong> 在 22B 单层实验中：</p>
<ul>
<li>全层重算：forward+backward 总时延增加 39%；</li>
<li>selective 重算：增加 7%；</li>
<li>selective + SP：仅增加 4%。</li>
</ul></li>
<li><p><strong>对大模型越友好，模型越大收益越高：</strong> 对 530B 和 1T 模型，full recompute 的重算 overhead 约 36%，而 selective+SP 的 overhead 仅 2%。</p></li>
<li><p><strong>端到端吞吐提升约 30%：</strong> 在 4 组模型（22B/175B/530B/1T）上，与“full recompute + 无 SP”相比，本文方案的迭代时间缩短 29%–32%，对应 throughput 同比例提升。</p></li>
<li><p><strong>FLOPs 利用率稳步提升：</strong> 随模型规模变大，MFU / HFU 从 40%+ 提升到 56% 左右，说明激活内存优化带来的“更大 batch、更好并行配置”对整体硬件利用率收益显著。</p></li>
</ul>
<h3 id="关键图表解读">5.1 关键图表解读</h3>
<ol type="1">
<li><p><strong>图 7：不同方案的激活内存占比（相对于 TP baseline）</strong></p>
<ul>
<li>现象：随着模型规模增大，sequence parallelism 与 selective recompute 单独使用时都能将激活压到约 50%；合用后能降到不到 20%，而 full recompute 在 10% 左右。</li>
<li>支撑主张：说明在“只付出 2%–7% 重算开销”的前提下，TP+SP+selective 已经非常接近 full recompute 的内存效率，但少了大量无谓的算力开销，是实践中更平衡的方案。</li>
</ul></li>
<li><p><strong>图 8：每层 forward / backward / recompute 时间拆分</strong></p>
<ul>
<li><p>现象：</p>
<ul>
<li>baseline 无重算时，backward 时间远大于 forward；</li>
<li>full recompute 给 backward 顶上去一大块；</li>
<li>selective+SP 的“重算条”非常细，对整体影响极小。</li>
</ul></li>
<li><p>支撑主张：证明 selective 重算确实只把重算集中在 FLOPs 稍多的那一小块，且通过重叠通信/计算把 overhead 压得很低。</p></li>
</ul></li>
<li><p><strong>表 5：端到端迭代时间与 FLOPs 利用率</strong></p>
<ul>
<li>现象：所有规模模型的 iteration time 都从 “full recompute” 配置中减掉了 ~30%；同时 MFU/HFU 随规模增大而升高，1T 模型可到 56%+。</li>
<li>支撑主张：说明本文不仅仅是“把显存凑够就完事”，而是在实际训练吞吐和硬件利用率上都证明了工程价值。</li>
</ul></li>
</ol>
<p><strong>结果解读与边界</strong></p>
<p>总体来看，实验非常有说服力：公式推导和实测数据高度匹配，从单层 micro benchmark 到上百层、上万卡的端到端实验，都展示了 TP+SP+selective 的稳定优势。</p>
<p>但也存在一些未完全覆盖的维度，例如：</p>
<ul>
<li>并未系统评估 <strong>更复杂的重复结构</strong>（MoE、带多路分支的 encoder-decoder）中 selective recompute 的收益与开销；</li>
<li>对 <strong>梯度 checkpoint 搜索算法</strong>（如 CVPR 2021 的 Optimal Checkpoint Search）只在相关工作中提及，未做直接对比；</li>
<li>实验主要集中在单一硬件平台与网络拓扑，对低带宽多机环境下“all-gather / reduce-scatter 数量增加是否成为瓶颈”缺乏系统评价。</li>
</ul>
<hr />
<h2 id="六优点与局限">六、优点与局限</h2>
<p><strong>亮点（Strengths）</strong></p>
<ul>
<li><strong>问题刻画非常精准</strong>：从“激活内存”而不是“总显存”切入，将 attention / MLP / LN / Dropout 的激活占比拆得非常细，有利于工程上针对性优化。</li>
<li><strong>解析模型简单但威力大</strong>：几个短公式就解释了 TP / SP / PP 的内存行为，并自然给出“激活均匀分摊到 TP 组”的最优形式，为后续工作提供了统一的度量尺。</li>
<li><strong>TP+SP 设计优雅</strong>：通过 <span class="math inline">\(g/\bar g\)</span> 将 all-reduce 拆成 all-gather + reduce-scatter，不改变通信带宽，仅改变通信算子的形态，就解决了非 TP 区域的激活重复问题。</li>
<li><strong>选择性重算非常工程友好</strong>：只需要在 attention 内部加几处 checkpoint 标记，既减少大量激活，又避免像“全层重算”那样动辄 +30% FLOPs。</li>
<li><strong>实验覆盖到 trillion-scale</strong>：在 22B–1T 四个量级模型上完整评估，包含单层时延、整模型迭代时间、MFU/HFU，非常贴近实际大模型训练场景。</li>
<li><strong>与现有并行栈高度兼容</strong>：TP+SP+PP 的组合可以自然嵌入到主流 3D 并行框架中，不与 ZeRO/FSDP 等参数/优化器切分技术冲突。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02054">arXiv</a>)</li>
</ul>
<p><strong>局限（Limitations）</strong></p>
<ul>
<li><strong>结构假设较强</strong>：只针对标准单 stack Transformer，且默认层结构高度一致，对 MoE、encoder-decoder、多任务头等复杂拓扑的适配并未深入讨论。</li>
<li><strong>完全手工的 checkpoint 策略</strong>：当前 selective recompute 方案基于人工分析，并未利用图搜索/自动调度算法去进一步逼近理论最优。</li>
<li><strong>缺少对激活碎片化问题的定量分析</strong>：虽然结论部分提到碎片和首 stage 内存不均是未来工作方向，但正文未给出系统测量或模型。</li>
<li><strong>通信性能假设偏理想</strong>：将 all-reduce = reduce-scatter + all-gather 视作“通信带宽不变”，在跨机、非全连接拓扑下可能不完全成立。</li>
<li><strong>与其它内存优化技术的组合分析有限</strong>：例如与 ZeRO/FSDP、offload、FlashAttention 等组合后的整体收益，目前仍需读者自行探索。(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02054">arXiv</a>)</li>
</ul>
<hr />
<h2 id="七业内相关工作对比">七、业内相关工作对比</h2>
<p>下面选 3 类代表性工作，与本文做一个工程视角下的对比：</p>
<table>
<colgroup>
<col style="width: 29%" />
<col style="width: 22%" />
<col style="width: 25%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="header">
<th>工作</th>
<th>问题定义</th>
<th>方法路线</th>
<th>贡献与实用价值（主观）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>本文：减少激活重计算</strong></td>
<td>大规模 Transformer 训练中，<strong>激活显存</strong>成为主要瓶颈，full recompute 带来巨大算力开销。</td>
<td>精确建模激活内存，结合 TP + SP 均分激活，并在层内对子算子做 selective recompute。</td>
<td>在不改模型结构的前提下，显存压缩 5×，吞吐提升 ~30%，对于已有 3D 并行栈几乎是“必选项”。</td>
</tr>
<tr class="even">
<td><strong>ZeRO / FSDP 系列</strong>(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02054">arXiv</a>)</td>
<td>聚焦 <strong>参数+优化器状态</strong> 的内存冗余，使模型规模随设备数线性扩展。</td>
<td>通过切分 optimizer state、gradient、parameter，将数据并行中的冗余全部打散，配合 offload。</td>
<td>大幅减小“模型状态”占用，适合在 DP 维度扩展，和本文在维度上高度互补。</td>
</tr>
<tr class="odd">
<td><strong>GSPMD / 通用 SPMD 并行</strong>(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.04663">arXiv</a>)</td>
<td>提供一种统一的图级 SPMD 并行抽象，支持 TP/PP/DP/混合。</td>
<td>将并行视作对 tensor shape 的“sharding spec”，由编译器自动完成调度与通信插入。</td>
<td>在编译层面对各种并行形式进行统一描述，适合作为 TP+SP+selective 这类优化的“载体”。</td>
</tr>
<tr class="even">
<td><strong>Sequence Parallelism from System Perspective</strong>（SP 系列工作）(<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/372917292_Sequence_Parallelism_Long_Sequence_Training_from_System_Perspective">ResearchGate</a>)</td>
<td>面向超长序列训练，关注 <strong>沿序列维切分 activations/参数</strong> 的系统设计。</td>
<td>提出多种 SP 变体（ring attention 等），通过在 attention 内加入特殊通信模式减少 <span class="math inline">\(s^2\)</span> 存储和计算。</td>
<td>对长上下文模型极为重要，与本文的 SP 思路类似但更关注“长序列下 attention 的计算 pattern”。</td>
</tr>
</tbody>
</table>
<p>整体而言，本文可以看作是 <strong>“TP-centric 3D 并行栈中针对激活的一块补完”</strong>：</p>
<ul>
<li>在参数/优化器维度，它自然可以与 ZeRO/FSDP 协同；</li>
<li>在编译/图调度维度，可以被 GSPMD 等 SPMD 框架实现为一套 sharding 规则与通信重写；</li>
<li>在长序列场景下，可与更激进的 SP / context parallel / ring attention 等方案互补。</li>
</ul>
<h3 id="个人观点">7.1 个人观点</h3>
<p>从“如何写一篇系统论文”的角度看，这篇文章的论证路线非常清晰：</p>
<ul>
<li>先用解析模型解释清楚 <strong>“为什么需要 TP+SP + selective”，以及它在公式上的最优性</strong>；</li>
<li>再用多组实验验证“模型和现实基本一致”，并贯穿不同模型规模，避免只在单一规模做 cherry-pick。</li>
</ul>
<p>如果要挑刺，我觉得可以加强的部分有：</p>
<ul>
<li><strong>baseline 更丰富</strong>：目前重算部分主要对比的是“full recompute vs selective”，如果能再加上“一些自动 checkpoint 搜索算法”（例如 Feng &amp; Huang 2021）或现有框架中的默认策略，对工程选型会更有参考意义。</li>
<li><strong>与其它内存优化的组合实验</strong>：例如将 TP+SP+selective 与 ZeRO/FSDP/FlashAttention/参数 offload 一起放入同一张对比表中，说明不同维度上的可叠加性。</li>
<li><strong>对碎片和调度的更系统分析</strong>：如能在附录中补充 pipeline 首 stage 的内存碎片分布、不同 micro-batch 数量对碎片的影响，会更利于工程落地时做二次权衡。</li>
</ul>
<hr />
<h2 id="八在实际训练栈中如何落地">八、在实际训练栈中如何落地？</h2>
<p>假设你已经有一套“3D 并行 + 激活 checkpoint” 的训练栈（比如某种 Megatron/DeepSpeed 风格），要引入本文方法，大致可以从以下几个层面动手：</p>
<ol type="1">
<li><p><strong>并行调度（TP / SP / PP 组合）</strong></p>
<ul>
<li>在现有 TP 配置上，新增 <strong>sequence parallel 维度</strong>，例如增加 <code>sequence_parallel_size</code>，并为 LN/Dropout/embedding/output 等非 TP 区域指定“按序列切分”的 layout。</li>
<li>在 pipeline 切分时，显式考虑“首 stage 需要 hold <span class="math inline">\(L\)</span> 层激活”的事实，用上面的公式评估不同 <code>pipeline_model_parallel_size</code> 下的 peak 显存。</li>
<li>对多机场景，确认 SP 的通信组（通常和 TP 组一致），避免跨节点频繁做 all-gather / reduce-scatter。</li>
</ul></li>
<li><p><strong>kernel / 算子实现</strong></p>
<ul>
<li>为 LN/Dropout/write-back 等算子增加 <strong>SP awareness</strong>：输入张量在序列维上是 shard 的，算子应能在局部 shard 上工作。</li>
<li>将原本在 TP 内部使用的 <code>all_reduce</code> 改写成成对的 <code>all_gather</code> + <code>reduce_scatter</code>，并尽可能与 GEMM kernel 融合，减少中间 buffer。</li>
<li>在 attention 中，对 <span class="math inline">\(QK^\top\)</span>、softmax、dropout、attention over V 那一段子图增加“便于重算”的边界，比如使用框架内的 <code>checkpoint</code> 包一层。</li>
</ul></li>
<li><p><strong>激活 checkpoint / 重算策略</strong></p>
<ul>
<li><p>提供一个<strong>细粒度的重算配置接口</strong>，允许用户单独控制：</p>
<ul>
<li>是否对 attention 内部做 selective checkpoint；</li>
<li>是否对 MLP 或整层做额外 checkpoint（在更紧张显存下）。</li>
</ul></li>
<li><p>将论文里的“GPU 级 FLOPs overhead 估算公式”固化为工具函数，让用户在配置文件中看到“预估重算 overhead 与激活节省比例”，以帮助选边界。</p></li>
</ul></li>
<li><p><strong>通信与集体操作 backend</strong></p>
<ul>
<li>在通信层额外支持“基于形状与 layout 的 all-reduce ↔ AG+RS 重写”，必要时对 <code>all_gather</code> 和 <code>reduce_scatter</code> 做专门调优（pipeline overlap、组内拓扑 awareness 等）。</li>
<li>为 SP/TP 的通信 group 提供统一管理，避免出现“一个 rank 同时隶属太多 group 导致 NCCL resource 紧张”的问题。</li>
</ul></li>
<li><p><strong>DataLoader / 预处理与打包策略</strong></p>
<ul>
<li><p>虽然本文不直接改变 DataLoader，但在实践中通常会利用“节省下来的激活显存”去增加 micro-batch 或 global batch，此时需要检查：</p>
<ul>
<li>数据打包是否支持更大 batch（尤其是多任务混合数据集）；</li>
<li>长序列训练时，是否与 SP / context parallel 等策略冲突。</li>
</ul></li>
</ul></li>
<li><p><strong>配置搜索 / 自动调参</strong></p>
<ul>
<li>将激活内存模型与 FLOPs 模型做成一个小工具（甚至可以写成 Python 脚本），在给定硬件规格和模型配置的情况下，自动搜索可行的 <code>(TP, SP, PP, micro-batch)</code> 组合。</li>
<li>对于自动化的 launcher，可以在提交前直接给出“预估 peak 显存、重算开销、MFU 上限”等信息。</li>
</ul></li>
<li><p><strong>监控与调试</strong></p>
<ul>
<li>在框架中增加 per-layer / per-stage 的 <strong>激活内存追踪</strong>（通过 forward/backward hooks），验证是否符合论文公式的预估。</li>
<li>监控“重算区”的时间占比，确认 selective 重算的 overhead 是否接近论文中的 2%–7%，若远高于此需要检查通信 overlap 是否生效。</li>
</ul></li>
</ol>
<p>总的来说，引入本文方法的工程工作量主要集中在 <strong>算子 layout 改写 + 通信模式重写 + checkpoint 策略细化</strong>，对上层模型代码侵入较小。</p>
<hr />
<h2 id="九值得进一步探索的研究方向">九、值得进一步探索的研究方向</h2>
<ol type="1">
<li><p><strong>自动化激活重算策略搜索</strong></p>
<ul>
<li>问题：目前 selective recompute 仍基于手工划分；对于更复杂的网络结构，人肉选择 checkpoint 边界既费时又可能 sub-optimal。</li>
<li>价值：结合已有的“最优 checkpoint 搜索”算法（如 CVPR 2021 Feng &amp; Huang）与本文的激活内存模型，有望自动给出在不同显存预算下的最佳重算策略。</li>
</ul></li>
<li><p><strong>与 ZeRO / FSDP / offload 的统一建模</strong></p>
<ul>
<li>问题：当前实践往往同时启用参数/梯度/优化器的切分与 offload，以及激活层面的 TP+SP+selective，缺乏统一的成本模型。</li>
<li>价值：构建一个统一的“显存+FLOPs+通信三元模型”，自动在“加大 DP、加大 TP/SP、加大小重算”之间平衡，指导 trillion-scale 训练栈设计。</li>
</ul></li>
<li><p><strong>面向长上下文的序列并行与重算协同</strong></p>
<ul>
<li>问题：随着 128K+ 上下文模型普及，各类 sequence/context parallel（ring attention、Ulysses 等）将 attention 变得更加复杂。(<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/372917292_Sequence_Parallelism_Long_Sequence_Training_from_System_Perspective">ResearchGate</a>)</li>
<li>价值：在这些 SP 变体中引入 selective recompute，分析在 <span class="math inline">\(s\gg h\)</span> 情况下重算开销的精确行为，可能会给长上下文模型带来新的可行配置。</li>
</ul></li>
<li><p><strong>针对 MoE 与稀疏结构的激活内存优化</strong></p>
<ul>
<li>问题：MoE 将计算稀疏化，但激活内存仍可能较高，且路由/门控带来新的通信与存储模式。</li>
<li>价值：扩展本文的激活模型到“稀疏激活”场景，定义 per-expert 的激活与重算策略，有助于在保持稀疏计算优势的同时进一步压缩显存。</li>
</ul></li>
<li><p><strong>pipeline 首 stage 内存碎片与动态调度</strong></p>
<ul>
<li>问题：论文提到 pipeline 首 stage 的显存不均与碎片化是未来方向之一，但尚无系统方案。</li>
<li>价值：结合 allocator 行为（如 buddy / caching allocator）与 dynamic micro-batching，探索在不改模型结构的前提下，通过调度与分配策略进一步降低首 stage 峰值。</li>
</ul></li>
</ol>
<hr />
<h2 id="十知识图谱思维链">十、知识图谱思维链</h2>
<p>从“大模型系统”的知识图谱来看，这篇论文涉及的连接点大致如下：</p>
<ul>
<li><p><strong>并行与调度</strong></p>
<ul>
<li>提供了一个把 TP+SP+PP 一起放进激活内存公式的框架，让“如何选 TP/SP/PP 组合”从拍脑袋变成可计算的问题。</li>
<li>把 1F1B / interleaved pipeline 的显存峰值特性用简洁公式刻画出来，为之后的流水调度论文（如多种 1F1B 变体）提供了对比基线。(<a target="_blank" rel="noopener" href="https://aclanthology.org/2025.naacl-long.454.pdf">ACL Anthology</a>)</li>
</ul></li>
<li><p><strong>内存管理与显存优化</strong></p>
<ul>
<li>把激活内存拆解成“主干（<span class="math inline">\(34sbh\)</span>）+ attention 方阵（<span class="math inline">\(5as^2b\)</span>）”，让人一眼看出优化空间在哪里。</li>
<li>selective recompute 展示了“通过精细定位 FLOPs 便宜区”来换显存，是一类值得在其他结构上重复使用的模式。</li>
</ul></li>
<li><p><strong>通信与集体操作</strong></p>
<ul>
<li>显式利用“all-reduce = RS + AG”这一事实，通过 <span class="math inline">\(g/\bar g\)</span> 改写通信图，实现激活切分而不增加总通信量。</li>
<li>对比了不同方案下 per-layer 通信 bytes，为之后的通信优化工作提供了一个可参考的 baseline。</li>
</ul></li>
<li><p><strong>kernel 与算子优化</strong></p>
<ul>
<li>强调在 LN / Dropout / embedding / output 等算子中也做 SP，让这些“看似简单”的算子真正享受到并行带来的内存与速度收益。</li>
<li>鼓励把通信算子和 GEMM 融合，从而减少中间 buffer 与 kernel launch overhead。</li>
</ul></li>
<li><p><strong>模型结构与架构设计</strong></p>
<ul>
<li>虽然模型结构本身未修改，但激活内存分析可以直接用来评估“加宽/加深/加头数/加序列长度”对显存的影响，为设计新架构提供量化依据。</li>
<li>对 GPT-3/MT-NLG 的具体参数做了代入，不仅告诉你“公式长啥样”，还告诉你“在真实配置下数值是多大”。</li>
</ul></li>
<li><p><strong>数据、预处理与打包策略</strong></p>
<ul>
<li>从侧面说明了“节省激活显存之后可以做什么”：可以换成更大 micro-batch、更长序列或更多 global batch，对 DataLoader 与数据打包策略提出了新的需求。</li>
</ul></li>
</ul>
<h3 id="个人收获与反思">10.1 个人收获与反思</h3>
<p>对我个人来说，这篇论文最大的启发在于——<strong>很多看似“经验主义”的并行/重算技巧，其实可以被一个非常简洁的解析模型统一描述</strong>。一旦把激活内存拆成 <span class="math inline">\(34sbh\)</span> 和 <span class="math inline">\(5as^2b\)</span> 两块，很多选择就变得显而易见：TP+SP 应该怎么切、attention 中哪一部分值得 checkpoint、pipeline stage 怎么分层等，都可以从公式里直接读出来。</p>
<p>另一个收获是对 <strong>“局部重算”这一模式的再认识</strong>：以前提到 gradient checkpoint，多数人只想到“按层 checkpoint”；本文展示了“按层内子算子 checkpoint”可以更精细地调节显存/算力的 trade-off，而且实现成本并没有想象中那么高——只要你愿意在算子图上多画几条边界。</p>
<p>从实践角度，我认为值得立刻尝试迁移到自己训练栈里的点主要有两个：</p>
<ul>
<li>第一是 <strong>在现有 TP 栈上补齐 SP</strong>，至少要让 LN/Dropout/embedding/output 这些激活也能按序列 shard；</li>
<li>第二是在 attention 内部实现类似的 selective recompute，把 <span class="math inline">\(QK^\top\)</span>、softmax、attention over V 那块抽成一个 checkpoint 子图，并配合通信/计算 overlap 做些微调。</li>
</ul>
<blockquote>
<p>总体评价：这篇工作在不改变模型结构、不过度侵入训练栈的前提下，用一套简洁的理论和一组扎实的大规模实验，给出了一个几乎“默认应当启用”的激活内存优化方案。对于已经运行 3D 并行大模型训练的团队，它更偏工程实践；而对于正在搭建设备/并行栈的人，则提供了一个非常清晰的“并行+重算联合设计”参考范式。</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/paper/" rel="tag"># paper</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/11/23/paper/efficient_large_scale/" rel="prev" title="Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM">
      <i class="fa fa-chevron-left"></i> Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88"><span class="nav-number">1.</span> <span class="nav-text">一、论文速览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">二、论文结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E6%96%B9%E6%B3%95%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.</span> <span class="nav-text">三、方法与系统设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E4%B8%80%E8%A7%88"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 核心模块一览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%8E%E6%8E%A7%E5%88%B6%E6%B5%81"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 数据流与控制流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%81%87%E8%AE%BE%E4%B8%8E%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 关键假设与适用范围</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E4%B8%8E%E7%AE%97%E6%B3%95%E8%A7%A3%E8%AF%BB"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 数学公式与算法解读</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E5%B1%82%E6%BF%80%E6%B4%BB%E5%86%85%E5%AD%98%E6%97%A0%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C"><span class="nav-number">3.4.1.</span> <span class="nav-text">3.4.1 单层激活内存（无模型并行）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E5%B9%B6%E8%A1%8C%E4%B8%8B%E7%9A%84%E5%8D%95%E5%B1%82%E6%BF%80%E6%B4%BBtp"><span class="nav-number">3.4.2.</span> <span class="nav-text">3.4.2 张量并行下的单层激活（TP）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F-%E5%BA%8F%E5%88%97%E5%B9%B6%E8%A1%8Ctpsp"><span class="nav-number">3.4.3.</span> <span class="nav-text">3.4.3 张量 + 序列并行（TP+SP）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E4%B8%8A%E6%B5%81%E6%B0%B4%E5%B9%B6%E8%A1%8C%E5%90%8E%E7%9A%84%E6%80%BB%E6%BF%80%E6%B4%BB%E5%86%85%E5%AD%98"><span class="nav-number">3.4.4.</span> <span class="nav-text">3.4.4 加上流水并行后的总激活内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E5%B1%82%E9%87%8D%E7%AE%97-vs-%E9%80%89%E6%8B%A9%E6%80%A7%E9%87%8D%E7%AE%97"><span class="nav-number">3.4.5.</span> <span class="nav-text">3.4.5 全层重算 vs 选择性重算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F%E4%B8%8E%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.</span> <span class="nav-text">四、建模方式与评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%A2%E5%BC%8F%E5%8C%96%E7%9A%84"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 问题是如何形式化的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 核心评估指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E4%B8%BB%E8%A6%81%E5%AE%9E%E9%AA%8C%E5%8F%91%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">五、主要实验发现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%9B%BE%E8%A1%A8%E8%A7%A3%E8%AF%BB"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 关键图表解读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E4%BC%98%E7%82%B9%E4%B8%8E%E5%B1%80%E9%99%90"><span class="nav-number">6.</span> <span class="nav-text">六、优点与局限</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E4%B8%9A%E5%86%85%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E5%AF%B9%E6%AF%94"><span class="nav-number">7.</span> <span class="nav-text">七、业内相关工作对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E8%A7%82%E7%82%B9"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 个人观点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E5%9C%A8%E5%AE%9E%E9%99%85%E8%AE%AD%E7%BB%83%E6%A0%88%E4%B8%AD%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0"><span class="nav-number">8.</span> <span class="nav-text">八、在实际训练栈中如何落地？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E5%80%BC%E5%BE%97%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8E%A2%E7%B4%A2%E7%9A%84%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91"><span class="nav-number">9.</span> <span class="nav-text">九、值得进一步探索的研究方向</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%80%9D%E7%BB%B4%E9%93%BE"><span class="nav-number">10.</span> <span class="nav-text">十、知识图谱思维链</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E6%94%B6%E8%8E%B7%E4%B8%8E%E5%8F%8D%E6%80%9D"><span class="nav-number">10.1.</span> <span class="nav-text">10.1 个人收获与反思</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">PePe</p>
  <div class="site-description" itemprop="description">技术博客-分享自然语言处理、人工智能等相关知识</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备2024078386号 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PePe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
<script defer src="https://events.vercount.one/js"></script>

</body>
</html>
