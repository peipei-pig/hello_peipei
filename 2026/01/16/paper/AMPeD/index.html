<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yaopepe.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"post","display":"post","padding":18,"offset":12,"onmobile":false,"width_dual_column":240},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="AMPeD：Transformer分布式训练性能分析模型">
<meta property="og:type" content="article">
<meta property="og:title" content="AMPeD: An Analytical Model for Performance in Distributed Training of Transformers">
<meta property="og:url" content="https://yaopepe.com/2026/01/16/paper/AMPeD/index.html">
<meta property="og:site_name" content="果冻甜甜的">
<meta property="og:description" content="AMPeD：Transformer分布式训练性能分析模型">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2026-01-16T11:52:13.627Z">
<meta property="article:modified_time" content="2026-01-16T11:54:01.447Z">
<meta property="article:author" content="PePe">
<meta property="article:tag" content="paper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yaopepe.com/2026/01/16/paper/AMPeD/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AMPeD: An Analytical Model for Performance in Distributed Training of Transformers | 果冻甜甜的</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">果冻甜甜的</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-首页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

<div class="site-meta-counts" style="font-size:.9em;opacity:.85;margin-top:.25rem;display:flex;gap:.75rem;flex-wrap:wrap">
  <span class="post-meta-item">
    <i class="fa fa-eye"></i>
    <span class="post-meta-item-text">总访问量</span>
    <span id="vercount_value_site_pv">0</span>
  </span>

  <span class="post-meta-item">
    <i class="fa fa-file"></i>
    <span class="post-meta-item-text">总文章数</span>
    <span id="total_posts_count">18</span>
  </span>
</div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://yaopepe.com/2026/01/16/paper/AMPeD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="PePe">
      <meta itemprop="description" content="做正确的事">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="果冻甜甜的">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AMPeD: An Analytical Model for Performance in Distributed Training of Transformers
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2026-01-16 19:52:13 / Modified: 19:54:01" itemprop="dateCreated datePublished" datetime="2026-01-16T19:52:13+08:00">2026-01-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

<span class="post-meta-item">
  <span class="post-meta-item-icon"><i class="fa fa-eye"></i></span>
  <span class="post-meta-item-text">Views:</span>
  <span id="vercount_value_page_pv">0</span>  
</span>


            <div class="post-description">AMPeD：Transformer分布式训练性能分析模型</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>
<hr />
<p>原文：AMPeD: An Analytical Model for Performance in Distributed Training of Transformers · <a target="_blank" rel="noopener" href="https://diksha-moolchandani.github.io/files/papers/ispass_amped.pdf">PDF</a></p>
<!-- toc -->
<h2 id="一论文速览">一、论文速览</h2>
<p>AMPeD 提出了一种面向 <strong>Transformer 大模型分布式训练</strong> 的性能分析方法，核心要解决的是<strong>如何在不同并行策略组合下快速预测训练耗时</strong>。作者采用了一个<strong>解析建模</strong>的总体思路，将模型结构参数、数据/模型/流水线并行方案以及硬件配置都作为可调节输入，通过数学公式估计各部分计算与通信开销。实验表明，该模型预测的最优并行组合与业界经验基本一致，并预示了在未来<strong>高速互联</strong>硬件上训练速度可提升数倍，同时模型预测与真实测量的误差控制在约 12% 以内。</p>
<h2 id="二论文结构">二、论文结构</h2>
<ol type="1">
<li><strong>引言</strong>：介绍Transformer模型训练的规模和耗时挑战，指出分布式并行是必要手段，但不同并行策略组合可能带来开销，需要事先预测最优配置的重要性。</li>
<li><strong>背景</strong>：解释Transformer的基本架构与训练流程，并概述常见的分布式并行策略（数据并行、模型并行、流水线并行等）的原理和各自优缺点，为后续建模提供基础概念。</li>
<li><strong>相关工作</strong>：回顾近年来深度学习训练性能预测方面的研究，包括针对卷积网络的早期性能模型和模拟器等，突出目前尚无同时涵盖多种并行策略、专门面向Transformer的大规模训练预测模型。</li>
<li><strong>AMPeD 模型构建</strong>：详细阐述作者提出的性能分析模型，包括对计算部分（前向、反向、更新）的解析估计，对不同并行度下通信延迟的建模，以及流水线并行产生的等待气泡时间的计算方法。</li>
<li><strong>模型验证</strong>：介绍使用真实硬件实验和已有文献数据对模型进行验证的过程。作者针对数据并行和流水线并行组合进行了小规模测验，比较预测时间与实测时间，结果显示模型精度在可接受范围内。</li>
<li><strong>案例研究 I</strong>：面向给定的硬件配置（如单机多卡系统），利用模型探索各种并行策略组合的性能，找出<strong>该系统下的最优配置</strong>。读者可以从中了解如何应用模型指导实际并行方案选择。</li>
<li><strong>案例研究 II</strong>：针对资源受限的小型多机集群，分析在节点间并行扩展时的效率瓶颈。适合关心<strong>低端硬件在扩容时</strong>何种并行方式收益最高的读者参考。</li>
<li><strong>案例研究 III</strong>：展望未来硬件，模拟在光通信互联的分布式系统中训练超大模型的情景。对于关注<strong>前沿硬件对训练提速</strong>影响的读者，这部分提供了定量分析依据。</li>
<li><strong>结论</strong>：总结论文贡献，强调AMPeD模型在软硬协同设计中的作用，并指出模型可扩展到其它模型类型的潜力，鼓励未来在更广泛场景下应用。</li>
</ol>
<blockquote>
<p><strong>核心思想</strong>：提出一个将<strong>模型规模、并行策略和系统架构</strong>全部抽象为可调参数的性能模型，使研究者和工程师在不开启真实大规模训练的情况下，就能<strong>快速预测不同并行配置的训练时间</strong>。这一工作关键贡献在于量化了多维并行组合的代价，验证了模型预测的<strong>最优配置与实测最佳基本一致</strong>，为大模型分布式训练的方案选型提供了可靠依据。</p>
</blockquote>
<h2 id="三方法与系统设计">三、方法与系统设计</h2>
<p>作者构建了一个解析性能模型，用于估算Transformer模型在各种并行配置下每步迭代的耗时。整体思路是将训练过程拆解为若干可度量的子部分，并分别建模它们的开销，然后汇总得到总时间。为此，作者需要解决以下子问题： - 子问题 1：<strong>如何精确建模Transformer每层的计算开销</strong>（包括前向传播、反向传播和参数更新）。 - 子问题 2：<strong>如何刻画不同并行策略引入的通信延迟</strong>（数据并行的梯度同步、张量并行的张量切分通信、流水线并行的激活/梯度传输等）。 - 子问题 3：<strong>如何评估流水线并行产生的等待气泡时间</strong>，以及不同微批大小对流水线效率的影响。 - 子问题 4：<strong>如何利用性能模型搜索并行度配置空间</strong>，在给定硬件资源限制下找出最优的并行组合以最小化训练时间。</p>
<h3 id="核心模块一览">3.1 核心模块一览</h3>
<ul>
<li><strong>参数配置模块</strong>：负责读取和组织训练超参数、并行度配置、硬件规格等输入（通常通过<code>config.json</code>配置文件），为性能模型提供所有必要的参数。针对标记为需要计算或查表的参数，该模块还调用相应函数进行推导或查询。</li>
<li><strong>计算性能模型</strong>：估算Transformer各层的计算耗时。包括前向<code>U_f(l)</code>、反向<code>U_b(l)</code>和权重更新<code>U_w(l)</code>三个部分，通过模型参数（如每层的算术运算数量 <span class="math inline">\(N_{MAC}\)</span>）结合硬件算力（如GPU主频、核心数、峰值FLOPS及利用率）计算出每层计算所需时间。</li>
<li><strong>通信开销模型</strong>：估计不同并行策略下的数据传输时间。对于<strong>数据并行</strong>，建模每一步梯度全局聚合（All-Reduce）的时间；对于<strong>张量并行</strong>，计算层内切分张量所需的All-Gather/Reduce时间；对于<strong>流水线并行</strong>，计算相邻流水线 stage 之间传递激活值和梯度的时间；若使用<strong>MoE专家并行</strong>，也相应添加专家路由通信开销。模型用带宽和延迟参数来统一描述这些通信时间。</li>
<li><strong>流水线气泡模型</strong>：针对流水线并行特有的<strong>bubble 等待时间</strong>进行建模。当流水线分为多阶段且采用1F1B等调度策略时，不同阶段可能出现等待空闲。模型通过参数 <span class="math inline">\(N_{ub}\)</span>（micro-batch 数）等计算这些非重叠时间占用，用系数 <span class="math inline">\(R\)</span> 表示气泡无法重叠的比例，从而估算出流水线并行导致的等待损失。</li>
<li><strong>性能汇总与可视化</strong>：将计算、通信、气泡等各部分耗时汇总得到训练的<strong>总时间和效率指标</strong>。模型可以输出各项指标的明细，例如利用率、通信占比等，并通过辅助脚本将结果生成饼图或表格，帮助直观分析哪个部分是主要瓶颈。</li>
<li><strong>设计空间搜索模块</strong>（可选）：提供遍历不同并行配置的功能。通过脚本自动调整批大小以及数据/张量/流水线并行的内外层组合，利用性能模型计算每种组合的训练时间，列出最快的几个配置。这个模块相当于一个<strong>自动调参工具</strong>，方便用户在给定硬件上找到性能最优点。</li>
</ul>
<h3 id="数据流与控制流">3.2 数据流与控制流</h3>
<ol type="1">
<li><strong>数据输入与批处理</strong>：读取训练数据并按照设定的<code>mini-batch</code>划分，每个<code>mini-batch</code>再根据并行策略分发到多个设备上。如果采用流水线并行，还需进一步将<code>mini-batch</code>拆分为若干<code>micro-batch</code>，以便不同阶段可以流水线方式并行处理。</li>
<li><strong>前向传播计算</strong>：各个加速器（GPU）按模型划分执行自己负责的层的前向计算（矩阵乘法、注意力计算、激活函数等）。在数据并行策略下，每个加速器处理不同的训练样本，但执行完整模型的前向；在张量并行下，每一层的参数和运算被拆分到多卡执行，需要在层间或层内适时进行部分结果的通信整合；在流水线并行下，不同加速器各自处理模型的一段子网络，按<code>micro-batch</code>流水顺序计算多个样本的前向。</li>
<li><strong>反向传播计算</strong>：前向结束后，启动反向传播，各层按相反顺序计算梯度。每个加速器负责其对应层的梯度计算和传递。在张量并行策略中，某些梯度需要跨设备聚合（例如全连接层切分后需要跨卡All-Reduce累加梯度）；在流水线并行下，各加速器需等待相邻下游阶段传来的梯度，再继续往前传播。</li>
<li><strong>梯度同步与参数更新</strong>：当一个<code>mini-batch</code>的反向传播完成后，进行参数梯度的全局同步和更新。数据并行策略下，各GPU通过All-Reduce交换彼此计算的梯度并取平均，然后每个GPU独立执行优化器步骤（例如Adam或SGD更新参数）；在张量并行策略下，由于梯度在计算过程中已经All-Reduce过，各GPU直接更新自己持有的参数片；在流水线并行下，通常在所有微批处理完一个完整批次后，再统一更新一次参数。参数更新本身计算开销较小，但在超大模型下仍需考虑其对总时间的贡献。</li>
<li><strong>通信与同步</strong>：在前/后向过程中，分布式策略引入了多次通信。例如<strong>数据并行</strong>每一步会有一次全梯度All-Reduce；<strong>张量并行</strong>每层的前后向都会进行张量碎片的交换（All-Gather/Reduce-Scatter等）；<strong>流水线并行</strong>每处理完一个微批，需要将激活值发送到下一个阶段，以及将计算好的梯度传回上一个阶段。这些通信操作与计算穿插进行，由框架的调度策略协调。通信耗时取决于消息大小和带宽/延迟，与计算是否重叠执行也相关。</li>
<li><strong>流水线调度</strong>：在流水线并行中，采用例如“1F1B”（一次前向、一次反向交替）的调度策略来尽可能重叠各阶段的工作。当流水线进入稳定阶段时，各GPU交替执行前向和反向，使得大部分计算可以并行。然而，由于边缘条件（首尾微批没有前序/后续阶段可并行），仍可能出现<strong>pipeline bubble</strong>空闲周期。微批数量通常设置为并行阶段数的倍数，以减少气泡比例。</li>
<li><strong>性能汇总</strong>：性能模型对以上各环节的耗时进行累加，得到单步<code>mini-batch</code>迭代的总时间。随后乘以总批数<code>N_batch</code>可估计完整训练所需时间。模型也可进一步计算GPU的实际计算吞吐（与理论峰值相比）以及通信占整体的比重等指标，为分析瓶颈提供量化依据。</li>
</ol>
<h3 id="关键假设与适用范围">3.3 关键假设与适用范围</h3>
<ul>
<li><strong>硬件同构与同步假设</strong>：模型假定训练使用的集群由性能一致的节点/GPU构成，所有加速器同步执行每步计算。这意味着不存在某些GPU显著慢于其他GPU的情况，也默认没有异步更新或梯度失同步的训练模式。如果实际环境中硬件异构或采用了异步训练（如参数服务器架构），模型假设将被打破，预测的精度会降低或不再适用。</li>
<li><strong>计算效率近似假设</strong>：AMPeD通过给定的硬件参数（核心数、主频、FLOPS等）和一个经验效率系数来估算算子执行时间，暗含假设实际计算能够以较稳定的效率利用硬件峰值算力。这忽略了不同批大小、不同算子类型可能导致的算力利用变化（例如非常小的矩阵乘法无法充分利用GPU算力）。在某些场景下，如果实际计算效率因为访存瓶颈或kernels调度开销显著下降，模型基于固定效率的假设将不成立，预测时间可能偏乐观。</li>
<li><strong>通信线性叠加假设</strong>：模型将主要通信开销视作与数据量线性相关，并分别计算各并行模式下通信所需的时间，假定这些通信与计算的重叠程度有限或按照预定方式重叠。例如流水线并行部分，作者用一个固定比例 <span class="math inline">\(R\)</span> 来表示流水线气泡无法隐藏的部分，没有详细建模通信与计算动态交叠的复杂情况。如果在真实训练中通信可以更充分地与计算并行（或相反无法达到模型假定的并行度），则模型预测可能出现系统性偏差。</li>
<li><strong>忽略数据加载与其他开销</strong>：AMPeD聚焦于GPU上的训练计算和通信时间，假设数据准备、IO以及CPU端开销相对于长达数小时的训练可以忽略不计。对于大多数GPU密集型训练这假设成立，但在某些数据输入复杂的任务中（如大量数据增强或低带宽存储），数据加载可能成为瓶颈。此时模型未包含的数据IO耗时会使实际训练慢于预测。</li>
<li><strong>内存容量限制假设</strong>：模型在推荐并行配置时并未显式计算显存占用是否超出硬件限制，而是默认为只考虑那些<strong>能放下模型</strong>的配置。换言之，模型假设每张GPU的显存足以容纳按该并行度划分后的参数和激活。若某配置实际需要超出单卡显存（例如batch过大或并行度不足导致单卡模型太大），框架通常会报错或采用CPU/offload等措施，而这些都不在模型计算范围内。这意味着模型在<strong>适用范围</strong>上主要针对可行配置，超限配置需用户自行排除。</li>
</ul>
<h3 id="数学公式与算法解读">3.4 数学公式与算法解读</h3>
<p>论文的方法部分包含了较多解析公式，下文将挑选其中<strong>核心的性能估计公式</strong>进行解读。</p>
<p><strong>原文中的公式：</strong> <span class="math display">\[
T = N_{\text{batch}} \cdot \sum_{l=1}^{L} \frac{U_f(l) + U_b(l) + U_w(l)}{N_{DP} \times N_{TP} \times N_{PP}}\,,
\]</span> 其中 <span class="math inline">\(T\)</span> 表示完成整个训练任务的总时间，<span class="math inline">\(L\)</span> 为模型的层数。<span class="math inline">\(N_{\text{batch}}\)</span> 是总训练批次数（假设每批大小和迭代次数已知），<span class="math inline">\(N_{DP}, N_{TP}, N_{PP}\)</span> 分别是数据并行、张量并行、流水线并行的并行度（例如使用了多少路数据并行副本、将每层参数切分到几卡、分成几段流水线）。<span class="math inline">\(U_f(l)\)</span>、<span class="math inline">\(U_b(l)\)</span> 和 <span class="math inline">\(U_w(l)\)</span> 分别表示第 <span class="math inline">\(l\)</span> 层的前向计算、反向计算和权重更新所耗时间（在未并行拆分的情况下单卡执行时间）。</p>
<p>这个公式表达了<strong>训练总耗时</strong>随并行配置的变化：它将每层的计算开销求和并除以并行带来的加速因子。直观来看，如果采用数据并行，<span class="math inline">\(N_{DP}\)</span> 个GPU同时处理不同样本，可近似把每步计算时间除以 <span class="math inline">\(N_{DP}\)</span>；采用张量并行将每层的矩阵运算拆分到 <span class="math inline">\(N_{TP}\)</span> 个GPU，同样降低单层耗时；流水线并行有 <span class="math inline">\(N_{PP}\)</span> 个阶段理论上也可并行执行不同微批的前后向。但需要注意，<span class="math inline">\(N_{PP}\)</span> 的加速不像数据/张量并行那样线性（因为存在流水线气泡等待），所以上述公式是建立在理想重叠充分的前提下。如果并行度越高导致通信和等待变多，该模型后续还会引入校正项。</p>
<p><strong>符号解释</strong>：在上式中，<span class="math inline">\(N_{\text{batch}}\)</span> 和 <span class="math inline">\(L\)</span> 是训练任务和模型本身的已知量；<span class="math inline">\(N_{DP},N_{TP},N_{PP}\)</span> 是用户在并行配置上可调的整数变量；<span class="math inline">\(U_f(l), U_b(l), U_w(l)\)</span> 则由模型架构和硬件性能决定——例如 <span class="math inline">\(U_f(l)\)</span> 可以细化为该层总浮点运算量除以单卡每秒浮点能力再乘上经验效率系数。若第 <span class="math inline">\(l\)</span> 层包含 <span class="math inline">\(N_{MAC}(l)\)</span> 次乘加运算，GPU峰值每秒执行 <span class="math inline">\(F\)</span> 次运算且利用率为 <span class="math inline">\(\eta\)</span>，则 <span class="math inline">\(U_f(l)\)</span> 约为 <span class="math inline">\(\frac{N_{MAC}(l)}{F \cdot \eta}\)</span>。类似地，反向传播往往是前向运算量的两倍左右（具体取决于实现），权重更新涉及的运算量相对较小，可通过公式近似。总之，这些 <span class="math inline">\(U\)</span> 值反映每层纯计算所需时间。</p>
<p><strong>公式解读</strong>：以上公式实际对应<strong>“总训练时间 = 每步迭代时间 × 总迭代数”</strong>。每步迭代时间由各层顺序执行的时间叠加而成，但由于引入了并行，顺序执行的总时间被并行因子除减。比如，数据并行将样本划分到多卡，使得每批处理时间约为原来的 <span class="math inline">\(1/N_{DP}\)</span>；张量并行把单层工作分摊到 <span class="math inline">\(N_{TP}\)</span> 卡共同完成；流水线并行若各阶段都在忙于不同微批，也可接近把整模型计算并行化 <span class="math inline">\(N_{PP}\)</span> 倍。这个理想状态下，总体加速比就是 <span class="math inline">\(N_{DP}\times N_{TP}\times N_{PP}\)</span>。公式假设了这种<strong>理想线性加速</strong>，然后再乘以需要执行的批次数得到总时间。</p>
<p><strong>进一步说明</strong>：在论文中，作者实际上在更复杂的推导里加入了通信和气泡的影响，并不直接使用上述简化公式计算最终结果。例如，他们定义了额外的通信耗时函数 <span class="math inline">\(M(l)\)</span> 以及流水线等待时间 <span class="math inline">\(W(l)\)</span>，并在总时间计算中加上这些项。当并行度提高时，<span class="math inline">\(M(l)\)</span> 和 <span class="math inline">\(W(l)\)</span> 可能不为零，从而拉低并行效率。因此，更完整的模型公式为： <span class="math display">\[
T = N_{\text{batch}} \sum_{l=1}^{L} \Big[\frac{U_f(l)+U_b(l)+U_w(l)}{N_{DP}N_{TP}N_{PP}} + M(l) + W(l)\Big]\,,
\]</span> 不过为了便于理解，我们在此选取了核心的计算加速部分进行说明。总的来说，这组公式为不同并行策略下训练时间的定量分析提供了基础，使我们可以<strong>快速算出某配置的大致耗时</strong>，然后比较哪个配置更优。</p>
<p><strong>与常见训练栈的对应关系</strong>： - <strong>数据加载 / 预处理</strong>：AMPeD模型本身不涉及数据读取部分，相当于默认数据管道不会成为瓶颈。这对应在实际训练栈中，AMPeD从<code>DataLoader</code>之后的阶段开始发挥作用：假如数据加载足够快，GPU上计算和通信就是主要耗时，模型的预测准确；若数据加载较慢，在实践中需要确保这一阶段并行、缓存等优化，使得输入供给跟上GPU节奏。 - <strong>分布式并行调度</strong>：AMPeD要求用户明确指定数据并行、张量并行、流水线并行的规模，并将其映射到系统架构上。这对应训练框架中的<strong>并行策略调度层</strong>——例如在Megatron-LM或类似框架中，如何划分模型张量、如何安排流水线分段、以及如何在多机上部署。在这一层面，AMPeD相当于一个离线的调度顾问：它不参与实际调度执行，但提供了关于各种调度方案性能的预估，帮助调度层决定采用哪种方案。 - <strong>张量并行 / 模型并行实现</strong>：在训练栈中，模型并行属于中间层实现（典型如模型的线性层参数在多卡间拆分）。AMPeD通过参数 <span class="math inline">\(N_{TP,\text{intra}}\)</span> / <span class="math inline">\(N_{TP,\text{inter}}\)</span> 以及相关公式来模拟这种<strong>模型层的水平切分</strong>对计算和通信的影响。这对应实际框架中调用通信库（如NCCL）在每层前后向进行All-Gather或All-Reduce操作。换句话说，AMPeD模型中的通信延迟参数（带宽、延迟）就对应底层通信Backend的性能；它把框架里隐含的all-reduce开销显式量化出来了。 - <strong>并行通信Backend</strong>：对于分布式训练，通信Backend（例如Collective通信库）是训练栈的关键一环。AMPeD在架构层参数中要求提供节点内、节点间的带宽和延迟（如 <code>Cintra</code>、<code>Cinter</code>、<code>BWintra</code>、<code>BWinter</code>），这些就对应实际集群所使用的网络硬件和通信协议性能。例如，<code>BWinter</code>反映多机分布式通信的带宽（对应InfiniBand/NVLink等硬件），<code>Cintra</code>类似于NCCL通信的一次启动延迟。这些参数使模型可以估计All-Reduce、All-Gather等<strong>集合通信</strong>耗时，与训练栈中实际网络通信层的行为保持一致。 - <strong>GPU计算与Kernel效率</strong>：AMPeD用抽象的MAC次数和硬件算力来表示计算耗时，对应训练栈中<strong>GPU Kernel级的性能</strong>。比如模型中的 <span class="math inline">\(C_{MAC}\)</span> 可以理解为一个反映底层矩阵乘法kernel效率的参数。实际框架中，如果使用了Tensor Core、FFN融合等优化Kernel，等效于提高了 <span class="math inline">\(C_{MAC}\)</span> 所对应的效率，在AMPeD预测中就会体现为更短的 <span class="math inline">\(U_f(l)\)</span>。因此AMPeD的计算模型可视作训练栈里Profiler的延伸：它把各层算子的执行时间估计组合起来，形成整体性能预判。 - <strong>配置搜索与自动调参</strong>：许多大规模训练系统（如某些AutoML工具或超参数调优器）在训练开始前会尝试不同配置以找到更优解。AMPeD的设计空间探索功能相当于在训练栈之外增加了一个<strong>自动并行配置调优</strong>模块。通过AMPeD，可以在不开启真正训练的情况下试遍不同的并行组合，找出预计最快的配置，再将该配置用于实际框架执行。这避免了繁琐的人工试错和重复实验，提高了并行策略选择的效率。</p>
<h2 id="四建模方式与评估指标">四、建模方式与评估指标</h2>
<h3 id="问题是如何形式化的">4.1 问题是如何形式化的？</h3>
<p>作者将问题形式化为一个<strong>性能优化问题</strong>：给定模型结构和硬件资源，选择合适的并行策略使训练总时间最短。具体来说，他们建立了总训练时间 <span class="math inline">\(T\)</span> 对各类参数的函数关系，如上节公式所示 <span class="math inline">\(T(N_{DP},N_{TP},N_{PP},\ldots)\)</span>，并希望在资源限制（如GPU总数固定、每卡显存容量限制等）下，找到使 <span class="math inline">\(T\)</span> 最小的组合。</p>
<p>这个优化目标没有显式写成闭形式的最小化问题，但在方法上等价于<strong>枚举搜索</strong>最优点。作者通过解析建模把每一步计算 <span class="math inline">\(U_f,U_b,U_w\)</span> 和通信 <span class="math inline">\(M,W\)</span> 都表达为输入参数的函数，并考虑以下主要约束和简化假设： - <strong>并行度资源约束</strong>：数据并行、张量并行、流水并行在节点内外的乘积不能超过总GPU数，即 <span class="math display">\[
  N_{DP,\text{intra}} \times N_{TP,\text{intra}} \times N_{PP,\text{intra}} \times N_{\text{nodes}} = N_{\text{total\,GPUs}}\,,
  \]</span> 并且 <span class="math inline">\(N_{DP,\text{inter}} \times N_{TP,\text{inter}} \times N_{PP,\text{inter}} = N_{\text{nodes}}\)</span>。例如总共有64卡双机，则可能的组合是每机32卡的数据并行 (<span class="math inline">\(DP_{\text{intra}}=32\)</span>) 加2机 (<span class="math inline">\(DP_{\text{inter}}=2\)</span>)，或每机8卡数据并行+4卡张量并行 (<span class="math inline">\(8\times4=32\)</span> 每机) 再2机等等。框架要求并行配置必须满足这种整除关系。 - <strong>显存容量约束</strong>：虽然模型中未显式列出，但选择并行度时隐含确保单卡分配的模型参数和激活不超过其显存。比如如果张量并行度太低，每卡需容纳整个模型，会超内存，则该配置无效。作者在枚举配置时据此剔除了显存放不下的组合。 - <strong>流水线调度近似</strong>：采用Narayanan等人提出的1F1B调度策略，并假定微批数等于 <span class="math inline">\(N_{TP} \times N_{PP}\)</span>【注：这样设置通常能充分利用流水线并行】。这个简化使得模型计算流水线气泡时只用一个参数 <span class="math inline">\(R\)</span> 表示未重叠的比例，而不对调度过程做更复杂模拟。这样形式化处理降低了模型复杂度，但也意味着需要根据经验选择合适的 <span class="math inline">\(N_{ub}\)</span>（micro-batch数量）。 - <strong>通信时间的线性模型</strong>：作者将通信耗时形式化为 <span class="math inline">\(T_{\text{comm}} = \text{Latency} + \frac{\text{Message Size}}{\text{Bandwidth}}\)</span> 的形式，并为每种通信模式（节点内、节点间）设置不同的延迟和带宽。这是一个常用的线性化简化，忽略了潜在的网络争用、协议开销等非线性因素，使问题易于解析计算。 - <strong>忽略梯度精度/收敛影响</strong>：形式化过程中关注的是性能层面，并未将并行策略对收敛的影响纳入模型（例如更大的全局batch可能影响收敛速度，但模型不涉及loss或epoch，只计算每步耗时）。因此优化目标纯粹以时间最短为准，不考虑其他质量指标。</p>
<p>综合而言，作者将问题转化为了<strong>一个带约束的组合优化</strong>：硬件和模型参数是输入常量，变量是并行划分方式，评价函数是解析得到的训练时间。由于解析模型计算成本很低，他们采用穷举搜索验证少量备选配置即可找到近似最优。与其说是数学规划，不如说AMPeD提供了一个<strong>快速评价函数</strong>，工程上通过枚举+评价实现了最优化目标。</p>
<h3 id="核心评估指标">4.2 核心评估指标</h3>
<p>作者在实验中使用了多种指标来评估模型性能和并行方案优劣，主要包括：</p>
<ul>
<li><strong>单步迭代时间</strong>：模型计算每一次参数更新（处理一个mini-batch）所需的时间，通常以秒为单位衡量。
<ul>
<li>该指标体现了并行配置下<strong>单次前向+反向传播</strong>的耗时，是训练速度的直接度量。迭代时间越短，意味着吞吐越高，因此比较不同方案时此指标越小越好。</li>
</ul></li>
<li><strong>总训练耗时</strong>：根据单步时间乘以总迭代数得到完整训练任务的预计耗时，可用小时甚至天来表示。
<ul>
<li>这个指标在本文中通常以“训练X天”呈现，直接对应实际完成整个训练需要的时间长短。它是管理者关心的最终结果，用于衡量一种配置能否在可接受时间内完成训练任务（越短越理想）。</li>
</ul></li>
<li><strong>GPU计算利用率（TFLOPS）</strong>：衡量GPU实际执行的浮点运算速度与其理论峰值的比例。作者通过模型计算总浮点运算量除以总时间，得到每张GPU的平均有效TFLOPS占比。
<ul>
<li>该指标反映<strong>硬件资源利用效率</strong>。接近100%说明GPU算力几乎被榨干，低于理想值则表示有相当时间GPU在等待或闲置。通过此指标可以判断并行策略是否让GPU忙起来了。</li>
</ul></li>
<li><strong>通信耗时占比</strong>：一次迭代中花在通信上的时间比例。作者利用性能模型拆分出计算和通信部分，计算通信占总迭代的百分比。
<ul>
<li>该指标对应<strong>并行扩展的开销</strong>。如果通信占比高，说明增加并行度后大量时间耗在同步上，真正计算效率提升有限。理想情况下通信开销应尽量小，使计算主导总耗时。</li>
</ul></li>
<li><strong>流水线气泡时间</strong>：针对流水线并行特有的指标，指流水线各阶段等待其他阶段的空转时间占比（由于pipeline初始和结尾的填充和排空）。
<ul>
<li>这个指标体现<strong>流水线方案的并行度损失</strong>。气泡时间越短越好，长的话表示流水线没有完全保持所有设备忙碌。通过对比气泡占比，可以判断微批大小和阶段数是否设置合理。</li>
</ul></li>
<li><strong>预测误差</strong>：模型预测的时间与真实实验测量值之间的相对误差百分比。
<ul>
<li>这是评估本文提出模型<strong>准确度</strong>的指标。误差越小表示模型可信度越高。作者报告的最大误差约12%，证明模型在各种配置下都能较好逼近真实性能，为其实用性提供了支撑。</li>
</ul></li>
</ul>
<h2 id="五主要实验发现">五、主要实验发现</h2>
<ul>
<li><strong>大batch对并行效率至关重要</strong>：实验发现，要有效利用多种并行，需要使用<strong>较大的全局batch size</strong>。小batch下即使加更多GPU，GPU利用率仍然不高，增加并行度反而因为通信开销占比变大而效果不佳:contentReference<span data-index="0">oaicite:0</span>。只有batch足够大时，计算才能填满流水线和并行副本，减少空转和同步损耗。</li>
<li><strong>混合并行优于单一并行</strong>：对于给定硬件，纯数据并行或纯模型并行并非最佳。<strong>组合多维并行</strong>（数据+张量+流水线的3D并行）往往能更充分利用资源。案例研究显示AMPeD预测的最优配置通常是多种并行的折中，如单机用流水线并行减小每卡模型规模，多机再用数据并行扩展，总耗时比只用数据并行更低。</li>
<li><strong>跨节点扩展受网络限制</strong>：在多机情况下，并非简单增加节点就线性加速。实验表明，当每节点GPU较少或网络带宽一般时，继续以数据并行扩展会碰到<strong>通信瓶颈</strong>，收益递减甚至变负:contentReference<span data-index="1">oaicite:1</span>。这提示在低端集群上，与其一味堆机器，不如引入模型并行/流水线并行在节点间分摊计算，减少跨节点通信量。</li>
<li><strong>未来高速互联价值巨大</strong>：通过模拟光通信互连，作者发现如果网络带宽和时延显著改善（例如光纤互联取代传统以太网），大型模型训练可以提速<strong>最高4倍</strong>以上:contentReference<span data-index="2">oaicite:2</span>:contentReference<span data-index="3">oaicite:3</span>。这意味着当前训练耗时很大一部分是受制于通信，硬件升级通信子系统将明显缩短训练时间而无需增加算力。</li>
<li><strong>模型预测可靠性验证</strong>：作者将AMPeD模型预测与实际测量和文献结果进行了对比，<strong>绝大多数情况下误差在10%以内</strong>，且模型能准确预测哪种并行方案更快:contentReference<span data-index="4">oaicite:4</span>。例如，他们验证了GPipe流水线训练等公开数据，AMPeD给出的时间与发表结果非常接近。这一发现增强了对模型的信心，证明其可以指导实际配置选择。</li>
</ul>
<h3 id="关键图表解读">5.1 关键图表解读</h3>
<ul>
<li><em>图1：不同批大小下并行扩展收益</em> – 作者绘制了小、中、大三种全局batch下，随着节点间数据并行数增加，训练时间的变化曲线。图中呈现出<strong>小batch时曲线几乎下降很慢甚至持平</strong>，说明增加节点几乎无效；而<strong>大batch时训练时间随节点数近似线性下降</strong>。这一现象验证了：若批量过小，通信开销主导，扩展GPU无益；只有批量足够大才能发挥并行计算潜力:contentReference<span data-index="5">oaicite:5</span>。</li>
<li><em>图2：两种并行配置的时间开销构成对比</em> – 作者提供了一个饼图对比，在相同硬件上采用“纯数据并行” vs “数据+流水线并行”时，每步训练时间分配。结果显示纯数据并行时<strong>通信同步（梯度All-Reduce）占主要部分</strong>，而加入流水线并行后每卡计算变少，但出现了<strong>约20%的流水线等待气泡</strong>。图表直观地证明了两种方案的瓶颈差异：数据并行受限于大量通信，混合并行减通信但引入流水线等待，需要权衡两者。</li>
<li><em>图3：当前互联 vs 光互联的训练速度</em> – 作者以柱状图形式比较了使用传统以太网和理想光互联两种网络环境下完成同一大模型训练所需时间。可以看到，在光互联条件下总耗时仅为当前的约1/4:contentReference<span data-index="6">oaicite:6</span>。这支撑了论文核心主张之一：<strong>通信技术升级能带来数量级的性能提升</strong>，模型成功预测了硬件改进的收益，为硬件-软件协同优化提供了量化依据。</li>
</ul>
<p><strong>结果解读与边界</strong>：总体而言，这些实验充分证明了AMPeD模型对<strong>性能趋势的抓取是准确且有用的</strong>。通过对比不同并行组合的预测和实测，论文展示了模型能够识别最佳方案（支持核心结论“混合并行最优”），并且对未来硬件的分析也给出了令人信服的数据（支持“改善网络显著提速”这一展望）。模型预测与真实结果的误差较小，说明其关键假设在实验范围内基本成立，这为其在实际中的应用奠定了信任基础。</p>
<p>然而，需要注意实验本身也有一定局限，留下了一些未完全覆盖的维度： - <strong>超大规模验证</strong>：论文主要验证在最多数十卡规模，对上千卡级别的超大集群没有直接实验支撑。不同拓扑、大规模情况下是否依然12%内误差，有待更多测试。 - <strong>其他模型结构</strong>：AMPeD声称可推广到其他DNN，但实验聚焦Transformer。对于CNN、RNN甚至混合架构，其计算通信模式差异较大，模型未在论文中验证这些情形下的准确性。 - <strong>训练稳定性因素</strong>：并行度配置不仅影响性能也可能影响收敛（如大batch需调学习率）。论文结果只看时间，没有覆盖这些<strong>训练稳定性/收敛</strong>方面的因素，实际应用时需要额外验证模型精度不会因为并行策略改变而显著下降。</p>
<h2 id="六优点与局限">六、优点与局限</h2>
<p><strong>亮点（Strengths）</strong>： - <em>问题切中要害</em>：论文瞄准了当前AI领域极为现实的问题——<strong>大模型训练的性能瓶颈</strong>。在GPU资源昂贵、训练耗时动辄数周的背景下，提出性能预测模型可以说抓住了业界痛点，具有很高实用价值:contentReference<span data-index="7">oaicite:7</span>。 - <em>方法全面系统</em>：AMPeD模型将模型规模、并行策略、硬件参数都纳入统一框架，几乎覆盖了影响分布式训练性能的主要因素（包括少见的MoE专家并行）。这种<strong>全栈式建模</strong>在相关工作中少见，使其成为一个通用而系统的分析工具:contentReference<span data-index="8">oaicite:8</span>。 - <em>替代试错，降本增效</em>：传统上想找最优并行配置需要反复实验调参，耗费大量计算资源和时间。AMPeD提供了<strong>解析评估</strong>的手段，在笔尖上比较方案，大幅降低了探索开销。这对工程实践意义重大，可帮助团队在部署前筛选方案，节省真机试验成本。 - <em>验证严格</em>：作者既有自主的真实多GPU实验验证，又引用了多篇文献中的数据对比，并涵盖了多种模型/硬件配置，验证过程相当充分。结果显示模型预测与实测高度吻合（误差仅5%～12%），用数据证明了方法的可靠性:contentReference<span data-index="9">oaicite:9</span>。 - <em>实验丰富有洞见</em>：论文的三个案例研究横跨当前、高性能和未来硬件场景，每个都给出具体洞察。例如Case I总结出“大batch方能发挥并行效率”，Case III量化了光互联的价值。这些<strong>经验结论</strong>对于从业者调优并行方案有直接指导意义，不仅验证模型也产出了独立见解。 - <em>论文结构清晰</em>：文章在写作上层次分明，从背景-&gt;模型-&gt;验证-&gt;案例-&gt;结论逻辑顺畅。尤其公式和符号定义清楚，关键参数列出了含义，阅读起来尽管涉及复杂并行概念但<strong>易于跟随</strong>。这一优点让读者更容易理解并接受作者的观点和方法。</p>
<p><strong>局限（Limitations）</strong>： - <em>适用范围局限</em>：AMPeD目前专为Transformer类模型设计，在其他模型类型（如卷积网络、图神经网络）上未验证通用性。同时模型假定同步更新和均质GPU集群，对于<strong>异步训练或异构硬件</strong>环境并不适用，需要额外扩展才能覆盖更广泛场景。 - <em>模型假设简化</em>：为了解析计算，作者做出了一些简化假设，例如固定计算效率、不考虑通信与计算完全重叠等:contentReference<span data-index="10">oaicite:10</span>。这些假设在极端情况下可能不成立，从而降低模型准确性。例如pipeline气泡假设R=1在高并行度下高估了等待时间，导致某些配置预测偏保守:contentReference<span data-index="11">oaicite:11</span>。模型对不同并行度下缓存命中率、通信算法变化等细节也未涉及，需谨慎解读。 - <em>缺乏动态行为刻画</em>：AMPeD是静态性能模型，没有考虑训练过程中可能出现的动态变化。例如前期冷启动、I/O抖动、GPU热衰减等。真实训练中性能并非恒定，像learning rate调整、混合精度带来的变化都未在模型中体现。这使得模型更适合作为宏观规划工具，而非细粒度运行时优化。 - <em>实验规模有限</em>：论文的实证主要在单机和小规模多机（最多64卡）上进行，通过引用文献外推更大规模情况。这留下一个疑问：在超大规模分布式训练（数百上千卡）时，模型能否同样精确？例如网络拓扑复杂度和分层通信的影响未通过实验直接验证，是一个潜在薄弱之处。 - <em>未涉及能耗指标</em>：引言中提到训练能耗和碳排放:contentReference<span data-index="12">oaicite:12</span>是动机之一，但研究重点放在性能时间上。模型没有直接输出能耗或功率估计，实验也未提供不同方案在能源效率上的比较。对于关心水耗能效的人来说，这是一个遗憾的缺口。 - <em>使用门槛偏高</em>：尽管代码开源，AMPeD模型需要用户提供较详细的硬件和模型参数（如带宽、核心数、各层算子量）。普通开发者可能对这些底层参数不熟悉或获取不易。这意味着模型在落地时需要先做好系统测量和参数配置工作，否则预测精度无法保证。相较于直接运行基准实验，前期准备工作多了一步，这在快速试错场景下可能降低其吸引力。</p>
<h2 id="七业内相关工作对比">七、业内相关工作对比</h2>
<ul>
<li><strong>Paleo (2016)</strong>：<em>问题定义</em> – 面向早期的分布式深度学习，Paleo关注在给定计算资源下预测训练时间，但主要针对卷积和全连接网络，对Transformer这类新架构未作专门研究。【方法路线】 – Paleo使用解析方法建模GPU计算和通信，但模型中假设比较简单（不支持流水线并行、模型并行等多维组合）。<em>两者关系</em> – Paleo是性能建模领域的开拓工作，与AMPeD属于<strong>互补</strong>关系：前者奠定了分析思路，但局限于旧模型类型；后者扩展到Transformer和复杂并行，填补了大模型时代的空白。【贡献与实用价值】 – Paleo在当时为研究者提供了直观的性能预估工具，但如今已不太适用于超大模型的训练优化。相较之下，AMPeD面向当前主流模型和硬件，实用性更强，其贡献在于跟上了技术发展的需求。</li>
<li><strong>FlexFlow (OSDI 2018)</strong>：<em>问题场景</em> – 自动并行化工具，目标是在不给定并行策略的情况下，通过探索找到深度学习模型的最佳并行调度（包括数据、模型、混合并行）。<em>方法路线</em> – FlexFlow采取基于图的搜索和模拟，它将模型计算表示为依赖图，然后用启发式搜索遍历并行划分方案，通过模拟执行计算时间来评价方案优劣。相对于AMPeD，它是<strong>竞争</strong>关系的方案：FlexFlow直接给出具体的并行部署方案，而AMPeD给出的是通用的性能估计函数。<em>贡献与实用价值</em> – FlexFlow的强项在于高度工程化，已经融入部分框架实现自动并行，用户不需了解细节即可得到可执行的并行策略。但它的搜索过程对大型模型可能耗时较长。AMPeD提供了理论分析视角，计算开销很小，可用于指导类似搜索或在设计阶段快速评估方案。在实际应用中，AMPeD可以和FlexFlow结合：先用AMPeD筛掉明显次优的并行组合，再让FlexFlow在精简空间中精细搜索，从而兼顾效率和全局最优性。</li>
<li><strong>ZeRO (SC 2020)</strong>：<em>问题定义</em> – 面向超大模型的分布式训练，ZeRO重点解决单机显存不足的问题，通过分块存储优化器状态等方法来突破数据并行的内存瓶颈。<em>方法路线</em> – 它不是性能预测模型，而是一套分布式训练优化技术，将梯度、参数等拆分到不同GPU持有，分阶段同步，极大降低每卡内存占用。与AMPeD属于<strong>互补</strong>：ZeRO关注“<strong>能训下</strong>”更大模型，而AMPeD关注“<strong>训得快</strong>”哪个配置更优。<em>贡献与实用价值</em> – ZeRO已成为业界大模型训练（如DeepSpeed框架）的关键组件，使上百亿甚至万亿参数模型的训练成为可能。相应地，AMPeD的价值在于当模型能训时，帮助选择最快的并行策略。如果将来把AMPeD预测功能集成进类似DeepSpeed的调度器，就能在使用ZeRO等技术保障内存的同时，用AMPeD决定计算/通信资源的最佳分配，实现两者优势互补。</li>
</ul>
<h3 id="个人观点">7.1 个人观点</h3>
<p>总体来说，我认为论文在论证上是<strong>充分且有说服力</strong>的，但也存在一些可以改进之处。首先，在基准选择上，由于直观上很难找到另一个“一站式”性能模型与AMPeD直接对比，作者选择通过多案例验证而非与baseline比拼，这无可厚非。然而，如果能增加<strong>与已有模拟器或框架配置的对照</strong>，效果会更好。例如，与其只引用GPipe结果，不妨实际用AMPeD预测GPipe配置下的性能并和GPipe原论文实现的测量做一对一比较，展示AMPeD既能预言趋势也能接近绝对值。再者，实验设置上大多采用模型推断或文献数据，<strong>缺少真实训练消耗对比</strong>：我会期待作者亲自跑两个对比实验，比如在8卡机器上分别用AMPeD推荐的配置和一种次优配置训练一小段时间，直接测量吞吐差异。这种实证能更直观地证明模型指导配置选择的价值。</p>
<p>如果由我来设计额外实验或改进撰写，我可能会<strong>强化模型的可用性证明</strong>。例如，开发一个简单脚本，读取一份Megatron-LM配置文件，通过AMPeD预测不同并行度组合下的每秒样本数，然后在真机上跑几个epoch验证排名。这样能进一步赢得工程读者的信任。在论述中，我也愿意看到对模型假设的敏感性分析：作者提到R值可以调整，如果能有一张图展示调整R如何让预测更贴近实测，将使读者明白模型可以调参校准，而不至于完全失准。此外，我会考虑<strong>拓展评估维度</strong>，比如测量不同方案的能耗，或者展示一下并行策略对收敛的影响（哪怕只是说明大batch可能需要学习率调整，但不影响模型性能）。这些补充都会使论文结论更全面，使读者相信最优性能配置在实际中也是可取且收益明确的。</p>
<h2 id="八在实际训练栈中如何落地">八、在实际训练栈中如何落地？</h2>
<p>将AMPeD的方法引入实际的大规模训练栈（例如Megatron-LM、DeepSpeed等）中，需要对训练流程的若干部分进行相应调整和改进：</p>
<ul>
<li><strong>数据管道与批次处理</strong>：使用AMPeD可能会建议使用更大的<code>batch size</code>或不同的<code>micro-batch</code>数量。这要求数据加载环节有能力提供足够的数据吞吐，并支持灵活的批次大小配置。工程实现上需要确保DataLoader可以无阻塞地供应大batch（可能需要开启更多预取线程或扩大缓存）。主要风险在于<strong>数据I/O瓶颈</strong>：若批量突然增大数倍，磁盘读写或数据预处理可能跟不上，从而抵消模型预测的加速效果。此外，更大batch也意味着单次加载的数据增多，内存占用上需要关注，避免OOM。</li>
<li><strong>分布式并行配置调度</strong>：在训练框架中暴露并控制AMPeD涉及的并行参数。以Megatron-LM为例，本身允许用户指定<code>tensor-parallel</code>和<code>pipeline-parallel</code>大小以及数据并行进程数。要落地AMPeD，就要<strong>根据模型预测结果设置这些配置</strong>。工程改动主要是将AMPeD模块集成到配置流程中，例如在启动训练前调用AMPeD函数推荐配置，再用此配置初始化分布式进程。目前许多框架并没有自动调整并行度的逻辑，因此这部分需要新增脚本或接口。风险在于某些AMPeD推荐的组合可能框架暂不支持（例如跨节点的流水线并行或非常特殊的并行比），这需要开发者了解框架限制并对不支持的方案做回退处理。</li>
<li><strong>模型切分与张量并行</strong>：如果AMPeD建议启用或增大<strong>张量并行</strong>（模型并行），在工程上意味着需要确保模型各层的参数正确切分并放置在多张GPU上。Megatron-LM等框架内部已经实现了张量并行的张量切片和All-Reduce融合优化，但当并行度发生变化时，仍需注意：模型初始化、广播参数、保存检查点这些流程能否兼容新的切分维度。工作量主要体现在验证<strong>不同并行度下模型精度一致性</strong>和调试潜在的分布式同步Bug。另一个风险是<strong>显存峰值变化</strong>：增加张量并行后，每卡模型参数减少但需要额外通信缓冲，一些中间激活也需分片/聚合，实际显存占用走势可能与单卡模式有差异，需在部署前通过profiling检查peak memory是否在可控范围。</li>
<li><strong>流水线并行集成</strong>：AMPeD如果推荐更大的<strong>流水线并行</strong>深度，则需要在框架中把模型按推荐的stage数进行划分。工程上要修改模型定义或使用框架已有工具将模型分割成多段，分别放到对应rank运行。以DeepSpeed Pipeline为例，可以通过参数配置不同层数一个stage。关键是<strong>同步控制</strong>：需要调整backward梯度汇聚和update的时机，保证多阶段的梯度在正确的位置等待。这通常使用现有的流水线引擎（如1F1B调度器），无需太多新代码，但<strong>调试工作量</strong>会上升，因为跨多进程的forward/backward异步流水很难逐步跟踪。主要风险是<strong>Bubble控制</strong>：AMPeD假设微批数等于并行度以最小化bubble，如果框架无法灵活调整micro-batch数量，就可能出现实际气泡时间比模型预期多，达不到理想加速比。因此需要在框架配置层面确保微批数满足模型假设，否则考虑对AMPeD预测做修正。</li>
<li><strong>Kernel或算子实现影响</strong>：AMPeD模型假定底层算子性能符合预估（例如矩阵乘法按峰值FLOPS跑）。在实践中，如果引入某种并行导致使用了非最优kernel（比如非常小的批使得GPU算子launch开销相对变大），实际性能会偏离预测。工程上可以通过<strong>内核优化</strong>来弥合差距：例如启用NVIDIA的Kernel Fusion、调整算子实现以适配并行度（如分组MatMul以更好利用Tensor Core）。这可能需要修改或升级框架底层库。一旦涉及低层优化，工作量和风险都偏高，因为需要确保新Kernel的正确性和稳定性。不过收益也明显：如果实现的计算效率接近AMPeD假设值，预测与实际将更吻合。</li>
<li><strong>通信与集体操作调整</strong>：当并行度变化时，通信模式可能随之改变。比如数据并行从8卡变为32卡，全局All-Reduce的通信量和层次结构都会变化；增加流水线阶段会带来更多跨节点激活传输。工程上应结合AMPeD提供的通信占比信息，对集体通信算法和拓扑进行优化设置。例如在NCCL中选择Tree模式或Ring模式、分层All-Reduce等，以匹配规模和网络结构。若引入光互联等新硬件，还需相应升级通信库驱动。主要风险在于<strong>通信风暴和死锁</strong>：大规模通信可能出现网络拥塞或异常情况（比如集群网络不稳定导致AllReduce超时）。因此实际落地时需要通过小规模测试验证通信的健壮性，并在监控中加入对延迟和带宽的观察，以便及时调整参数（如梯度压缩、异步通信等手段缓解）。</li>
<li><strong>自动调参与配置搜索</strong>：要将AMPeD完全融入训练栈，理想情况是<strong>自动完成并行配置搜索</strong>。这需要开发一个上层调度器，在训练开始前甚至训练过程中调用AMPeD，对不同配置进行评估。工程步骤包括：获取当前集群可用GPU信息、设定可能的并行组合列表、批量调用AMPeD计算耗时、选出最优并行度组合，然后自动应用这个配置来启动或重排训练作业。这个流程可以集成到集群调度系统或者训练脚本中。工程工作量取决于接口易用性：AMPeD有Python API，可在脚本中直接import并调用，例如 <code>amped.estimateTrainingTime(config)</code>。风险则在于<strong>过度信任预测</strong>：如果AMPeD有未考虑因素导致某配置实际效果不如预测，自动化系统可能选错并行度。为降低风险，可以在运行过程中监控实际吞吐，对比预测值，必要时让调度器进行二次调整甚至回滚配置。此外，在多用户多任务环境下，引入自动配置也需考虑公平性和兼容现有调度策略，防止频繁调整干扰其他作业。</li>
</ul>
<h2 id="九值得进一步探索的研究方向">九、值得进一步探索的研究方向</h2>
<ul>
<li><strong>异构硬件与拓扑感知建模</strong>：让性能模型支持<strong>异构集群</strong>（不同类型GPU/加速器混用）和复杂网络拓扑。例如，扩展AMPeD在GPU+CPU协同训练、NVLink非完全连接、交换机多级互联等情形下的精度。这将解决目前模型仅限同构环境的限制，使其在云服务等多样化硬件环境中也能提供可靠预测。</li>
<li><strong>引入能耗与成本维度</strong>：当前模型关注时间，加下一步可以集成<strong>能耗模型</strong>，预测不同并行方案的功耗和能源效率。同时结合云资源定价，估算各方案的云计算成本。这样可以让优化目标从“最快”拓展为“最快且能耗最低”或“单位成本下最高效”，助力绿色AI和降低训练开销的研究。</li>
<li><strong>训练过程自适应优化</strong>：探索<strong>动态调整并行策略</strong>的机制，例如在训练的不同阶段或不同epoch采用不同的并行度。早期迭代可能对通信敏感，用小并行度减少开销；后期模型大梯度小可以提高并行度加速收敛。需要研究如何基于AMPeD的预测实时决定切换点和配置，以及切换并行方案对收敛和精度的影响。如果成功，将实现训练过程中的自适应优化。</li>
<li><strong>与自动并行框架融合</strong>：将解析模型与现有自动并行编译器/调度器结合，如把AMPeD嵌入Megatron-LM或DeepSpeed的规划器中作为<strong>快速代价评估函数</strong>。这可以显著缩短自动并行搜索时间。例如，FlexFlow这类系统可用AMPeD筛选部分并行策略，再对候选做精准模拟，从而提升整体搜索效率。研究如何无缝对接两种方法，并验证组合后在实际大模型任务上既高效又能找到全局最优方案。</li>
<li><strong>提升模型细粒度精度</strong>：进一步精进AMPeD的建模，例如加入对<strong>通信与计算重叠</strong>的更精确模拟、对<strong>cache命中率</strong>或<strong>kernel启动延迟</strong>的建模，让公式更贴近真实硬件行为。这可能需要借助一些微基准测试校准模型参数，或引入统计学方法刻画不确定性。通过提高模型精度，可以将预测误差从当前~10%降至几个百分点，从而增强模型在关键决策场景（如非常昂贵的大型实验前预测）时的可信度。</li>
</ul>
<h2 id="十知识图谱思维链">十、知识图谱思维链</h2>
<ul>
<li><strong>并行与调度</strong>：本论文最紧密相关的是<strong>分布式并行训练策略</strong>。它提供了对数据并行、张量并行、流水线并行等多种策略组合的系统化分析，相当于在我的知识图谱中丰富了“如何调度并行以优化性能”这一节点。例如，现在我清晰地看到不同并行方案的通信代价差异，以及组合并行的取舍原理，这与我对Megatron-LM等3D并行实践的认识形成了呼应。</li>
<li><strong>内存管理与显存优化</strong>：虽然AMPeD专注性能，但涉及参数和激活在不同并行度下如何分布，也隐含影响显存占用。我的知识图谱中“显存优化”部分因此得到补充：通过AMPeD的视角，我认识到像ZeRO拆分梯度、Activation Checkpoint减少激活内存这些技术，可以与并行策略结合起来考虑整体性能。例如，显存优化允许更高的并行度（模型能放得下），而并行加速又改变了计算通信比例，两者需要协同设计。</li>
<li><strong>通信与集体操作</strong>：论文大量涉及通信耗时建模，使我对“<strong>通信对训练的影响</strong>”有了定量认识。在知识图谱里，我会将这部分与“NCCL优化”“网络拓扑”连接起来——AMPeD显示了All-Reduce之类操作在扩展训练时是如何成为瓶颈的，也提示了优化通信（更高带宽、更优算法）的重要性。这扩展了我在分布式计算领域的知识链条，从只是知道通信开销存在，到能进一步估算它在特定配置下有多大占比。</li>
<li><strong>Kernel与算子优化</strong>：AMPeD将宏观的训练过程拆解到每层算子，特别是用每秒运算数这样的指标衡量算子性能。这在我的知识图谱中把“算子层优化”和“整体训练性能”这两个层面关联起来了。我意识到，通过提高单个Kernel的效率（例如利用张量核、算子融合），可以直接反映在AMPeD的参数中，从而降低总训练时间。因此以后考虑优化时，不仅关注算法，还会定量评估底层Kernel改进对全局性能的贡献。</li>
<li><strong>模型结构与架构设计</strong>：论文把模型层数、隐藏维度、专家数等都作为输入，这让我在知识图谱中连接了“模型架构改动”和“训练代价”两个节点。以前调整模型结构更多考虑精度，现在通过AMPeD的公式，我可以预估比如将层数翻倍会使训练时间增长多少，或者引入MoE专家需要增加哪些通信。这种对架构-性能关系的理解，将指导我在设计模型时考虑可训练性，把性能因素纳入决策。</li>
<li><strong>数据、预处理与打包策略</strong>：AMPeD间接强调了<strong>批处理策略</strong>（batching）对性能的影响。知识图谱里关于数据打包部分，我新增了一点：为了充分利用并行，需要提供尽可能大的batch和合理的micro-batch划分。这促使我反思数据集准备时，是否可以通过混合作样、本地shuffle等手段构建更大批次，同时观察是否达到硬件吞吐上限。总之，该论文提醒我数据层面的策略也不能孤立看待，必须服务于并行训练的整体效率。</li>
</ul>
<h3 id="个人收获与反思">10.1 个人收获与反思</h3>
<p>这篇论文带给我最大的启发是<strong>用解析方式看待大规模训练的瓶颈</strong>。以往在工程实践中，我们优化并行训练更多是凭经验调参或依赖一些benchmark测试。AMPeD提供了一个数学分析框架，告诉我可以在代码运行前就定量估算不同方案的代价。这种思维方式很有价值：它迫使我去拆分训练中的各个耗时环节，然后逐一优化或者平衡。不仅如此，通过作者提供的公式和案例，我对训练过程中哪些部分占用时间有了更清晰的认知——比如之前模糊觉得“通信很重要”，现在能具体到“在我们这个8机配置下，全局梯度All-Reduce可能吃掉40%时间”。这种定量认知将帮助我在日常工作中做出更有依据的决策。</p>
<p>我认为论文中的一些理念完全可以迁移到自己的实践中去。首先是<strong>提前规划并行策略</strong>：在拿到新模型或新硬件时，我可以先用类似AMPeD的思路粗算一下最优并行组合，而不是盲目套用以前的配置。例如，如果要在一个4机集群上训练一个100亿参数模型，我会事先估算数据并行4倍 vs 模型并行4倍哪个更划算，是否需要引入流水线，以及batch size应该多大以避免资源浪费。再有，<strong>监控指标选取</strong>方面，我准备在后续训练中加入对通信时间、GPU利用率的监控，这些指标和论文中的评估指标对应，可以帮助及时发现瓶颈（比如某轮通信耗时突然增加，说明网络异常等）。总之，通过阅读AMPeD，我在思想上更趋向于“性能工程”的系统性方法，会尝试将这种方法论融入我负责的训练平台中，如开发一个小工具计算理论吞吐，与实际监控做对比，从而不断优化我们的并行实现。</p>
<blockquote>
<p>总的来说，AMPeD论文将大规模模型训练的性能问题进行了深入而理性的剖析。它偏重工程应用，提供了实用的分析工具，对于那些需要在大型集群上训练Transformer模型的人而言特别值得一读。论文在架构和性能之间搭建了一个桥梁，既有学术上的创新性，也对当前训练生态有直接的启示作用。</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/paper/" rel="tag"># paper</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2026/01/14/other/nvrx/" rel="prev" title="NVIDIA Resiliency Extension (NVRx) 简介">
      <i class="fa fa-chevron-left"></i> NVIDIA Resiliency Extension (NVRx) 简介
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E8%AE%BA%E6%96%87%E9%80%9F%E8%A7%88"><span class="nav-number">1.</span> <span class="nav-text">一、论文速览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">二、论文结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E6%96%B9%E6%B3%95%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1"><span class="nav-number">3.</span> <span class="nav-text">三、方法与系统设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E4%B8%80%E8%A7%88"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 核心模块一览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%8E%E6%8E%A7%E5%88%B6%E6%B5%81"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 数据流与控制流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%81%87%E8%AE%BE%E4%B8%8E%E9%80%82%E7%94%A8%E8%8C%83%E5%9B%B4"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 关键假设与适用范围</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E4%B8%8E%E7%AE%97%E6%B3%95%E8%A7%A3%E8%AF%BB"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 数学公式与算法解读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E5%BB%BA%E6%A8%A1%E6%96%B9%E5%BC%8F%E4%B8%8E%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.</span> <span class="nav-text">四、建模方式与评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%A2%E5%BC%8F%E5%8C%96%E7%9A%84"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 问题是如何形式化的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 核心评估指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E4%B8%BB%E8%A6%81%E5%AE%9E%E9%AA%8C%E5%8F%91%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">五、主要实验发现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%9B%BE%E8%A1%A8%E8%A7%A3%E8%AF%BB"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 关键图表解读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E4%BC%98%E7%82%B9%E4%B8%8E%E5%B1%80%E9%99%90"><span class="nav-number">6.</span> <span class="nav-text">六、优点与局限</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E4%B8%9A%E5%86%85%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E5%AF%B9%E6%AF%94"><span class="nav-number">7.</span> <span class="nav-text">七、业内相关工作对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E8%A7%82%E7%82%B9"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 个人观点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E5%9C%A8%E5%AE%9E%E9%99%85%E8%AE%AD%E7%BB%83%E6%A0%88%E4%B8%AD%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0"><span class="nav-number">8.</span> <span class="nav-text">八、在实际训练栈中如何落地？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E5%80%BC%E5%BE%97%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8E%A2%E7%B4%A2%E7%9A%84%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91"><span class="nav-number">9.</span> <span class="nav-text">九、值得进一步探索的研究方向</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%80%9D%E7%BB%B4%E9%93%BE"><span class="nav-number">10.</span> <span class="nav-text">十、知识图谱思维链</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E6%94%B6%E8%8E%B7%E4%B8%8E%E5%8F%8D%E6%80%9D"><span class="nav-number">10.1.</span> <span class="nav-text">10.1 个人收获与反思</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">PePe</p>
  <div class="site-description" itemprop="description">做正确的事</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备2024078386号 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">PePe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
<script defer src="https://events.vercount.one/js"></script>

</body>
</html>
